2024-05-31 12:48:55;INFO;

alpaca Batch Size 1 alpha_dy fine-tuning 14 Layers
2024-05-31 12:48:55;INFO;
Total param      : 6738.423808
Train param      : 631.242752
Dataset          : alpaca
Method           : alpha_dy
Layers           : 14
Batch size       : 1
Learning Rate    : 2e-06
Eval Loss        : 1.0521228313446045
Forward time     : 13.57340157032013 min
Backward time    : 29.07870485385259 min
Weight memory    : 26953.711616 MB
Optimizer memory : 5049.942016 MB
Activation memory: 963.3251875940392 MB
Gradient memory  : 2558.3058068580394 MB
Input memory     : 0.0030693898039215687 MB
Total memory     : 34564.99712 MB
Peak memory      : 37148.601344 MB


