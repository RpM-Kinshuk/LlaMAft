2024-06-01 05:01:07;INFO;

alpaca Batch Size 1 full fine-tuning 250 Layers
2024-06-01 05:01:07;INFO;
Total param      : 8030.26944
Train param      : 8030.26944
Dataset          : alpaca
Method           : full
Layers           : 250
Batch size       : 1
Learning Rate    : 2e-07
Eval Loss        : 1.1652305126190186
Forward time     : 79.39877878824869 min
Backward time    : 104.31947911580404 min
Weight memory    : 36592.222208 MB
Optimizer memory : 64242.15552 MB
Activation memory: 1549.428992276203 MB
Gradient memory  : 28041.56289256975 MB
Input memory     : 0.0026754829825215293 MB
Total memory     : 128869.853696 MB
Peak memory      : 165270.19776 MB


