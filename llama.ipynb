{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-07 13:51:23.784283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 13:51:23.784325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 13:51:23.785724: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-07 13:51:23.794557: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 13:51:28.202288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "    dataset=\"alpaca\", # DATASET [alpaca|chip2|self-instruct|hh-rlhf|oasst1|longform]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "\n",
    "    learning_rate=2e-6,     # LEARNING RATE\n",
    "    \n",
    "    max_steps=2000,         # NUMBER OF STEPS\n",
    "\n",
    "    sortby=\"alpha\",         # CAN DO \"alpha\" or \"lora\" or \"dora\"\n",
    "\n",
    "    num_layers=12,           # NUMBER OF LAYERS FOR FULL FINE-TUNING\n",
    "\n",
    "    per_device_train_batch_size = 4, # BATCH-SIZE\n",
    "    memlog=True,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=256 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "args.cache_dir=cachedir\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "Dataset: alpaca\n",
      "Sort by: alpha\n",
      "Layers to train: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lora' in args.sortby:\n",
    "    args.num_layers = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.27.self_attn.v_proj', 'model.layers.11.self_attn.v_proj', 'model.layers.23.self_attn.o_proj', 'model.layers.3.mlp.up_proj']\n",
      "Enabling model.layers.3.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.11.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.17.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.23.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.27.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.28.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12745"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore\n",
    "dataset = {k:v for k,v in data_module.items()}\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.649153232574463\n",
      "Step: 1, Train Loss: 1.6787036657333374\n",
      "Step: 2, Train Loss: 1.626383900642395\n",
      "Step: 3, Train Loss: 1.5892391502857208\n",
      "Step: 4, Train Loss: 1.5332839012145996\n",
      "Step: 5, Train Loss: 1.5172670483589172\n",
      "Step: 6, Train Loss: 1.4861642462866647\n",
      "Step: 7, Train Loss: 1.4683053195476532\n",
      "Step: 8, Train Loss: 1.4805304606755574\n",
      "Step: 9, Train Loss: 1.4859204173088074\n",
      "Step: 10, Train Loss: 1.479223294691606\n",
      "Step: 11, Train Loss: 1.4619700014591217\n",
      "Step: 12, Train Loss: 1.4758446491681612\n",
      "Step: 13, Train Loss: 1.4736823780196053\n",
      "Step: 14, Train Loss: 1.426924204826355\n",
      "Step: 15, Train Loss: 1.4374700412154198\n",
      "Step: 16, Train Loss: 1.4432140799129711\n",
      "Step: 17, Train Loss: 1.4497534963819716\n",
      "Step: 18, Train Loss: 1.4462758553655524\n",
      "Step: 19, Train Loss: 1.4329681813716888\n",
      "Step: 20, Train Loss: 1.4306757279804774\n",
      "Step: 21, Train Loss: 1.423410865393552\n",
      "Step: 22, Train Loss: 1.4256389866704526\n",
      "Step: 23, Train Loss: 1.4093513488769531\n",
      "Step: 24, Train Loss: 1.408671908378601\n",
      "Step: 25, Train Loss: 1.410955745440263\n",
      "Step: 26, Train Loss: 1.410574237505595\n",
      "Step: 27, Train Loss: 1.4091510261808122\n",
      "Step: 28, Train Loss: 1.3964860398193886\n",
      "Step: 29, Train Loss: 1.3855899413426718\n",
      "Step: 30, Train Loss: 1.3809268820670344\n",
      "Step: 31, Train Loss: 1.3679469395428896\n",
      "Step: 32, Train Loss: 1.3614248163772351\n",
      "Step: 33, Train Loss: 1.3598263666910284\n",
      "Step: 34, Train Loss: 1.3564396807125636\n",
      "Step: 35, Train Loss: 1.3724475221501455\n",
      "Step: 36, Train Loss: 1.3715215357574257\n",
      "Step: 37, Train Loss: 1.3477462428180795\n",
      "Step: 38, Train Loss: 1.3440815661198053\n",
      "Step: 39, Train Loss: 1.3428942404687405\n",
      "Step: 40, Train Loss: 1.3282824936436444\n",
      "Step: 41, Train Loss: 1.326574981922195\n",
      "Step: 42, Train Loss: 1.3257187168265498\n",
      "Step: 43, Train Loss: 1.320274023169821\n",
      "Step: 44, Train Loss: 1.314383637242847\n",
      "Step: 45, Train Loss: 1.3122661223878032\n",
      "Step: 46, Train Loss: 1.3101405571115778\n",
      "Step: 47, Train Loss: 1.3070739737401407\n",
      "Step: 48, Train Loss: 1.3000895155935872\n",
      "Step: 49, Train Loss: 1.2983135038614273\n",
      "Step: 50, Train Loss: 1.294317867826013\n",
      "Step: 51, Train Loss: 1.2954446633274739\n",
      "Step: 52, Train Loss: 1.2918645329070542\n",
      "Step: 53, Train Loss: 1.2879083382862586\n",
      "Step: 54, Train Loss: 1.2824238880114123\n",
      "Step: 55, Train Loss: 1.280965142484222\n",
      "Step: 56, Train Loss: 1.2801197322837092\n",
      "Step: 57, Train Loss: 1.2837355953866039\n",
      "Step: 58, Train Loss: 1.2957855195312176\n",
      "Step: 59, Train Loss: 1.293775742749373\n",
      "Step: 60, Train Loss: 1.2868206838115317\n",
      "Step: 61, Train Loss: 1.2858117054547034\n",
      "Step: 62, Train Loss: 1.28248344953098\n",
      "Step: 63, Train Loss: 1.3001206782646477\n",
      "Step: 64, Train Loss: 1.2967042652460246\n",
      "Step: 65, Train Loss: 1.295629108042428\n",
      "Step: 66, Train Loss: 1.2959909443535023\n",
      "Step: 67, Train Loss: 1.2915422193267767\n",
      "Step: 68, Train Loss: 1.2884307767170062\n",
      "Step: 69, Train Loss: 1.2914544578109468\n",
      "Step: 70, Train Loss: 1.2883957049376529\n",
      "Step: 71, Train Loss: 1.2860290569563706\n",
      "Step: 72, Train Loss: 1.28165519931545\n",
      "Step: 73, Train Loss: 1.2793406574307262\n",
      "Step: 74, Train Loss: 1.276192706823349\n",
      "Step: 75, Train Loss: 1.2769503220915794\n",
      "Step: 76, Train Loss: 1.2705777159758977\n",
      "Step: 77, Train Loss: 1.2693869444804313\n",
      "Step: 78, Train Loss: 1.2646544556828994\n",
      "Step: 79, Train Loss: 1.2611535627394914\n",
      "Step: 80, Train Loss: 1.2537584514529616\n",
      "Step: 81, Train Loss: 1.2575419083601092\n",
      "Step: 82, Train Loss: 1.2606907784938812\n",
      "Step: 83, Train Loss: 1.2618272719638688\n",
      "Step: 84, Train Loss: 1.2603976758087383\n",
      "Step: 85, Train Loss: 1.2564281512831532\n",
      "Step: 86, Train Loss: 1.2546274905232178\n",
      "Step: 87, Train Loss: 1.2552323202517899\n",
      "Step: 88, Train Loss: 1.2521736129616083\n",
      "Step: 89, Train Loss: 1.2532618459728029\n",
      "Step: 90, Train Loss: 1.2513242544053675\n",
      "Step: 91, Train Loss: 1.2482971850296725\n",
      "Step: 92, Train Loss: 1.2490442443278529\n",
      "Step: 93, Train Loss: 1.250286880008718\n",
      "Step: 94, Train Loss: 1.2498326668613835\n",
      "Step: 95, Train Loss: 1.2487509874626994\n",
      "Step: 96, Train Loss: 1.2496629686085219\n",
      "Step: 97, Train Loss: 1.2464248078818223\n",
      "Step: 98, Train Loss: 1.245769231307386\n",
      "Step: 99, Train Loss: 1.2452101507782936\n",
      "Step: 100, Train Loss: 1.2442831830813152\n",
      "Step: 101, Train Loss: 1.2460737523494982\n",
      "Step: 102, Train Loss: 1.2422935959783572\n",
      "Step: 103, Train Loss: 1.2398910634219646\n",
      "Step: 104, Train Loss: 1.2369870347636087\n",
      "Step: 105, Train Loss: 1.2315289780216396\n",
      "Step: 106, Train Loss: 1.2291701043320593\n",
      "Step: 107, Train Loss: 1.230518583070349\n",
      "Step: 108, Train Loss: 1.2310435413767438\n",
      "Step: 109, Train Loss: 1.2318548541177403\n",
      "Step: 110, Train Loss: 1.228814080223307\n",
      "Step: 111, Train Loss: 1.2288345196949584\n",
      "Step: 112, Train Loss: 1.2283945750873702\n",
      "Step: 113, Train Loss: 1.2256812806192197\n",
      "Step: 114, Train Loss: 1.2272804983284162\n",
      "Step: 115, Train Loss: 1.2231158000128022\n",
      "Step: 116, Train Loss: 1.2230902219939435\n",
      "Step: 117, Train Loss: 1.2234871056625398\n",
      "Step: 118, Train Loss: 1.2249540503786391\n",
      "Step: 119, Train Loss: 1.22321915452679\n",
      "Step: 120, Train Loss: 1.2260526844785233\n",
      "Step: 121, Train Loss: 1.2290035635232925\n",
      "Step: 122, Train Loss: 1.228465781221545\n",
      "Step: 123, Train Loss: 1.2253405434950706\n",
      "Step: 124, Train Loss: 1.2237527697086334\n",
      "Step: 125, Train Loss: 1.2257520435821443\n",
      "Step: 126, Train Loss: 1.227205736665275\n",
      "Step: 127, Train Loss: 1.2273514077533036\n",
      "Step: 128, Train Loss: 1.227302037699278\n",
      "Step: 129, Train Loss: 1.2252213365756548\n",
      "Step: 130, Train Loss: 1.2237532737145898\n",
      "Step: 131, Train Loss: 1.2223996310071512\n",
      "Step: 132, Train Loss: 1.2231698948189729\n",
      "Step: 133, Train Loss: 1.2230757686184413\n",
      "Step: 134, Train Loss: 1.2203331185711754\n",
      "Step: 135, Train Loss: 1.2224528690909637\n",
      "Step: 136, Train Loss: 1.2236206546752122\n",
      "Step: 137, Train Loss: 1.2231315536343532\n",
      "Step: 138, Train Loss: 1.223779566639619\n",
      "Step: 139, Train Loss: 1.2235056636588915\n",
      "Step: 140, Train Loss: 1.2238465841357589\n",
      "Step: 141, Train Loss: 1.2229228563291925\n",
      "Step: 142, Train Loss: 1.2221616212721471\n",
      "Step: 143, Train Loss: 1.2219622555292315\n",
      "Step: 144, Train Loss: 1.223125205163298\n",
      "Step: 145, Train Loss: 1.2240455956899956\n",
      "Step: 146, Train Loss: 1.223632412500122\n",
      "Step: 147, Train Loss: 1.2236199433336388\n",
      "Step: 148, Train Loss: 1.2224472207111001\n",
      "Step: 149, Train Loss: 1.2230285547176998\n",
      "Step: 150, Train Loss: 1.221587316682007\n",
      "Step: 151, Train Loss: 1.2201083626009916\n",
      "Step: 152, Train Loss: 1.2204764086047029\n",
      "Step: 153, Train Loss: 1.221165985449568\n",
      "Step: 154, Train Loss: 1.2203369765512404\n",
      "Step: 155, Train Loss: 1.2204462123604922\n",
      "Step: 156, Train Loss: 1.2178988545943217\n",
      "Step: 157, Train Loss: 1.2157255035789707\n",
      "Step: 158, Train Loss: 1.2166169561305136\n",
      "Step: 159, Train Loss: 1.2151423709467053\n",
      "Step: 160, Train Loss: 1.2155192403689674\n",
      "Step: 161, Train Loss: 1.216719165444374\n",
      "Step: 162, Train Loss: 1.2182454018870745\n",
      "Step: 163, Train Loss: 1.2184108199506272\n",
      "Step: 164, Train Loss: 1.2175543664079724\n",
      "Step: 165, Train Loss: 1.2159615254186722\n",
      "Step: 166, Train Loss: 1.2123675680089139\n",
      "Step: 167, Train Loss: 1.2108368250940527\n",
      "Step: 168, Train Loss: 1.2129932488562793\n",
      "Step: 169, Train Loss: 1.2128443972152823\n",
      "Step: 170, Train Loss: 1.2128376272338175\n",
      "Step: 171, Train Loss: 1.211014902695667\n",
      "Step: 172, Train Loss: 1.2115717449284702\n",
      "Step: 173, Train Loss: 1.2113049282082196\n",
      "Step: 174, Train Loss: 1.2111840191909244\n",
      "Step: 175, Train Loss: 1.2109308444302191\n",
      "Step: 176, Train Loss: 1.213008622496815\n",
      "Step: 177, Train Loss: 1.2130345003993324\n",
      "Step: 178, Train Loss: 1.2121795873735204\n",
      "Step: 179, Train Loss: 1.2116591590974066\n",
      "Step: 180, Train Loss: 1.2106701581846944\n",
      "Step: 181, Train Loss: 1.2116302852774714\n",
      "Step: 182, Train Loss: 1.2114634635995647\n",
      "Step: 183, Train Loss: 1.2106359968690767\n",
      "Step: 184, Train Loss: 1.2107664867027386\n",
      "Step: 185, Train Loss: 1.2104785004931111\n",
      "Step: 186, Train Loss: 1.209237907818932\n",
      "Step: 187, Train Loss: 1.206109771227583\n",
      "Step: 188, Train Loss: 1.2064488484430564\n",
      "Step: 189, Train Loss: 1.207075693889668\n",
      "Step: 190, Train Loss: 1.2061912415851472\n",
      "Step: 191, Train Loss: 1.2050192702251177\n",
      "Step: 192, Train Loss: 1.2037466809230766\n",
      "Step: 193, Train Loss: 1.2034321828294046\n",
      "Step: 194, Train Loss: 1.203341250694715\n",
      "Step: 195, Train Loss: 1.2031607118492225\n",
      "Step: 196, Train Loss: 1.2035136430094084\n",
      "Step: 197, Train Loss: 1.202864778914837\n",
      "Step: 198, Train Loss: 1.2028603782905407\n",
      "Step: 199, Train Loss: 1.2026543606817723\n",
      "Step: 200, Train Loss: 1.2011323101188414\n",
      "Step: 201, Train Loss: 1.1979094243580752\n",
      "Step: 202, Train Loss: 1.1972145102000589\n",
      "Step: 203, Train Loss: 1.1966514594706834\n",
      "Step: 204, Train Loss: 1.1961426137424096\n",
      "Step: 205, Train Loss: 1.197344861707641\n",
      "Step: 206, Train Loss: 1.197646862210859\n",
      "Step: 207, Train Loss: 1.1974925658164115\n",
      "Step: 208, Train Loss: 1.1954835634767724\n",
      "Step: 209, Train Loss: 1.1948622639690127\n",
      "Step: 210, Train Loss: 1.1958460879834343\n",
      "Step: 211, Train Loss: 1.196851578382951\n",
      "Step: 212, Train Loss: 1.1955874057163096\n",
      "Step: 213, Train Loss: 1.1969180810395803\n",
      "Step: 214, Train Loss: 1.1969085195729898\n",
      "Step: 215, Train Loss: 1.1961048699363515\n",
      "Step: 216, Train Loss: 1.1943496538472065\n",
      "Step: 217, Train Loss: 1.1947414258477884\n",
      "Step: 218, Train Loss: 1.1945503521455478\n",
      "Step: 219, Train Loss: 1.194897683235732\n",
      "Step: 220, Train Loss: 1.193259979130456\n",
      "Step: 221, Train Loss: 1.193439929737701\n",
      "Step: 222, Train Loss: 1.1945133432411827\n",
      "Step: 223, Train Loss: 1.1939328481842364\n",
      "Step: 224, Train Loss: 1.1943658536010318\n",
      "Step: 225, Train Loss: 1.194769628675638\n",
      "Step: 226, Train Loss: 1.194273941711182\n",
      "Step: 227, Train Loss: 1.192995583409803\n",
      "Step: 228, Train Loss: 1.193031330426187\n",
      "Step: 229, Train Loss: 1.193395675394846\n",
      "Step: 230, Train Loss: 1.193365345398585\n",
      "Step: 231, Train Loss: 1.194853847525243\n",
      "Step: 232, Train Loss: 1.1954516808106663\n",
      "Step: 233, Train Loss: 1.1941168151604824\n",
      "Step: 234, Train Loss: 1.1950543338948107\n",
      "Step: 235, Train Loss: 1.1940068197199853\n",
      "Step: 236, Train Loss: 1.192693442106247\n",
      "Step: 237, Train Loss: 1.1896631460981208\n",
      "Step: 238, Train Loss: 1.1899056463311406\n",
      "Step: 239, Train Loss: 1.1875541718055804\n",
      "Step: 240, Train Loss: 1.1851187549933357\n",
      "Step: 241, Train Loss: 1.1850713778379536\n",
      "Step: 242, Train Loss: 1.1854793511545707\n",
      "Step: 243, Train Loss: 1.185544480187971\n",
      "Step: 244, Train Loss: 1.1852315281118666\n",
      "Step: 245, Train Loss: 1.1848333421034543\n",
      "Step: 246, Train Loss: 1.1840357639287646\n",
      "Step: 247, Train Loss: 1.1829789289303365\n",
      "Step: 248, Train Loss: 1.1822649320684762\n",
      "Step: 249, Train Loss: 1.1821474546194077\n",
      "Step: 250, Train Loss: 1.1822624732536149\n",
      "Step: 251, Train Loss: 1.1807507649064064\n",
      "Step: 252, Train Loss: 1.180617297473161\n",
      "Step: 253, Train Loss: 1.1794785299873727\n",
      "Step: 254, Train Loss: 1.1796660775063084\n",
      "Step: 255, Train Loss: 1.1780270534800366\n",
      "Step: 256, Train Loss: 1.1767770762805345\n",
      "Step: 257, Train Loss: 1.175975657133169\n",
      "Step: 258, Train Loss: 1.176437390241844\n",
      "Step: 259, Train Loss: 1.1737746035823455\n",
      "Step: 260, Train Loss: 1.1721957215175776\n",
      "Step: 261, Train Loss: 1.1713503759553414\n",
      "Step: 262, Train Loss: 1.1710822692162184\n",
      "Step: 263, Train Loss: 1.1709539972697243\n",
      "Step: 264, Train Loss: 1.169202088297538\n",
      "Step: 265, Train Loss: 1.1669724970159674\n",
      "Step: 266, Train Loss: 1.1668130656083424\n",
      "Step: 267, Train Loss: 1.1655754952955601\n",
      "Step: 268, Train Loss: 1.1646997663832952\n",
      "Step: 269, Train Loss: 1.164824304426158\n",
      "Step: 270, Train Loss: 1.1645993765209874\n",
      "Step: 271, Train Loss: 1.1652657395119177\n",
      "Step: 272, Train Loss: 1.1640982550360781\n",
      "Step: 273, Train Loss: 1.1641446844722232\n",
      "Step: 274, Train Loss: 1.16296088272875\n",
      "Step: 275, Train Loss: 1.1632883066068525\n",
      "Step: 276, Train Loss: 1.1626638781938312\n",
      "Step: 277, Train Loss: 1.1621008049026669\n",
      "Step: 278, Train Loss: 1.1619905213087691\n",
      "Step: 279, Train Loss: 1.1600609476012842\n",
      "Step: 280, Train Loss: 1.1597912064438614\n",
      "Step: 281, Train Loss: 1.1586557942713407\n",
      "Step: 282, Train Loss: 1.157528074069916\n",
      "Step: 283, Train Loss: 1.158067060713197\n",
      "Step: 284, Train Loss: 1.1572750525516375\n",
      "Step: 285, Train Loss: 1.1559057207582715\n",
      "Step: 286, Train Loss: 1.1563936111611357\n",
      "Step: 287, Train Loss: 1.156481602643099\n",
      "Step: 288, Train Loss: 1.1540925977139325\n",
      "Step: 289, Train Loss: 1.1535295667319463\n",
      "Step: 290, Train Loss: 1.1530716193910318\n",
      "Step: 291, Train Loss: 1.1532944224468649\n",
      "Step: 292, Train Loss: 1.1516277336423308\n",
      "Step: 293, Train Loss: 1.151029556584196\n",
      "Step: 294, Train Loss: 1.1511862538628659\n",
      "Step: 295, Train Loss: 1.14955314812628\n",
      "Step: 296, Train Loss: 1.149756382009397\n",
      "Step: 297, Train Loss: 1.1499364094046138\n",
      "Step: 298, Train Loss: 1.1504284544134618\n",
      "Step: 299, Train Loss: 1.150588132739067\n",
      "Step: 300, Train Loss: 1.148591676225694\n",
      "Step: 301, Train Loss: 1.1481757963335277\n",
      "Step: 302, Train Loss: 1.14838307899217\n",
      "Step: 303, Train Loss: 1.1482691951096058\n",
      "Step: 304, Train Loss: 1.1473772076309705\n",
      "Step: 305, Train Loss: 1.1459933620652343\n",
      "Step: 306, Train Loss: 1.1452201800936597\n",
      "Step: 307, Train Loss: 1.1453223551635618\n",
      "Step: 308, Train Loss: 1.1442258666634173\n",
      "Step: 309, Train Loss: 1.1440075001408976\n",
      "Step: 310, Train Loss: 1.1445759090196665\n",
      "Step: 311, Train Loss: 1.1449461162854464\n",
      "Step: 312, Train Loss: 1.1452318761295404\n",
      "Step: 313, Train Loss: 1.1459132896107473\n",
      "Step: 314, Train Loss: 1.1457515092123123\n",
      "Step: 315, Train Loss: 1.1455005328112011\n",
      "Step: 316, Train Loss: 1.1461500301722096\n",
      "Step: 317, Train Loss: 1.1473343844683666\n",
      "Step: 318, Train Loss: 1.1467776995467542\n",
      "Step: 319, Train Loss: 1.1465693550184368\n",
      "Step: 320, Train Loss: 1.1465408959864085\n",
      "Step: 321, Train Loss: 1.145960011097215\n",
      "Step: 322, Train Loss: 1.1453247140435612\n",
      "Step: 323, Train Loss: 1.1457207570841283\n",
      "Step: 324, Train Loss: 1.1446355241995592\n",
      "Step: 325, Train Loss: 1.1444578505366858\n",
      "Step: 326, Train Loss: 1.1451870051728112\n",
      "Step: 327, Train Loss: 1.1455896524030988\n",
      "Step: 328, Train Loss: 1.1451092963885392\n",
      "Step: 329, Train Loss: 1.1455724587946228\n",
      "Step: 330, Train Loss: 1.1460651719318056\n",
      "Step: 331, Train Loss: 1.1461040982639934\n",
      "Step: 332, Train Loss: 1.145416087395436\n",
      "Step: 333, Train Loss: 1.144993139062813\n",
      "Step: 334, Train Loss: 1.1439976206466334\n",
      "Step: 335, Train Loss: 1.142725604630652\n",
      "Step: 336, Train Loss: 1.1430857761677367\n",
      "Step: 337, Train Loss: 1.1434529843415029\n",
      "Step: 338, Train Loss: 1.1438433371462302\n",
      "Step: 339, Train Loss: 1.1435808956623077\n",
      "Step: 340, Train Loss: 1.144022246609685\n",
      "Step: 341, Train Loss: 1.1441493808177479\n",
      "Step: 342, Train Loss: 1.142596776909453\n",
      "Step: 343, Train Loss: 1.1428257393282513\n",
      "Step: 344, Train Loss: 1.1417634181354357\n",
      "Step: 345, Train Loss: 1.1412794490081037\n",
      "Step: 346, Train Loss: 1.1416664470513203\n",
      "Step: 347, Train Loss: 1.1409234469649436\n",
      "Step: 348, Train Loss: 1.1402646543986477\n",
      "Step: 349, Train Loss: 1.1397326736790794\n",
      "Step: 350, Train Loss: 1.1396172815917904\n",
      "Step: 351, Train Loss: 1.14050527069379\n",
      "Step: 352, Train Loss: 1.141989587555526\n",
      "Step: 353, Train Loss: 1.1406583205118017\n",
      "Step: 354, Train Loss: 1.1408146967350596\n",
      "Step: 355, Train Loss: 1.140298633595531\n",
      "Step: 356, Train Loss: 1.139962009188174\n",
      "Step: 357, Train Loss: 1.1399706284094122\n",
      "Step: 358, Train Loss: 1.139774140043179\n",
      "Step: 359, Train Loss: 1.139432647327582\n",
      "Step: 360, Train Loss: 1.1410095686727613\n",
      "Step: 361, Train Loss: 1.1407017179286283\n",
      "Step: 362, Train Loss: 1.1403901933310117\n",
      "Step: 363, Train Loss: 1.1397542023396754\n",
      "Step: 364, Train Loss: 1.1392416513129457\n",
      "Step: 365, Train Loss: 1.139548200401452\n",
      "Step: 366, Train Loss: 1.1396244832540403\n",
      "Step: 367, Train Loss: 1.1395111462992171\n",
      "Step: 368, Train Loss: 1.1395378917213377\n",
      "Step: 369, Train Loss: 1.138981198459058\n",
      "Step: 370, Train Loss: 1.1382661161397023\n",
      "Step: 371, Train Loss: 1.1391846422226197\n",
      "Step: 372, Train Loss: 1.1395335421165895\n",
      "Step: 373, Train Loss: 1.1400023752992803\n",
      "Step: 374, Train Loss: 1.1406334362030028\n",
      "Step: 375, Train Loss: 1.1400618672053864\n",
      "Step: 376, Train Loss: 1.1405878286779085\n",
      "Step: 377, Train Loss: 1.1399043727488745\n",
      "Step: 378, Train Loss: 1.1398536930927186\n",
      "Step: 379, Train Loss: 1.1387867918140009\n",
      "Step: 380, Train Loss: 1.1379484169752265\n",
      "Step: 381, Train Loss: 1.1379111867924636\n",
      "Step: 382, Train Loss: 1.1373994422021174\n",
      "Step: 383, Train Loss: 1.1371177649125457\n",
      "Step: 384, Train Loss: 1.136799566467087\n",
      "Step: 385, Train Loss: 1.1360531611454918\n",
      "Step: 386, Train Loss: 1.1366908182161415\n",
      "Step: 387, Train Loss: 1.136591625121451\n",
      "Step: 388, Train Loss: 1.1371383699169502\n",
      "Step: 389, Train Loss: 1.1370849681206239\n",
      "Step: 390, Train Loss: 1.137426928028731\n",
      "Step: 391, Train Loss: 1.1373787390030161\n",
      "Step: 392, Train Loss: 1.1386266626777843\n",
      "Step: 393, Train Loss: 1.1393398060108804\n",
      "Step: 394, Train Loss: 1.1387020412879654\n",
      "Step: 395, Train Loss: 1.1387687978720424\n",
      "Step: 396, Train Loss: 1.1384719895475757\n",
      "Step: 397, Train Loss: 1.1390271165862156\n",
      "Step: 398, Train Loss: 1.1388308948143981\n",
      "Step: 399, Train Loss: 1.1390411546826362\n",
      "Step: 400, Train Loss: 1.1374714567774251\n",
      "Step: 401, Train Loss: 1.136881879757886\n",
      "Step: 402, Train Loss: 1.13668334587632\n",
      "Step: 403, Train Loss: 1.1360659264396913\n",
      "Step: 404, Train Loss: 1.1344727223302111\n",
      "Step: 405, Train Loss: 1.1338176756656815\n",
      "Step: 406, Train Loss: 1.133321178926004\n",
      "Step: 407, Train Loss: 1.133783961335818\n",
      "Step: 408, Train Loss: 1.133360974508859\n",
      "Step: 409, Train Loss: 1.1341409402649576\n",
      "Step: 410, Train Loss: 1.1341141373861736\n",
      "Step: 411, Train Loss: 1.1342719540144632\n",
      "Step: 412, Train Loss: 1.1330956392369027\n",
      "Step: 413, Train Loss: 1.1334559749002042\n",
      "Step: 414, Train Loss: 1.133502922431532\n",
      "Step: 415, Train Loss: 1.133940557591044\n",
      "Step: 416, Train Loss: 1.132994926804833\n",
      "Step: 417, Train Loss: 1.1325741351791545\n",
      "Step: 418, Train Loss: 1.132478616260401\n",
      "Step: 419, Train Loss: 1.1327826030197599\n",
      "Step: 420, Train Loss: 1.1322252058076745\n",
      "Step: 421, Train Loss: 1.1328382629071367\n",
      "Step: 422, Train Loss: 1.1323882362521287\n",
      "Step: 423, Train Loss: 1.1329711907595958\n",
      "Step: 424, Train Loss: 1.1324271378797643\n",
      "Step: 425, Train Loss: 1.1323954365063162\n",
      "Step: 426, Train Loss: 1.1319173700078031\n",
      "Step: 427, Train Loss: 1.131236678807535\n",
      "Step: 428, Train Loss: 1.131491784727101\n",
      "Step: 429, Train Loss: 1.132125947641772\n",
      "Step: 430, Train Loss: 1.1322203727996543\n",
      "Step: 431, Train Loss: 1.1323316284903773\n",
      "Step: 432, Train Loss: 1.1326053043435957\n",
      "Step: 433, Train Loss: 1.1316557054146095\n",
      "Step: 434, Train Loss: 1.1316847965635102\n",
      "Step: 435, Train Loss: 1.131353844743256\n",
      "Step: 436, Train Loss: 1.1311183035782872\n",
      "Step: 437, Train Loss: 1.1313996617108175\n",
      "Step: 438, Train Loss: 1.1320208365390403\n",
      "Step: 439, Train Loss: 1.131658115441149\n",
      "Step: 440, Train Loss: 1.132198913838047\n",
      "Step: 441, Train Loss: 1.1322581908282112\n",
      "Step: 442, Train Loss: 1.1322655258006489\n",
      "Step: 443, Train Loss: 1.1336459936322392\n",
      "Step: 444, Train Loss: 1.1338057426924115\n",
      "Step: 445, Train Loss: 1.1333262249493279\n",
      "Step: 446, Train Loss: 1.1330410370090664\n",
      "Step: 447, Train Loss: 1.1335526167282037\n",
      "Step: 448, Train Loss: 1.1328402862516969\n",
      "Step: 449, Train Loss: 1.1330754723813798\n",
      "Step: 450, Train Loss: 1.133076613996087\n",
      "Step: 451, Train Loss: 1.132298926072838\n",
      "Step: 452, Train Loss: 1.131273558334536\n",
      "Step: 453, Train Loss: 1.1316604083855246\n",
      "Step: 454, Train Loss: 1.1311781190253876\n",
      "Step: 455, Train Loss: 1.1314979685764563\n",
      "Step: 456, Train Loss: 1.1308811490853976\n",
      "Step: 457, Train Loss: 1.1302667015504628\n",
      "Step: 458, Train Loss: 1.1299114819445641\n",
      "Step: 459, Train Loss: 1.1292985456145328\n",
      "Step: 460, Train Loss: 1.128421270924897\n",
      "Step: 461, Train Loss: 1.128166686662864\n",
      "Step: 462, Train Loss: 1.127086798521664\n",
      "Step: 463, Train Loss: 1.1269809023059647\n",
      "Step: 464, Train Loss: 1.126170926452965\n",
      "Step: 465, Train Loss: 1.1256566208831231\n",
      "Step: 466, Train Loss: 1.1251936242207725\n",
      "Step: 467, Train Loss: 1.124874150651133\n",
      "Step: 468, Train Loss: 1.1254732964643792\n",
      "Step: 469, Train Loss: 1.1249095276315162\n",
      "Step: 470, Train Loss: 1.1251503063361863\n",
      "Step: 471, Train Loss: 1.124781450351416\n",
      "Step: 472, Train Loss: 1.1244389625230753\n",
      "Step: 473, Train Loss: 1.1247490501102013\n",
      "Step: 474, Train Loss: 1.124067542552948\n",
      "Step: 475, Train Loss: 1.123821731500265\n",
      "Step: 476, Train Loss: 1.1247911944329363\n",
      "Step: 477, Train Loss: 1.1248911036856504\n",
      "Step: 478, Train Loss: 1.1245472902793725\n",
      "Step: 479, Train Loss: 1.124472500011325\n",
      "Step: 480, Train Loss: 1.125121652584314\n",
      "Step: 481, Train Loss: 1.1243428103162045\n",
      "Step: 482, Train Loss: 1.1246477387706686\n",
      "Step: 483, Train Loss: 1.1245101675514346\n",
      "Step: 484, Train Loss: 1.1240656184166977\n",
      "Step: 485, Train Loss: 1.1230420805053947\n",
      "Step: 486, Train Loss: 1.1236503884288076\n",
      "Step: 487, Train Loss: 1.1238098077353884\n",
      "Step: 488, Train Loss: 1.1238956537958547\n",
      "Step: 489, Train Loss: 1.1229491391960456\n",
      "Step: 490, Train Loss: 1.1228987061079068\n",
      "Step: 491, Train Loss: 1.123296337883647\n",
      "Step: 492, Train Loss: 1.1236633782938092\n",
      "Step: 493, Train Loss: 1.124008481319134\n",
      "Step: 494, Train Loss: 1.1242352044943607\n",
      "Step: 495, Train Loss: 1.1236939594870614\n",
      "Step: 496, Train Loss: 1.123800206832003\n",
      "Step: 497, Train Loss: 1.1234207663191371\n",
      "Step: 498, Train Loss: 1.1232828305575078\n",
      "Step: 499, Train Loss: 1.123432347536087\n",
      "Step: 500, Train Loss: 1.124146779616198\n",
      "Step: 501, Train Loss: 1.1245216677388348\n",
      "Step: 502, Train Loss: 1.1246954574736638\n",
      "Step: 503, Train Loss: 1.1246828001642983\n",
      "Step: 504, Train Loss: 1.1233420175490993\n",
      "Step: 505, Train Loss: 1.1229643175606672\n",
      "Step: 506, Train Loss: 1.1227526142164566\n",
      "Step: 507, Train Loss: 1.1222429388738053\n",
      "Step: 508, Train Loss: 1.122077260890972\n",
      "Step: 509, Train Loss: 1.1215874981062084\n",
      "Step: 510, Train Loss: 1.1218476890934423\n",
      "Step: 511, Train Loss: 1.1211222318815999\n",
      "Step: 512, Train Loss: 1.1213059559551597\n",
      "Step: 513, Train Loss: 1.1210547802165325\n",
      "Step: 514, Train Loss: 1.1193811932813773\n",
      "Step: 515, Train Loss: 1.119274451289066\n",
      "Step: 516, Train Loss: 1.119841510368731\n",
      "Step: 517, Train Loss: 1.1197026153328795\n",
      "Step: 518, Train Loss: 1.119739682458499\n",
      "Step: 519, Train Loss: 1.120123632137592\n",
      "Step: 520, Train Loss: 1.1199820384869419\n",
      "Step: 521, Train Loss: 1.1198111852466832\n",
      "Step: 522, Train Loss: 1.1194438777051965\n",
      "Step: 523, Train Loss: 1.1198417744563736\n",
      "Step: 524, Train Loss: 1.1186961063884553\n",
      "Step: 525, Train Loss: 1.1191359306242983\n",
      "Step: 526, Train Loss: 1.1184453043584806\n",
      "Step: 527, Train Loss: 1.1183447684302474\n",
      "Step: 528, Train Loss: 1.118083540681179\n",
      "Step: 529, Train Loss: 1.1183665386910708\n",
      "Step: 530, Train Loss: 1.1177794992811487\n",
      "Step: 531, Train Loss: 1.1166287458928905\n",
      "Step: 532, Train Loss: 1.1159826504803956\n",
      "Step: 533, Train Loss: 1.1165259769570068\n",
      "Step: 534, Train Loss: 1.116551636201199\n",
      "Step: 535, Train Loss: 1.1162850589894537\n",
      "Step: 536, Train Loss: 1.115980455440294\n",
      "Step: 537, Train Loss: 1.1154837093167145\n",
      "Step: 538, Train Loss: 1.1163051321909025\n",
      "Step: 539, Train Loss: 1.1155888492310488\n",
      "Step: 540, Train Loss: 1.1152951562030038\n",
      "Step: 541, Train Loss: 1.1148643477156592\n",
      "Step: 542, Train Loss: 1.1152964765634765\n",
      "Step: 543, Train Loss: 1.1153382351074148\n",
      "Step: 544, Train Loss: 1.115430914918217\n",
      "Step: 545, Train Loss: 1.1151290700767502\n",
      "Step: 546, Train Loss: 1.115003101459587\n",
      "Step: 547, Train Loss: 1.1147333220408782\n",
      "Step: 548, Train Loss: 1.1146984102513187\n",
      "Step: 549, Train Loss: 1.1142380250583996\n",
      "Step: 550, Train Loss: 1.11524319194406\n",
      "Step: 551, Train Loss: 1.1152054358651673\n",
      "Step: 552, Train Loss: 1.1148947448238973\n",
      "Step: 553, Train Loss: 1.1150452667840551\n",
      "Step: 554, Train Loss: 1.1153162360191344\n",
      "Step: 555, Train Loss: 1.1170392633556463\n",
      "Step: 556, Train Loss: 1.1177458753491647\n",
      "Step: 557, Train Loss: 1.1170582477550781\n",
      "Step: 558, Train Loss: 1.117671363149005\n",
      "Step: 559, Train Loss: 1.1168087784733092\n",
      "Step: 560, Train Loss: 1.1166637387930178\n",
      "Step: 561, Train Loss: 1.1165371168126415\n",
      "Step: 562, Train Loss: 1.1164055340666965\n",
      "Step: 563, Train Loss: 1.1168385828640444\n",
      "Step: 564, Train Loss: 1.1163049727414562\n",
      "Step: 565, Train Loss: 1.1164262499067892\n",
      "Step: 566, Train Loss: 1.1156300506053567\n",
      "Step: 567, Train Loss: 1.1161851563923795\n",
      "Step: 568, Train Loss: 1.1165816819521162\n",
      "Step: 569, Train Loss: 1.1164865460312157\n",
      "Step: 570, Train Loss: 1.116481320870528\n",
      "Step: 571, Train Loss: 1.1165265622255685\n",
      "Step: 572, Train Loss: 1.1169665974888294\n",
      "Step: 573, Train Loss: 1.1161363715701818\n",
      "Step: 574, Train Loss: 1.1154741798276486\n",
      "Step: 575, Train Loss: 1.1156368055898283\n",
      "Step: 576, Train Loss: 1.1152184913435264\n",
      "Step: 577, Train Loss: 1.1151589057231033\n",
      "Step: 578, Train Loss: 1.1153645870599105\n",
      "Step: 579, Train Loss: 1.1154735588821871\n",
      "Step: 580, Train Loss: 1.1158179923917668\n",
      "Step: 581, Train Loss: 1.1154538734057515\n",
      "Step: 582, Train Loss: 1.1155542054470775\n",
      "Step: 583, Train Loss: 1.1163665204219622\n",
      "Step: 584, Train Loss: 1.115498993335626\n",
      "Step: 585, Train Loss: 1.1142726211519371\n",
      "Step: 586, Train Loss: 1.114218199324161\n",
      "Step: 587, Train Loss: 1.114005131938425\n",
      "Step: 588, Train Loss: 1.114147177121028\n",
      "Step: 589, Train Loss: 1.1145884525472836\n",
      "Step: 590, Train Loss: 1.1144246356640573\n",
      "Step: 591, Train Loss: 1.1142087003367174\n",
      "Step: 592, Train Loss: 1.114596144797227\n",
      "Step: 593, Train Loss: 1.1145063599051048\n",
      "Step: 594, Train Loss: 1.114601063377717\n",
      "Step: 595, Train Loss: 1.1146210182533167\n",
      "Step: 596, Train Loss: 1.1145261117261858\n",
      "Step: 597, Train Loss: 1.1144280332486367\n",
      "Step: 598, Train Loss: 1.1146388603669772\n",
      "Step: 599, Train Loss: 1.1146406104664008\n",
      "Step: 600, Train Loss: 1.1145336955736163\n",
      "Step: 601, Train Loss: 1.114269292087254\n",
      "Step: 602, Train Loss: 1.1142822180320178\n",
      "Step: 603, Train Loss: 1.114567728873515\n",
      "Step: 604, Train Loss: 1.114626266394765\n",
      "Step: 605, Train Loss: 1.1141241301502725\n",
      "Step: 606, Train Loss: 1.113809809997015\n",
      "Step: 607, Train Loss: 1.1137142839694494\n",
      "Step: 608, Train Loss: 1.113381468848446\n",
      "Step: 609, Train Loss: 1.1127863127188604\n",
      "Step: 610, Train Loss: 1.1128536743247568\n",
      "Step: 611, Train Loss: 1.1129557708022642\n",
      "Step: 612, Train Loss: 1.1122793817306227\n",
      "Step: 613, Train Loss: 1.1127220190794538\n",
      "Step: 614, Train Loss: 1.1123215182525357\n",
      "Step: 615, Train Loss: 1.1124078367727916\n",
      "Step: 616, Train Loss: 1.1125686421290015\n",
      "Step: 617, Train Loss: 1.112524996249421\n",
      "Step: 618, Train Loss: 1.1125146100035777\n",
      "Step: 619, Train Loss: 1.1127559511411576\n",
      "Step: 620, Train Loss: 1.1128137408727226\n",
      "Step: 621, Train Loss: 1.1117678452343036\n",
      "Step: 622, Train Loss: 1.1113619202595462\n",
      "Step: 623, Train Loss: 1.111725174750273\n",
      "Step: 624, Train Loss: 1.1113338101387025\n",
      "Step: 625, Train Loss: 1.1114703225442015\n",
      "Step: 626, Train Loss: 1.1112935104818815\n",
      "Step: 627, Train Loss: 1.1103777002756763\n",
      "Step: 628, Train Loss: 1.1102668136026597\n",
      "Step: 629, Train Loss: 1.110294392563048\n",
      "Step: 630, Train Loss: 1.1100073517219011\n",
      "Step: 631, Train Loss: 1.1094490429268609\n",
      "Step: 632, Train Loss: 1.109960346990287\n",
      "Step: 633, Train Loss: 1.109435907861788\n",
      "Step: 634, Train Loss: 1.1096893188521617\n",
      "Step: 635, Train Loss: 1.109755074453054\n",
      "Step: 636, Train Loss: 1.1100993354803352\n",
      "Step: 637, Train Loss: 1.1095922611723872\n",
      "Step: 638, Train Loss: 1.1101256691047656\n",
      "Step: 639, Train Loss: 1.1100695364177227\n",
      "Step: 640, Train Loss: 1.1103179977390212\n",
      "Step: 641, Train Loss: 1.1101710137177108\n",
      "Step: 642, Train Loss: 1.110339448945148\n",
      "Step: 643, Train Loss: 1.1106567897411608\n",
      "Step: 644, Train Loss: 1.110490287551584\n",
      "Step: 645, Train Loss: 1.1102442460901596\n",
      "Step: 646, Train Loss: 1.1102761470552942\n",
      "Step: 647, Train Loss: 1.1099823305820242\n",
      "Step: 648, Train Loss: 1.1102186589285112\n",
      "Step: 649, Train Loss: 1.1104876797932846\n",
      "Step: 650, Train Loss: 1.110328610316949\n",
      "Step: 651, Train Loss: 1.1101213156994134\n",
      "Step: 652, Train Loss: 1.1104789630012162\n",
      "Step: 653, Train Loss: 1.1104680222291101\n",
      "Step: 654, Train Loss: 1.1102348131987885\n",
      "Step: 655, Train Loss: 1.1101310215890408\n",
      "Step: 656, Train Loss: 1.1102366286870007\n",
      "Step: 657, Train Loss: 1.1099154863132894\n",
      "Step: 658, Train Loss: 1.110989726096256\n",
      "Step: 659, Train Loss: 1.1110689586762226\n",
      "Step: 660, Train Loss: 1.1107736770937193\n",
      "Step: 661, Train Loss: 1.1108304073983448\n",
      "Step: 662, Train Loss: 1.1107481880784933\n",
      "Step: 663, Train Loss: 1.1099022691508373\n",
      "Step: 664, Train Loss: 1.1095046678880103\n",
      "Step: 665, Train Loss: 1.1091249473281093\n",
      "Step: 666, Train Loss: 1.1089879929393842\n",
      "Step: 667, Train Loss: 1.108911971221427\n",
      "Step: 668, Train Loss: 1.1076932008223683\n",
      "Step: 669, Train Loss: 1.1078983904265647\n",
      "Step: 670, Train Loss: 1.1072810785692246\n",
      "Step: 671, Train Loss: 1.106879132977199\n",
      "Step: 672, Train Loss: 1.1069170609891503\n",
      "Step: 673, Train Loss: 1.1068734186604752\n",
      "Step: 674, Train Loss: 1.1074732359691903\n",
      "Step: 675, Train Loss: 1.10722115622646\n",
      "Step: 676, Train Loss: 1.106650658909292\n",
      "Step: 677, Train Loss: 1.1065597821213855\n",
      "Step: 678, Train Loss: 1.1063367342123753\n",
      "Step: 679, Train Loss: 1.1063846296685584\n",
      "Step: 680, Train Loss: 1.106159658290216\n",
      "Step: 681, Train Loss: 1.1067281188415992\n",
      "Step: 682, Train Loss: 1.1062283019806596\n",
      "Step: 683, Train Loss: 1.1059662260071577\n",
      "Step: 684, Train Loss: 1.1065808859619781\n",
      "Step: 685, Train Loss: 1.106516030674078\n",
      "Step: 686, Train Loss: 1.10707604394903\n",
      "Step: 687, Train Loss: 1.1072998909323022\n",
      "Step: 688, Train Loss: 1.1072333886332713\n",
      "Step: 689, Train Loss: 1.1066762772591219\n",
      "Step: 690, Train Loss: 1.106702306034251\n",
      "Step: 691, Train Loss: 1.1068765195894104\n",
      "Step: 692, Train Loss: 1.1067625556415293\n",
      "Step: 693, Train Loss: 1.1063985910041187\n",
      "Step: 694, Train Loss: 1.1064595539364026\n",
      "Step: 695, Train Loss: 1.1063885232754822\n",
      "Step: 696, Train Loss: 1.1063312823580875\n",
      "Step: 697, Train Loss: 1.1064257597427998\n",
      "Step: 698, Train Loss: 1.1065411117029123\n",
      "Step: 699, Train Loss: 1.1064079720207622\n",
      "Step: 700, Train Loss: 1.1065664542639646\n",
      "Step: 701, Train Loss: 1.1065333787447367\n",
      "Step: 702, Train Loss: 1.1062529444440161\n",
      "Step: 703, Train Loss: 1.1064409879036248\n",
      "Step: 704, Train Loss: 1.1061004517348945\n",
      "Step: 705, Train Loss: 1.1057673666953365\n",
      "Step: 706, Train Loss: 1.105582462805501\n",
      "Step: 707, Train Loss: 1.105617086218912\n",
      "Step: 708, Train Loss: 1.1053367899822752\n",
      "Step: 709, Train Loss: 1.1049448315106647\n",
      "Step: 710, Train Loss: 1.1041529687061256\n",
      "Step: 711, Train Loss: 1.1039996321700263\n",
      "Step: 712, Train Loss: 1.1041411058180464\n",
      "Step: 713, Train Loss: 1.1043588392624335\n",
      "Step: 714, Train Loss: 1.1042641496741687\n",
      "Step: 715, Train Loss: 1.104801975357133\n",
      "Step: 716, Train Loss: 1.1046754835456651\n",
      "Step: 717, Train Loss: 1.1043739095122702\n",
      "Step: 718, Train Loss: 1.104142235490974\n",
      "Step: 719, Train Loss: 1.1045576468524005\n",
      "Step: 720, Train Loss: 1.1043946755825234\n",
      "Step: 721, Train Loss: 1.1047806077303979\n",
      "Step: 722, Train Loss: 1.1050351156980665\n",
      "Step: 723, Train Loss: 1.1051908263432386\n",
      "Step: 724, Train Loss: 1.1053755731829282\n",
      "Step: 725, Train Loss: 1.1050838223419899\n",
      "Step: 726, Train Loss: 1.1049994759497321\n",
      "Step: 727, Train Loss: 1.1044087515181893\n",
      "Step: 728, Train Loss: 1.104144807991831\n",
      "Step: 729, Train Loss: 1.1040647617349886\n",
      "Step: 730, Train Loss: 1.1039788484491826\n",
      "Step: 731, Train Loss: 1.104027408841855\n",
      "Step: 732, Train Loss: 1.1038259995178832\n",
      "Step: 733, Train Loss: 1.1035329040535788\n",
      "Step: 734, Train Loss: 1.1036936290004626\n",
      "Step: 735, Train Loss: 1.10379128113551\n",
      "Step: 736, Train Loss: 1.103848153935035\n",
      "Step: 737, Train Loss: 1.1033140790414036\n",
      "Step: 738, Train Loss: 1.1031684917832583\n",
      "Step: 739, Train Loss: 1.103479343048624\n",
      "Step: 740, Train Loss: 1.103183081798386\n",
      "Step: 741, Train Loss: 1.1035023568213147\n",
      "Step: 742, Train Loss: 1.1034543038136546\n",
      "Step: 743, Train Loss: 1.1036680044746527\n",
      "Step: 744, Train Loss: 1.103683087529752\n",
      "Step: 745, Train Loss: 1.1035083813178315\n",
      "Step: 746, Train Loss: 1.1035946868908613\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 116.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 110.94 MiB is free. Including non-PyTorch memory, this process has 46.42 GiB memory in use. Process 2164997 has 1016.00 MiB memory in use. Of the allocated memory 44.65 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m curr \u001b[38;5;241m=\u001b[39m memall()\n\u001b[1;32m     27\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m forward_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     30\u001b[0m activation_memory \u001b[38;5;241m=\u001b[39m memall() \u001b[38;5;241m-\u001b[39m curr\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:421\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    419\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    420\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    424\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:216\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/activations.py:150\u001b[0m, in \u001b[0;36mSiLUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/functional.py:2072\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 110.94 MiB is free. Including non-PyTorch memory, this process has 46.42 GiB memory in use. Process 2164997 has 1016.00 MiB memory in use. Of the allocated memory 44.65 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "times = []\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "optimizer.zero_grad()\n",
    "optimizer_memory = 0\n",
    "forward_time = 0\n",
    "backward_time = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_loss = 0\n",
    "    tr_steps = 0\n",
    "    tick = 0\n",
    "    total_time = 0\n",
    "    for step, batch in enumerate((train_dataloader)):\n",
    "\n",
    "        tick = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        curr = memall()\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        input_memory = memall() - curr\n",
    "        \n",
    "        curr = memall()\n",
    "        start = time.time()\n",
    "        output = model(**batch)\n",
    "        forward_time += time.time() - start\n",
    "        activation_memory = memall() - curr\n",
    "        \n",
    "        curr = memall()\n",
    "        start = time.time()\n",
    "        # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        backward_time += time.time() - start\n",
    "        gradient_memory = memall() - input_memory - weight_memory - optimizer_memory\n",
    "\n",
    "        curr = memall()\n",
    "        optimizer.step()\n",
    "        if step == 0:\n",
    "             optimizer_memory = memall() - curr\n",
    "    \n",
    "        loss = loss.cpu()\n",
    "        train_loss += loss.item()\n",
    "        tr_steps += 1\n",
    "        train_losses.append(train_loss/tr_steps)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "        torch.cuda.empty_cache()\n",
    "        total_time += time.time() - tick\n",
    "        times.append(total_time)\n",
    "        if step == args.max_steps:\n",
    "            model.eval()\n",
    "            break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/125 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =     1.1208\n",
      "  eval_runtime            = 0:01:07.76\n",
      "  eval_samples_per_second =     14.756\n",
      "  eval_steps_per_second   =      1.845\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.eval()\n",
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = sum([max_memory_allocated(i) for i in range(gpus)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': './output',\n",
       " 'eval_loss': 1.120755672454834,\n",
       " 'eval_runtime': 67.7672,\n",
       " 'eval_samples_per_second': 14.756,\n",
       " 'eval_steps_per_second': 1.845}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 265.29M\n",
      "(6738.423808, 265.293824)\n",
      "Method           : alpha\n",
      "Layers           : 8\n",
      "Learning Rate    : 2e-06\n",
      "Batch size       : 4\n",
      "Weight memory    : 27087.929344 MB\n",
      "Optimizer memory : 2122.350592 MB\n",
      "Activation memory: 2715.99616 MB\n",
      "Gradient memory  : 1155.809792 MB\n",
      "Input memory     : 0.011264 MB\n",
      "Total memory     : 29304.925696 MB\n",
      "Peak memory      : 45615.481344 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "memory_string = (\n",
    "        f\"{param_count(model)}\\n\"\n",
    "        f\"Dataset          : {args.dataset}\\n\"\n",
    "        f\"Method           : {args.sortby}\\n\"\n",
    "        f\"Layers           : {args.num_layers}\\n\"\n",
    "        f\"Batch size       : {args.per_device_train_batch_size}\\n\"\n",
    "        f\"Learning Rate    : {args.learning_rate}\\n\"\n",
    "        f\"Forward time     : {forward_time/60} min\\n\"\n",
    "        f\"Backward time    : {backward_time/60} min\\n\"\n",
    "        f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "        f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "        f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "        f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "        f\"Input memory     : {input_memory / 1e6} MB\\n\"\n",
    "        f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    "        f\"Peak memory      : {peek_memory / 1e6} MB\\n\"\n",
    "    )\n",
    "print(memory_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE TRAINING HISTORY\n",
    "base = {\"train_loss\": train_losses,\"time\": times}\n",
    "savepath = f\"./output/{args.dataset}/lr_{args.learning_rate}/batch_{args.per_device_train_batch_size}/{args.sortby}/layers_{args.num_layers}\"\n",
    "if args.memlog:\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(os.path.join(savepath, \"finetune.npy\"), base) # type: ignore\n",
    "    with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"Batch Size {args.per_device_train_batch_size} \"\n",
    "        + f\"{args.sortby} fine-tuning \"\n",
    "        + f\"{args.num_layers} Layers\"\n",
    "    )\n",
    "    logger = get_logger(savepath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
