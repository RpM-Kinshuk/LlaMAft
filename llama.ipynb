{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-22 23:21:23.981934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-22 23:21:23.981966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-22 23:21:23.983739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-22 23:21:23.991969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 23:21:27.515616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "from model import get_model\n",
    "from traineval.eval import eval_func\n",
    "from traineval.train import train_func\n",
    "from loader.callbacks import mmlu_callback\n",
    "from loader.data_module import make_data_module\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import ( \n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/rscratch/tpang/kinshuk/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"]=\"/rscratch/tpang/kinshuk/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=50,\n",
    "    dataset=\"oasst1\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    logging_steps=10,\n",
    "    data_seed=42,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=False,\n",
    "    max_steps=5,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "    seed=7,\n",
    "    sortby=\"random\",\n",
    "    num_layers=15,\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    # Define generation-specific arguments here, if any are required\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=1024, target_max_len=256, dataset='oasst1', dataset_format=None, output_dir='./output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/Feb22_23-21-36_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=0, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=False, freeze=True, sortby='random', num_layers=15, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, max_memory_MB=12000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=8, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "\n",
      "\n",
      "\n",
      "Seed: 7\n",
      "\n",
      "Dataset: oasst1\n",
      "\n",
      "Sort by: random\n",
      "\n",
      "Sort Descending: True\n",
      "\n",
      "Layers to train: 15\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.11.mlp.up_proj', 'model.layers.5.self_attn.o_proj', 'model.layers.14.self_attn.o_proj', 'model.layers.23.mlp.up_proj', 'model.layers.1.mlp.up_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.30.self_attn.q_proj', 'model.layers.19.mlp.gate_proj', 'model.layers.3.self_attn.o_proj', 'model.layers.13.self_attn.v_proj', 'model.layers.21.self_attn.v_proj', 'model.layers.2.self_attn.q_proj', 'model.layers.18.self_attn.o_proj', 'model.layers.7.mlp.up_proj', 'model.layers.1.self_attn.v_proj']\n",
      "Enabling model.layers.1.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.1.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.2.self_attn.q_proj.weight parameter\n",
      "Enabling model.layers.2.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.3.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.5.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.7.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.11.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.13.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.14.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.19.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.23.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.q_proj.weight parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.01\n",
      "  total_flos               =   759104GF\n",
      "  train_loss               =     1.5364\n",
      "  train_runtime            = 0:00:38.85\n",
      "  train_samples_per_second =      2.059\n",
      "  train_steps_per_second   =      0.129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       0.01\n",
      "  eval_loss               =     1.5692\n",
      "  eval_runtime            = 0:00:17.51\n",
      "  eval_samples_per_second =      2.855\n",
      "  eval_steps_per_second   =        0.4\n",
      "\n",
      "\n",
      "\n",
      "Memory usage before: 0 MB\n",
      "Memory usage after: 27156 MB\n",
      "\n",
      "Peak Memory usage: 30715 MB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    print(args)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = args.cache_dir\n",
    "    cuda_device = torch.cuda.current_device()\n",
    "    gpus = torch.cuda.device_count()\n",
    "    sby = args.sortby\n",
    "    if \"alpha\" in (args.sortby).lower():\n",
    "        sby = \"alpha\"\n",
    "    elif \"layer\" in (args.sortby).lower():\n",
    "        sby = \"layer\"\n",
    "    else:\n",
    "        sby = \"rand\"\n",
    "\n",
    "    # Memory Log Path\n",
    "    mempath = (\n",
    "        f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "        + f\"{sby}\"\n",
    "    )\n",
    "    \n",
    "    # Control randomness\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # accelerate.utils.set_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    set_seed(args.seed)  # transformers seed\n",
    "    \n",
    "    start_memory = [0] * gpus\n",
    "    end_memory = [0] * gpus\n",
    "    peek_memory = 0\n",
    "    # Memory Stats Initialization\n",
    "    for device in range(gpus):\n",
    "        reset_peak_memory_stats(device=device)\n",
    "        reset_max_memory_allocated(device=device)\n",
    "        start_memory[device] = memory_allocated(device=device)\n",
    "\n",
    "    if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\n\\n\\nSeed: {args.seed}\\n\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\\n\"\n",
    "            + f\"Sort Descending: {not args.sort_ascending}\\n\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\\n\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "    else:\n",
    "        datasets_vb_err()\n",
    "        transformers_vb_err()\n",
    "        global _tqdm_active\n",
    "        _tqdm_active = False\n",
    "\n",
    "    # WIP >>>------------------------------------------>\n",
    "\n",
    "    model, tokenizer = get_model(args)\n",
    "\n",
    "    data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "    )\n",
    "\n",
    "    if args.do_mmlu_eval:\n",
    "        trainer = mmlu_callback(args, tokenizer, trainer)\n",
    "\n",
    "    all_metrics = {\"run_name\": args.run_name}\n",
    "\n",
    "    # Train\n",
    "    if args.do_train:\n",
    "        all_metrics = train_func(args, logger, trainer, all_metrics)\n",
    "    \n",
    "    # Eval\n",
    "    if args.do_eval:\n",
    "        all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "\n",
    "    for device in range(gpus):\n",
    "        end_memory[device] = memory_allocated(device=device)\n",
    "        peek_memory += max_memory_allocated(device=device)\n",
    "    print(\n",
    "        f\"\\n\\n\\nMemory usage before: {int(sum(start_memory)/1e6)} MB\\n\"\\\n",
    "        +f\"Memory usage after: {int(sum(end_memory)/1e6)} MB\"\n",
    "    )\n",
    "    print(f\"\\nPeak Memory usage: {int(peek_memory/1e6)} MB\\n\\n\\n\")\n",
    "\n",
    "    # WIP <-----------------------------------------<<<\n",
    "\n",
    "    if args.memlog: # Memory Logging\n",
    "        log_info = (\n",
    "            f\"\\n\\n{args.dataset} \"\n",
    "            + f\"{args.num_layers} Layers \"\n",
    "            + f\"{args.sortby} \"\n",
    "            + f\"Ascending {args.sort_ascending}\"\n",
    "        )\n",
    "        Path(mempath).mkdir(parents=True, exist_ok=True)\n",
    "        logger = get_logger(mempath, \"memlog.log\")\n",
    "        logger.info(log_info)\n",
    "        logger.info(\n",
    "            f\"\\nMemory usage before: {int(sum(start_memory)/1e6)} MB\\n\"\n",
    "            + f\"Memory usage after: {int(sum(end_memory)/1e6)} MB\"\n",
    "        )\n",
    "        logger.info(f\"\\nPeak Memory usage: {int(peek_memory/1e6)} MB\\n\\n\")\n",
    "\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        metrics_file_path = os.path.join(args.output_dir,\n",
    "                                    f'trainseed_{args.seed}',\n",
    "                                    args.dataset,\n",
    "                                    f\"{sby}_asc_{args.sort_ascending}\",\n",
    "                                    f\"layers_{args.num_layers}\",\n",
    "                                    \"metrics.json\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(metrics_file_path), exist_ok=True)\n",
    "        with open(metrics_file_path, \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
