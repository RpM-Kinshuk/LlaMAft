{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-22 22:48:07.200209: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-22 22:48:07.200257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-22 22:48:07.201911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-22 22:48:07.210798: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 22:48:10.281924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "# import accelerate.utils\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import ( \n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainerCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from tqdm import tqdm  \n",
    "from datasets import load_dataset\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/rscratch/tpang/kinshuk/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"]=\"/rscratch/tpang/kinshuk/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"meta-llama/Llama-2-7b-hf\"\n",
    "    )\n",
    "    trust_remote_code: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enable unpickling of arbitrary code in AutoModelForCausalLM#from_pretrained.\"}\n",
    "    )\n",
    "    use_auth_token: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"To use Huggingface auth token from Git Credentials.\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    eval_dataset_size: int = field(\n",
    "        default=1024, metadata={\"help\": \"Size of validation dataset.\"}\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging, truncate the number of train examples.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging, truncate the number of eval examples.\"\n",
    "        },\n",
    "    )\n",
    "    source_max_len: int = field(\n",
    "        default=1024,\n",
    "        metadata={\"help\": \"Maximum source sequence length.\"},\n",
    "    )\n",
    "    target_max_len: int = field(\n",
    "        default=256,\n",
    "        metadata={\"help\": \"Maximum target sequence length.\"},\n",
    "    )\n",
    "    dataset: str = field(\n",
    "        default='alpaca',\n",
    "        metadata={\"help\": \"Which dataset to finetune on. See datamodule for options.\"}\n",
    "    )\n",
    "    dataset_format: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Dataset format being used. [alpaca|chip2|self-instruct|hh-rlhf]\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.Seq2SeqTrainingArguments):\n",
    "    seed: Optional[int] = field(\n",
    "        default=7,\n",
    "        metadata={\"help\": \"Random seed for reproducibility.\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default='/rscratch/tpang/kinshuk/cache',\n",
    "    )\n",
    "    verbose: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to print verbose output.\"}\n",
    "    )\n",
    "    memlog: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to log memory usage.\"}\n",
    "    )\n",
    "    freeze: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to freeze the model.\"}\n",
    "    )\n",
    "    sortby: str = field(\n",
    "        default='random',\n",
    "        metadata={\"help\": \"Layer sorting method. [random|alpha|layer]\"}\n",
    "    )\n",
    "    num_layers: int = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Number of layers to train.\"}\n",
    "    )\n",
    "    sort_ascending: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to train in ascending order of layer sorting method.\"}\n",
    "    )\n",
    "    add_layer_norm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to add layer norm to the layers being trained.\"}\n",
    "    )\n",
    "    train_on_source: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to train on the input in addition to the target text.\"}\n",
    "    )\n",
    "    mmlu_split: Optional[str] = field(\n",
    "        default='eval',\n",
    "        metadata={\"help\": \"The MMLU split to run on\"}\n",
    "    )\n",
    "    mmlu_dataset: Optional[str] = field(\n",
    "        default='mmlu-fs',\n",
    "        metadata={\"help\": \"MMLU dataset to use: [`mmlu-zs`:zero-shot|`mmlu-fs`:few-shot].\"}\n",
    "    )\n",
    "    do_mmlu_eval: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to run the MMLU evaluation.\"}\n",
    "    )\n",
    "    max_mmlu_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If set, only evaluates on `max_mmlu_samples` of the MMMLU dataset.\"}\n",
    "    )\n",
    "    mmlu_source_max_len: int = field(\n",
    "        default=2048,\n",
    "        metadata={\"help\": \"Maximum source sequence length for MMLU.\"}\n",
    "    )\n",
    "    full_finetune: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Finetune the entire model without adapters.\"}\n",
    "    )\n",
    "    max_memory_MB: int = field(\n",
    "        default=12000,\n",
    "        metadata={\"help\": \"Free memory per gpu.\"}\n",
    "    )\n",
    "    report_to: str = field(\n",
    "        default='none',\n",
    "        metadata={\"help\": \"To use wandb or something else for reporting.\"}\n",
    "    )\n",
    "    output_dir: str = field(default='./output', metadata={\"help\": 'The output dir for logs and checkpoints'})\n",
    "    optim: str = field(default='paged_adamw_32bit', metadata={\"help\": 'The optimizer to be used'})\n",
    "    per_device_train_batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU.'})\n",
    "    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'Gradients to accumulate before performing an optimizer step'})\n",
    "    max_steps: int = field(default=10000, metadata={\"help\": 'How many optimizer update steps to take'})\n",
    "    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) # use lora dropout instead for regularization if needed\n",
    "    learning_rate: float = field(default=0.0002, metadata={\"help\": 'The learnign rate'})\n",
    "    remove_unused_columns: bool = field(default=False, metadata={\"help\": 'Removed unused columns. Needed to make this codebase work.'})\n",
    "    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n",
    "    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n",
    "    do_train: bool = field(default=True, metadata={\"help\": 'To train or not.'})\n",
    "    lr_scheduler_type: str = field(default='constant', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n",
    "    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n",
    "    logging_steps: int = field(default=10, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n",
    "    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n",
    "    save_strategy: str = field(default='steps', metadata={\"help\": 'When to save checkpoints'})\n",
    "    save_steps: int = field(default=250, metadata={\"help\": 'How often to save a model'})\n",
    "    save_total_limit: int = field(default=40, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n",
    "\n",
    "@dataclass\n",
    "class GenerationArguments:\n",
    "    # For more hyperparameters check:\n",
    "    # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    # Length arguments\n",
    "    max_new_tokens: Optional[int] = field(\n",
    "        default=256,\n",
    "        metadata={\"help\": \"Max number of new tokens to be generated in eval or prediction loops\"\n",
    "                          \"if predict_with_generate is set.\"}\n",
    "    )\n",
    "    min_new_tokens : Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Min number of new tokens to generate.\"}\n",
    "    )\n",
    "\n",
    "    # Generation strategy\n",
    "    do_sample: Optional[bool] = field(default=False)\n",
    "    num_beams: Optional[int] = field(default=1)\n",
    "    num_beam_groups: Optional[int] = field(default=1)\n",
    "    penalty_alpha: Optional[float] = field(default=None)\n",
    "    use_cache: Optional[bool] = field(default=True)\n",
    "\n",
    "    # Hyperparameters for logit manipulation\n",
    "    temperature: Optional[float] = field(default=1.0)\n",
    "    top_k: Optional[int] = field(default=50)\n",
    "    top_p: Optional[float] = field(default=1.0)\n",
    "    typical_p: Optional[float] = field(default=1.0)\n",
    "    diversity_penalty: Optional[float] = field(default=0.0)\n",
    "    repetition_penalty: Optional[float] = field(default=1.0)\n",
    "    length_penalty: Optional[float] = field(default=1.0)\n",
    "    no_repeat_ngram_size: Optional[int] = field(default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Borrowed from qlora codebase\n",
    "    Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings_data = model.get_input_embeddings().weight.data\n",
    "        output_embeddings_data = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings_data[:-num_new_tokens].mean( # type: ignore\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings_data[:-num_new_tokens].mean( # type: ignore\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings_data[-num_new_tokens:] = input_embeddings_avg # type: ignore\n",
    "        output_embeddings_data[-num_new_tokens:] = output_embeddings_avg # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    max_memory = f'{args.max_memory_MB}MB'\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "    device_map = \"auto\"\n",
    "\n",
    "    if os.environ.get('LOCAL_RANK') is not None:\n",
    "        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "        device_map = {'': local_rank}\n",
    "        max_memory = {'': max_memory[local_rank]}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path, \n",
    "        token=\"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\",\n",
    "        device_map=device_map,\n",
    "        max_memory=max_memory,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    setattr(model, 'model_parallel', True)\n",
    "    setattr(model, 'is_parallelizable', True)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        token=\"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\",\n",
    "        cache_dir=args.cache_dir,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False, # Fast tokenizer giving issues.\n",
    "        tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "        # use_auth_token=args.use_auth_token,\n",
    "    )\n",
    "    if tokenizer._pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer, # type: ignore\n",
    "            model=model,\n",
    "        )\n",
    "    if 'llama' in args.model_name_or_path or isinstance(tokenizer, LlamaTokenizer):\n",
    "        # LLaMA tokenizer may not have correct special tokens set.\n",
    "        # Check and add them if missing to prevent them from being parsed into different tokens.\n",
    "        # Note that these are present in the vocabulary.\n",
    "        # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
    "        print('Adding special tokens.')\n",
    "        tokenizer.add_special_tokens({\n",
    "                \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "                \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "                \"unk_token\": tokenizer.convert_ids_to_tokens(\n",
    "                    model.config.pad_token_id\n",
    "                    if model.config.pad_token_id != -1\n",
    "                    else tokenizer.pad_token_id # type: ignore\n",
    "                ),\n",
    "        })\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if hasattr(module, 'weight'):\n",
    "                if args.bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16) \n",
    "    \n",
    "    # SELECTIVE FINETUNING >>>------------------------------------->\n",
    "\n",
    "    if args.freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if \"lm_head\" in name:\n",
    "                param.requires_grad = True\n",
    "    else:\n",
    "        for name, param in model.named_parameters():  # type: ignore\n",
    "            param.requires_grad = True\n",
    "        return model, tokenizer\n",
    "    \n",
    "    # if \"lora\" not in args.sortby.lower():\n",
    "    #     # Save WeightWatcher Metrics\n",
    "    #     watcher = ww.WeightWatcher(model=model)\n",
    "    #     ww_details = watcher.analyze(min_evals=10)\n",
    "\n",
    "    # if not args.debug and \"lora\" not in args.sortby.lower():\n",
    "    #     ww_details.to_csv(os.path.join(stats_path, f\"epoch_{epoch}.csv\"))  # type: ignore\n",
    "\n",
    "    ww_details = pd.read_csv(\"./llama_ww.csv\")\n",
    "    # CHOOSING LAYERS TO TRAIN BASED ON WEIGHTWATCHER METRICS/SORTBY\n",
    "    if \"lora\" not in args.sortby.lower():\n",
    "        filtered = ww_details[  # type: ignore\n",
    "            ww_details[\"longname\"].str.contains(\"embed_tokens\") == False  # type: ignore\n",
    "        ]\n",
    "        sortby = \"alpha\"\n",
    "        if args.num_layers > len(filtered):\n",
    "            args.num_layers = len(filtered)\n",
    "        if \"random\" in (args.sortby).lower():\n",
    "            train_names = random.sample(filtered[\"longname\"].to_list(), args.num_layers)\n",
    "        else:\n",
    "            if \"alpha\" in (args.sortby).lower():\n",
    "                sortby = \"alpha\"\n",
    "            elif \"layer\" in (args.sortby).lower():\n",
    "                sortby = \"layer_id\"\n",
    "            else:\n",
    "                sortby = \"random\"\n",
    "            train_names = (\n",
    "                filtered.sort_values(by=[sortby], ascending=args.sort_ascending)[\n",
    "                    \"longname\"\n",
    "                ]\n",
    "                .iloc[: args.num_layers]\n",
    "                .to_list()\n",
    "            )\n",
    "        if args.verbose:\n",
    "            print(\"Sorted by \", sortby)\n",
    "            print(\"Training layers:\", train_names)\n",
    "        layer_to_train = []\n",
    "        for layer in train_names:\n",
    "            layer_to_train.append(layer + \".weight\")\n",
    "            layer_to_train.append(layer + \".bias\")\n",
    "            # Add Layer Norm\n",
    "            if args.add_layer_norm:\n",
    "                if \"output\" in layer:\n",
    "                    layer_to_train.append(\n",
    "                        layer.replace(\"dense\", \"LayerNorm\") + \".weight\"\n",
    "                    )\n",
    "                    layer_to_train.append(layer.replace(\"dense\", \"LayerNorm\") + \".bias\")\n",
    "        layer_to_train = list(set(layer_to_train))\n",
    "        # print(\"Final Training layers:\", layer_to_train)\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in layer_to_train:\n",
    "                if args.verbose:\n",
    "                    print(f\"Enabling {name} parameter\")\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if hasattr(module, 'weight'):\n",
    "                if args.bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16) \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForCausalLM(object):\n",
    "    \"\"\"Borrowed from qlora codebase.\"\"\"\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    source_max_len: int\n",
    "    target_max_len: int\n",
    "    train_on_source: bool\n",
    "    predict_with_generate: bool\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract elements\n",
    "        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n",
    "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n",
    "        # Tokenize\n",
    "        tokenized_sources_with_prompt = self.tokenizer(\n",
    "            sources,\n",
    "            max_length=self.source_max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        tokenized_targets = self.tokenizer(\n",
    "            targets,\n",
    "            max_length=self.target_max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        # Build the input and labels for causal LM\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        for tokenized_source, tokenized_target in zip(\n",
    "            tokenized_sources_with_prompt['input_ids'], # type: ignore\n",
    "            tokenized_targets['input_ids'] # type: ignore\n",
    "        ):\n",
    "            if not self.predict_with_generate:\n",
    "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
    "                if not self.train_on_source:\n",
    "                    labels.append(\n",
    "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
    "                    )\n",
    "                else:\n",
    "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
    "            else:\n",
    "                input_ids.append(torch.tensor(tokenized_source))\n",
    "        # Apply padding\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id) # type: ignore\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id), # type: ignore\n",
    "        }\n",
    "        if labels is not None:\n",
    "            data_dict['labels'] = labels\n",
    "        return data_dict\n",
    "\n",
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "\n",
    "def local_dataset(dataset_name):\n",
    "    if dataset_name.endswith('.json') or dataset_name.endswith('.jsonl'):\n",
    "        full_dataset = Dataset.from_json(path_or_paths=dataset_name)\n",
    "    elif dataset_name.endswith('.csv'):\n",
    "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name))\n",
    "    elif dataset_name.endswith('.tsv'):\n",
    "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name, delimiter='\\t'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset format: {dataset_name}\")\n",
    "\n",
    "    split_dataset = full_dataset.train_test_split(test_size=0.1) # type: ignore\n",
    "    return split_dataset\n",
    "\n",
    "def make_data_module(tokenizer: PreTrainedTokenizer, args) -> Dict:\n",
    "    \"\"\"\n",
    "    Make dataset and collator for supervised fine-tuning.\n",
    "    Datasets are expected to have the following columns: { `input`, `output` }\n",
    "\n",
    "    Available datasets to be selected with `dataset` argument:\n",
    "        - alpaca, 52002 examples\n",
    "        - alpaca cleaned, 51942 examples\n",
    "        - chip2 (OIG), 210289 examples\n",
    "        - self-instruct, 82612 examples\n",
    "        - hh-rlhf (Anthropic), 160800 examples\n",
    "        - longform, 23.7k examples\n",
    "        - oasst1 (OpenAssistant) primary message tree only, 9,846 examples\n",
    "\n",
    "    Coming soon:\n",
    "        - unnatural instructions core, 66010 examples\n",
    "        - unnatural instructions full, 240670 examples\n",
    "        - alpaca-gpt4, 52002 examples\n",
    "        - unnatural-instructions-gpt4, 9000 examples\n",
    "        - supernatural-instructions, 69624 examples (same as paper with 100 ex/task more can be used)\n",
    "        - flan (FLAN v2), up to 20M examples available\n",
    "        - vicuna\n",
    "\n",
    "    \"\"\"\n",
    "    def load_data(dataset_name):\n",
    "        if dataset_name == 'alpaca':\n",
    "            return load_dataset(\"tatsu-lab/alpaca\")\n",
    "        elif dataset_name == 'alpaca-clean':\n",
    "            return load_dataset(\"yahma/alpaca-cleaned\")\n",
    "        elif dataset_name == 'chip2':\n",
    "            return load_dataset(\"laion/OIG\", data_files='unified_chip2.jsonl')\n",
    "        elif dataset_name == 'self-instruct':\n",
    "            return load_dataset(\"yizhongw/self_instruct\", name='self_instruct')\n",
    "        elif dataset_name == 'hh-rlhf':\n",
    "            return load_dataset(\"Anthropic/hh-rlhf\")\n",
    "        elif dataset_name == 'longform':\n",
    "            return load_dataset(\"akoksal/LongForm\")\n",
    "        elif dataset_name == 'oasst1':\n",
    "            return load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "        elif dataset_name == 'vicuna':\n",
    "            raise NotImplementedError(\"Vicuna data was not released.\")\n",
    "        else:\n",
    "            if os.path.exists(dataset_name):\n",
    "                try:\n",
    "                    args.dataset_format = args.dataset_format if args.dataset_format else \"input-output\"\n",
    "                    full_dataset = local_dataset(dataset_name)\n",
    "                    return full_dataset\n",
    "                except:\n",
    "                    raise ValueError(f\"Error loading dataset from {dataset_name}\")\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Dataset {dataset_name} not implemented yet.\")\n",
    "\n",
    "    def format_dataset(dataset, dataset_format):\n",
    "        if (\n",
    "            dataset_format == 'alpaca' or dataset_format == 'alpaca-clean' or\n",
    "            (dataset_format is None and args.dataset in ['alpaca', 'alpaca-clean'])\n",
    "        ):\n",
    "            dataset = dataset.map(extract_alpaca_dataset, remove_columns=['instruction'])\n",
    "        elif dataset_format == 'chip2' or (dataset_format is None and args.dataset == 'chip2'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': x['text'].split('\\n<bot>: ')[0].replace('<human>: ', ''),\n",
    "                'output': x['text'].split('\\n<bot>: ')[1],\n",
    "            })\n",
    "        elif dataset_format == 'self-instruct' or (dataset_format is None and args.dataset == 'self-instruct'):\n",
    "            for old, new in [[\"prompt\", \"input\"], [\"completion\", \"output\"]]:\n",
    "                dataset = dataset.rename_column(old, new)\n",
    "        elif dataset_format == 'hh-rlhf' or (dataset_format is None and args.dataset == 'hh-rlhf'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': '',\n",
    "                'output': x['chosen']\n",
    "            })\n",
    "        elif dataset_format == 'oasst1' or (dataset_format is None and args.dataset == 'oasst1'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': '',\n",
    "                'output': x['text'],\n",
    "            })\n",
    "        elif dataset_format == 'input-output':\n",
    "            # leave as is\n",
    "            pass\n",
    "        # Remove unused columns.\n",
    "        dataset = dataset.remove_columns(\n",
    "            [col for col in dataset.column_names['train'] if col not in ['input', 'output']]\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "     # Load dataset.\n",
    "    dataset = load_data(args.dataset)\n",
    "    dataset = format_dataset(dataset, args.dataset_format)\n",
    "\n",
    "    # Split train/eval, reduce size\n",
    "    if args.do_eval or args.do_predict:\n",
    "        if 'eval' in dataset:\n",
    "            eval_dataset = dataset['eval']\n",
    "        else:\n",
    "            print('Splitting train dataset in train and validation according to `eval_dataset_size`')\n",
    "            dataset = dataset[\"train\"].train_test_split(\n",
    "                test_size=args.eval_dataset_size, shuffle=True, seed=42\n",
    "            )\n",
    "            eval_dataset = dataset['test']\n",
    "        if args.max_eval_samples is not None and len(eval_dataset) > args.max_eval_samples:\n",
    "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "        if args.group_by_length:\n",
    "            eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
    "    if args.do_train:\n",
    "        train_dataset = dataset['train']\n",
    "        if args.max_train_samples is not None and len(train_dataset) > args.max_train_samples:\n",
    "            train_dataset = train_dataset.select(range(args.max_train_samples))\n",
    "        if args.group_by_length:\n",
    "            train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
    "\n",
    "    data_collator = DataCollatorForCausalLM(\n",
    "        tokenizer=tokenizer,\n",
    "        source_max_len=args.source_max_len,\n",
    "        target_max_len=args.target_max_len,\n",
    "        train_on_source=args.train_on_source,\n",
    "        predict_with_generate=args.predict_with_generate,\n",
    "    )\n",
    "    return dict(\n",
    "        train_dataset=train_dataset if args.do_train else None,\n",
    "        eval_dataset=eval_dataset if args.do_eval else None,\n",
    "        predict_dataset=eval_dataset if args.do_predict else None,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_func(args, logger, trainer, all_metrics):\n",
    "    if args.do_train:\n",
    "        logger.info(\"*** Train ***\")\n",
    "        # Note: `resume_from_checkpoint` not supported for adapter checkpoints by HF.\n",
    "        # Currently adapter checkpoint is reloaded as expected but optimizer/scheduler states are not.\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        # trainer.save_state()\n",
    "        all_metrics.update(metrics)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "def eval_func(args, logger, trainer, all_metrics):\n",
    "    if args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "        all_metrics.update(metrics)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmlu_callback(args, tokenizer, trainer):\n",
    "    if args.do_mmlu_eval:\n",
    "        if args.mmlu_dataset == 'mmlu-zs':\n",
    "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
    "                'eval': 'data/mmlu/zero_shot_mmlu_val.json',\n",
    "                'test': 'data/mmlu/zero_shot_mmlu_test.json',\n",
    "            })\n",
    "            mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
    "        # MMLU Five-shot (Eval/Test only)\n",
    "        elif args.mmlu_dataset == 'mmlu' or args.mmlu_dataset == 'mmlu-fs':\n",
    "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
    "                'eval': 'data/mmlu/five_shot_mmlu_val.json',\n",
    "                'test': 'data/mmlu/five_shot_mmlu_test.json',\n",
    "            })\n",
    "            # mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
    "        mmlu_dataset = mmlu_dataset[args.mmlu_split] # type: ignore\n",
    "        if args.max_mmlu_samples is not None:\n",
    "            mmlu_dataset = mmlu_dataset.select(range(args.max_mmlu_samples))\n",
    "        abcd_idx = [\n",
    "            tokenizer(\"A\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"B\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"C\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"D\", add_special_tokens=False).input_ids[0],\n",
    "        ]\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "        class MMLUEvalCallback(TrainerCallback):\n",
    "            def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "                data_loader = trainer.get_eval_dataloader(mmlu_dataset)\n",
    "                source_max_len = trainer.data_collator.source_max_len\n",
    "                trainer.data_collator.source_max_len = args.mmlu_source_max_len\n",
    "                trainer.model.eval()\n",
    "                preds, refs = [], []\n",
    "                loss_mmlu = 0\n",
    "                for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "                    (loss, logits, labels) = trainer.prediction_step(trainer.model,batch,prediction_loss_only=False,)\n",
    "                    # There are two tokens, the output, and eos token.\n",
    "                    for i, logit in enumerate(logits):\n",
    "                        label_non_zero_id = (batch['labels'][i] != -100).nonzero()[0][0]\n",
    "                        logit_abcd = logit[label_non_zero_id-1][abcd_idx]\n",
    "                        preds.append(torch.argmax(logit_abcd).item())\n",
    "                    labels = labels[labels != IGNORE_INDEX].view(-1, 2)[:,0]\n",
    "                    refs += [abcd_idx.index(label) for label in labels.tolist()]\n",
    "                    loss_mmlu += loss.item()\n",
    "                # Extract results by subject.\n",
    "                results = {'mmlu_loss':loss_mmlu/len(data_loader)}\n",
    "                subject = mmlu_dataset['subject']\n",
    "                subjects = {s:{'refs':[], 'preds':[]} for s in set(subject)}\n",
    "                for s,p,r in zip(subject, preds, refs):\n",
    "                    subjects[s]['preds'].append(p)\n",
    "                    subjects[s]['refs'].append(r)\n",
    "                subject_scores = []\n",
    "                for subject in subjects:\n",
    "                    subject_score = accuracy.compute(\n",
    "                        references=subjects[subject]['refs'],\n",
    "                        predictions=subjects[subject]['preds']\n",
    "                    )['accuracy'] # type: ignore\n",
    "                    results[f'mmlu_{args.mmlu_split}_accuracy_{subject}'] = subject_score\n",
    "                    subject_scores.append(subject_score)\n",
    "                results[f'mmlu_{args.mmlu_split}_accuracy'] = np.mean(subject_scores)\n",
    "                trainer.log(results)\n",
    "                trainer.data_collator.source_max_len = source_max_len\n",
    "\n",
    "        trainer.add_callback(MMLUEvalCallback)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=50,\n",
    "    dataset=\"oasst1\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    logging_steps=10,\n",
    "    data_seed=42,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=False,\n",
    "    max_steps=5,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "    seed=7,\n",
    "    sortby=\"random\",\n",
    "    num_layers=15,\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    # Define generation-specific arguments here, if any are required\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=1024, target_max_len=256, dataset='oasst1', dataset_format=None, output_dir='./output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/Feb22_23-10-36_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=0, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=False, freeze=True, sortby='random', num_layers=15, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, max_memory_MB=12000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=8, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "\n",
      "\n",
      "\n",
      "Seed: 7\n",
      "\n",
      "Dataset: oasst1\n",
      "\n",
      "Sort by: random\n",
      "\n",
      "Sort Descending: True\n",
      "\n",
      "Layers to train: 15\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.11.mlp.up_proj', 'model.layers.5.self_attn.o_proj', 'model.layers.14.self_attn.o_proj', 'model.layers.23.mlp.up_proj', 'model.layers.1.mlp.up_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.30.self_attn.q_proj', 'model.layers.19.mlp.gate_proj', 'model.layers.3.self_attn.o_proj', 'model.layers.13.self_attn.v_proj', 'model.layers.21.self_attn.v_proj', 'model.layers.2.self_attn.q_proj', 'model.layers.18.self_attn.o_proj', 'model.layers.7.mlp.up_proj', 'model.layers.1.self_attn.v_proj']\n",
      "Enabling model.layers.1.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.1.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.2.self_attn.q_proj.weight parameter\n",
      "Enabling model.layers.2.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.3.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.5.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.7.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.11.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.13.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.14.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.19.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.23.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.q_proj.weight parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.01\n",
      "  total_flos               =   759104GF\n",
      "  train_loss               =     1.5364\n",
      "  train_runtime            = 0:00:38.88\n",
      "  train_samples_per_second =      2.057\n",
      "  train_steps_per_second   =      0.129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       0.01\n",
      "  eval_loss               =     1.5692\n",
      "  eval_runtime            = 0:00:17.54\n",
      "  eval_samples_per_second =       2.85\n",
      "  eval_steps_per_second   =      0.399\n",
      "\n",
      "\n",
      "\n",
      "Memory usage before: 0 MB\n",
      "Memory usage after: 27156 MB\n",
      "\n",
      "Peak Memory usage: 30715 MB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    print(args)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = args.cache_dir\n",
    "    cuda_device = torch.cuda.current_device()\n",
    "    gpus = torch.cuda.device_count()\n",
    "    sby = args.sortby\n",
    "    if \"alpha\" in (args.sortby).lower():\n",
    "        sby = \"alpha\"\n",
    "    elif \"layer\" in (args.sortby).lower():\n",
    "        sby = \"layer\"\n",
    "    else:\n",
    "        sby = \"rand\"\n",
    "\n",
    "    # Memory Log Path\n",
    "    mempath = (\n",
    "        f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "        + f\"{sby}\"\n",
    "    )\n",
    "    \n",
    "    # Control randomness\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # accelerate.utils.set_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    set_seed(args.seed)  # transformers seed\n",
    "    \n",
    "    start_memory = [0] * gpus\n",
    "    end_memory = [0] * gpus\n",
    "    peek_memory = 0\n",
    "    # Memory Stats Initialization\n",
    "    for device in range(gpus):\n",
    "        reset_peak_memory_stats(device=device)\n",
    "        reset_max_memory_allocated(device=device)\n",
    "        start_memory[device] = memory_allocated(device=device)\n",
    "\n",
    "    if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\n\\n\\nSeed: {args.seed}\\n\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\\n\"\n",
    "            + f\"Sort Descending: {not args.sort_ascending}\\n\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\\n\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "    else:\n",
    "        datasets_vb_err()\n",
    "        transformers_vb_err()\n",
    "        global _tqdm_active\n",
    "        _tqdm_active = False\n",
    "\n",
    "    # WIP >>>------------------------------------------>\n",
    "\n",
    "    model, tokenizer = get_model(args)\n",
    "\n",
    "    data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "    )\n",
    "\n",
    "    if args.do_mmlu_eval:\n",
    "        trainer = mmlu_callback(args, tokenizer, trainer)\n",
    "\n",
    "    all_metrics = {\"run_name\": args.run_name}\n",
    "\n",
    "    # Train\n",
    "    if args.do_train:\n",
    "        all_metrics = train_func(args, logger, trainer, all_metrics)\n",
    "    \n",
    "    # Eval\n",
    "    if args.do_eval:\n",
    "        all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "\n",
    "    for device in range(gpus):\n",
    "        end_memory[device] = memory_allocated(device=device)\n",
    "        peek_memory += max_memory_allocated(device=device)\n",
    "    print(\n",
    "        f\"\\n\\n\\nMemory usage before: {int(sum(start_memory)/1e6)} MB\\n\"\\\n",
    "        +f\"Memory usage after: {int(sum(end_memory)/1e6)} MB\"\n",
    "    )\n",
    "    print(f\"\\nPeak Memory usage: {int(peek_memory/1e6)} MB\\n\\n\\n\")\n",
    "\n",
    "    # WIP <-----------------------------------------<<<\n",
    "\n",
    "    if args.memlog: # Memory Logging\n",
    "        log_info = (\n",
    "            f\"\\n\\n{args.dataset} \"\n",
    "            + f\"{args.num_layers} Layers \"\n",
    "            + f\"{args.sortby} \"\n",
    "            + f\"Ascending {args.sort_ascending}\"\n",
    "        )\n",
    "        Path(mempath).mkdir(parents=True, exist_ok=True)\n",
    "        logger = get_logger(mempath, \"memlog.log\")\n",
    "        logger.info(log_info)\n",
    "        logger.info(\n",
    "            f\"\\nMemory usage before: {int(sum(start_memory)/1e6)} MB\\n\"\n",
    "            + f\"Memory usage after: {int(sum(end_memory)/1e6)} MB\"\n",
    "        )\n",
    "        logger.info(f\"\\nPeak Memory usage: {int(peek_memory/1e6)} MB\\n\\n\")\n",
    "\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        metrics_file_path = os.path.join(args.output_dir,\n",
    "                                    f'trainseed_{args.seed}',\n",
    "                                    args.dataset,\n",
    "                                    f\"{sby}_asc_{args.sort_ascending}\",\n",
    "                                    f\"layers_{args.num_layers}\",\n",
    "                                    \"metrics.json\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(metrics_file_path), exist_ok=True)\n",
    "        with open(metrics_file_path, \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
