{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-31 07:16:02.382599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-31 07:16:02.382637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-31 07:16:02.383924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-31 07:16:02.392442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-31 07:16:07.713643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "    dataset=\"alpaca\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    max_steps=100,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "    sortby=\"alpha\",\n",
    "    num_layers=2,\n",
    "    memlog=False,\n",
    "    per_device_train_batch_size = 4\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=128 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "\n",
      "Dataset: alpaca\n",
      "\n",
      "Sort by: alpha\n",
      "\n",
      "Layers to train: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "# print(args)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj']\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 50978\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'predict_dataset': None,\n",
       " 'data_collator': DataCollatorForCausalLM(tokenizer=LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[PAD]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False), source_max_len=1024, target_max_len=256, train_on_source=False, predict_with_generate=False)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {k:v for k,v in data_module.items()}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12745"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "input_memory = memall()- weight_memory\n",
    "\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12745 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/12745 [00:01<4:23:52,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.3192678689956665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/12745 [00:04<1:21:59,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10, Train Loss: 1.5755878253416582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/12745 [00:08<1:01:51,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20, Train Loss: 1.583515050865355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/12745 [00:12<1:13:59,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 30, Train Loss: 1.5684617084841574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/12745 [00:15<1:13:44,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 40, Train Loss: 1.5688507571453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/12745 [00:19<1:18:25,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50, Train Loss: 1.5460452180282742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 61/12745 [00:22<1:16:55,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 60, Train Loss: 1.5643514541328931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 71/12745 [00:26<1:11:37,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 70, Train Loss: 1.5498012337886111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 81/12745 [00:30<1:16:57,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 80, Train Loss: 1.5551040731830361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 91/12745 [00:33<1:27:03,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 90, Train Loss: 1.572028120795449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/12745 [00:37<1:18:25,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Train Loss: 1.5749579998526242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "        train_loss = 0\n",
    "        tr_steps = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            activation_memory = memall() - weight_memory\n",
    "            # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "            loss = output.loss\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            gradient_memory = memall() - weight_memory\n",
    "            optimizer.step()\n",
    "            optimizer_memory = memall() - gradient_memory - weight_memory \n",
    "            tr_steps += 1\n",
    "            train_losses.append(train_loss/tr_steps)\n",
    "            if step % 10 == 0:\n",
    "                print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "            if step == args.max_steps:\n",
    "                model.eval()\n",
    "                break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/125 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =      1.639\n",
      "  eval_runtime            = 0:01:06.32\n",
      "  eval_samples_per_second =     15.078\n",
      "  eval_steps_per_second   =      1.885\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "\n",
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = max([max_memory_allocated(i) for i in range(gpus)])\n",
    "\n",
    "memory_string = (\n",
    "    f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "    f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "    f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "    f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "    f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 164.63M\n",
      "Weight memory    : 27087.929344 MB\n",
      "Activation memory: 3546.110976 MB\n",
      "Gradient memory  : 2080.964096 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 29168.89344 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_count(model)\n",
    "print(memory_string)\n",
    "if args.memlog:\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"{args.sortby} \"\n",
    "        + f\"{args.num_layers} Layers \"\n",
    "        + f\"Batch Size: {args.per_device_train_batch_size}\"\n",
    "    )\n",
    "    Path(mempath).mkdir(parents=True, exist_ok=True)\n",
    "    logger = get_logger(mempath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "\n",
    "if (args.do_train or args.do_eval or args.do_predict):\n",
    "    with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
