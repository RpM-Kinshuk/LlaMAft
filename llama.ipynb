{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-07 04:05:48.441963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 04:05:48.442005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 04:05:48.443216: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-07 04:05:48.450737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 04:05:52.300641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=100,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "    dataset=\"alpaca\", # DATASET [alpaca|chip2|self-instruct|hh-rlhf|oasst1|longform]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "\n",
    "    learning_rate=5e-6,     # LEARNING RATE\n",
    "    \n",
    "    max_steps=300,         # NUMBER OF STEPS\n",
    "\n",
    "    sortby=\"alpha\",         # CAN DO \"alpha\" or \"lora\" or \"dora\"\n",
    "\n",
    "    num_layers=20,           # NUMBER OF LAYERS FOR FULL FINE-TUNING\n",
    "\n",
    "    per_device_train_batch_size = 2, # BATCH-SIZE\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=256 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "Dataset: alpaca\n",
      "Sort by: alpha\n",
      "Layers to train: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lora' in args.sortby:\n",
    "    args.num_layers = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.27.self_attn.v_proj', 'model.layers.11.self_attn.v_proj', 'model.layers.23.self_attn.o_proj', 'model.layers.3.mlp.up_proj', 'model.layers.21.self_attn.o_proj', 'model.layers.25.mlp.up_proj', 'model.layers.24.self_attn.o_proj', 'model.layers.20.self_attn.v_proj', 'model.layers.26.mlp.up_proj', 'model.layers.27.self_attn.o_proj', 'model.layers.25.self_attn.o_proj', 'model.layers.22.self_attn.o_proj']\n",
      "Enabling model.layers.3.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.11.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.17.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.20.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.23.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.24.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.25.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.25.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.26.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.27.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.27.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.28.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore\n",
    "dataset = {k:v for k,v in data_module.items()}\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.6104059219360352\n",
      "Step: 1, Train Loss: 2.7800209522247314\n",
      "Step: 2, Train Loss: 2.3085394303003945\n",
      "Step: 3, Train Loss: 2.010852873325348\n",
      "Step: 4, Train Loss: 1.902347159385681\n",
      "Step: 5, Train Loss: 1.7808004021644592\n",
      "Step: 6, Train Loss: 1.71690377167293\n",
      "Step: 7, Train Loss: 1.630031555891037\n",
      "Step: 8, Train Loss: 1.6000707546869914\n",
      "Step: 9, Train Loss: 1.6106316924095154\n",
      "Step: 10, Train Loss: 1.6683146845210681\n",
      "Step: 11, Train Loss: 1.625487208366394\n",
      "Step: 12, Train Loss: 1.6004160826022809\n",
      "Step: 13, Train Loss: 1.5283176260335105\n",
      "Step: 14, Train Loss: 1.5391154170036316\n",
      "Step: 15, Train Loss: 1.5425435416400433\n",
      "Step: 16, Train Loss: 1.5110683686593\n",
      "Step: 17, Train Loss: 1.5000833769639332\n",
      "Step: 18, Train Loss: 1.492152656379499\n",
      "Step: 19, Train Loss: 1.4621799647808076\n",
      "Step: 20, Train Loss: 1.4478481270018078\n",
      "Step: 21, Train Loss: 1.4306446476416155\n",
      "Step: 22, Train Loss: 1.4187893452851668\n",
      "Step: 23, Train Loss: 1.4300648073355358\n",
      "Step: 24, Train Loss: 1.4227530479431152\n",
      "Step: 25, Train Loss: 1.4079194481556232\n",
      "Step: 26, Train Loss: 1.4011110464731853\n",
      "Step: 27, Train Loss: 1.4033144584723882\n",
      "Step: 28, Train Loss: 1.3896658543882698\n",
      "Step: 29, Train Loss: 1.375403646628062\n",
      "Step: 30, Train Loss: 1.3711896019597207\n",
      "Step: 31, Train Loss: 1.370854802429676\n",
      "Step: 32, Train Loss: 1.3635053995883826\n",
      "Step: 33, Train Loss: 1.3580545642796684\n",
      "Step: 34, Train Loss: 1.3398938315255302\n",
      "Step: 35, Train Loss: 1.3252943125036027\n",
      "Step: 36, Train Loss: 1.3237245082855225\n",
      "Step: 37, Train Loss: 1.3181440453780324\n",
      "Step: 38, Train Loss: 1.3153529564539592\n",
      "Step: 39, Train Loss: 1.306502990424633\n",
      "Step: 40, Train Loss: 1.3074084883782922\n",
      "Step: 41, Train Loss: 1.2960817459083738\n",
      "Step: 42, Train Loss: 1.2931812904601874\n",
      "Step: 43, Train Loss: 1.2920820103450255\n",
      "Step: 44, Train Loss: 1.2753802087571886\n",
      "Step: 45, Train Loss: 1.2793607115745544\n",
      "Step: 46, Train Loss: 1.275102402301545\n",
      "Step: 47, Train Loss: 1.2579458641509216\n",
      "Step: 48, Train Loss: 1.2679294286941996\n",
      "Step: 49, Train Loss: 1.2670421135425567\n",
      "Step: 50, Train Loss: 1.2550878688400866\n",
      "Step: 51, Train Loss: 1.2556262704042287\n",
      "Step: 52, Train Loss: 1.2557927617486917\n",
      "Step: 53, Train Loss: 1.2592840349232708\n",
      "Step: 54, Train Loss: 1.2642224983735517\n",
      "Step: 55, Train Loss: 1.2633632251194544\n",
      "Step: 56, Train Loss: 1.2657113598104108\n",
      "Step: 57, Train Loss: 1.2661191451138463\n",
      "Step: 58, Train Loss: 1.2541906096167483\n",
      "Step: 59, Train Loss: 1.254566771785418\n",
      "Step: 60, Train Loss: 1.259106445507925\n",
      "Step: 61, Train Loss: 1.248792525260679\n",
      "Step: 62, Train Loss: 1.24890188754551\n",
      "Step: 63, Train Loss: 1.2428670767694712\n",
      "Step: 64, Train Loss: 1.2453878586108869\n",
      "Step: 65, Train Loss: 1.2425017952919006\n",
      "Step: 66, Train Loss: 1.2442763759129083\n",
      "Step: 67, Train Loss: 1.2480172024053686\n",
      "Step: 68, Train Loss: 1.238594607166622\n",
      "Step: 69, Train Loss: 1.237831164257867\n",
      "Step: 70, Train Loss: 1.2465313782154674\n",
      "Step: 71, Train Loss: 1.2471217802829213\n",
      "Step: 72, Train Loss: 1.2496846220264697\n",
      "Step: 73, Train Loss: 1.2435797783168587\n",
      "Step: 74, Train Loss: 1.2315367623170217\n",
      "Step: 75, Train Loss: 1.233310415948692\n",
      "Step: 76, Train Loss: 1.2301925331741184\n",
      "Step: 77, Train Loss: 1.2314640883451853\n",
      "Step: 78, Train Loss: 1.2355974976775013\n",
      "Step: 79, Train Loss: 1.2316956978291274\n",
      "Step: 80, Train Loss: 1.231966072762454\n",
      "Step: 81, Train Loss: 1.2310618100369848\n",
      "Step: 82, Train Loss: 1.2345891885728721\n",
      "Step: 83, Train Loss: 1.2301771303727513\n",
      "Step: 84, Train Loss: 1.2283437732387992\n",
      "Step: 85, Train Loss: 1.2256436420734538\n",
      "Step: 86, Train Loss: 1.2279263505305367\n",
      "Step: 87, Train Loss: 1.2267955490811304\n",
      "Step: 88, Train Loss: 1.2221230760719\n",
      "Step: 89, Train Loss: 1.2245854096280204\n",
      "Step: 90, Train Loss: 1.2287636821741585\n",
      "Step: 91, Train Loss: 1.2357493933776151\n",
      "Step: 92, Train Loss: 1.2358369644611114\n",
      "Step: 93, Train Loss: 1.239413432301359\n",
      "Step: 94, Train Loss: 1.2405025817845996\n",
      "Step: 95, Train Loss: 1.2452676069612305\n",
      "Step: 96, Train Loss: 1.2438193814656169\n",
      "Step: 97, Train Loss: 1.2433352710641161\n",
      "Step: 98, Train Loss: 1.2461424256213989\n",
      "Step: 99, Train Loss: 1.2461832270026207\n",
      "Step: 100, Train Loss: 1.2420348947000976\n",
      "Step: 101, Train Loss: 1.241818852576555\n",
      "Step: 102, Train Loss: 1.2428850540258352\n",
      "Step: 103, Train Loss: 1.2383539312734053\n",
      "Step: 104, Train Loss: 1.2397430263814473\n",
      "Step: 105, Train Loss: 1.2362875260834425\n",
      "Step: 106, Train Loss: 1.2330333855107567\n",
      "Step: 107, Train Loss: 1.2328214206629329\n",
      "Step: 108, Train Loss: 1.2309238847789414\n",
      "Step: 109, Train Loss: 1.2293735550208524\n",
      "Step: 110, Train Loss: 1.2272959746219017\n",
      "Step: 111, Train Loss: 1.228158005912389\n",
      "Step: 112, Train Loss: 1.2319349770524861\n",
      "Step: 113, Train Loss: 1.2301988572927944\n",
      "Step: 114, Train Loss: 1.226424648191618\n",
      "Step: 115, Train Loss: 1.2299727466085861\n",
      "Step: 116, Train Loss: 1.231366838909622\n",
      "Step: 117, Train Loss: 1.2317480745456986\n",
      "Step: 118, Train Loss: 1.2291140403567242\n",
      "Step: 119, Train Loss: 1.2277441956102848\n",
      "Step: 120, Train Loss: 1.2275163188946148\n",
      "Step: 121, Train Loss: 1.22169186810001\n",
      "Step: 122, Train Loss: 1.2215997259306715\n",
      "Step: 123, Train Loss: 1.2194992849423039\n",
      "Step: 124, Train Loss: 1.2191974971294404\n",
      "Step: 125, Train Loss: 1.2125036423168485\n",
      "Step: 126, Train Loss: 1.211606825430562\n",
      "Step: 127, Train Loss: 1.2104128012433648\n",
      "Step: 128, Train Loss: 1.2115506395813107\n",
      "Step: 129, Train Loss: 1.2080031876380628\n",
      "Step: 130, Train Loss: 1.2099511464133517\n",
      "Step: 131, Train Loss: 1.207846157930114\n",
      "Step: 132, Train Loss: 1.2023980153682536\n",
      "Step: 133, Train Loss: 1.2046122375264097\n",
      "Step: 134, Train Loss: 1.2063650221736342\n",
      "Step: 135, Train Loss: 1.203921795987031\n",
      "Step: 136, Train Loss: 1.2021745341102572\n",
      "Step: 137, Train Loss: 1.2005211950644203\n",
      "Step: 138, Train Loss: 1.202059607497222\n",
      "Step: 139, Train Loss: 1.2001511154430253\n",
      "Step: 140, Train Loss: 1.1979565888854629\n",
      "Step: 141, Train Loss: 1.2007376415209032\n",
      "Step: 142, Train Loss: 1.199536828519581\n",
      "Step: 143, Train Loss: 1.2005460475467973\n",
      "Step: 144, Train Loss: 1.1999953785846973\n",
      "Step: 145, Train Loss: 1.2007084135731605\n",
      "Step: 146, Train Loss: 1.200133699746359\n",
      "Step: 147, Train Loss: 1.1928029678560592\n",
      "Step: 148, Train Loss: 1.1924960731259928\n",
      "Step: 149, Train Loss: 1.1935529865821202\n",
      "Step: 150, Train Loss: 1.1938319196369473\n",
      "Step: 151, Train Loss: 1.1950757627032305\n",
      "Step: 152, Train Loss: 1.195996859299591\n",
      "Step: 153, Train Loss: 1.1975506348656368\n",
      "Step: 154, Train Loss: 1.200023583250661\n",
      "Step: 155, Train Loss: 1.198361373291566\n",
      "Step: 156, Train Loss: 1.1951298070181706\n",
      "Step: 157, Train Loss: 1.1964877355325072\n",
      "Step: 158, Train Loss: 1.193086045905479\n",
      "Step: 159, Train Loss: 1.1966455532237887\n",
      "Step: 160, Train Loss: 1.1961887998980765\n",
      "Step: 161, Train Loss: 1.1976531727446451\n",
      "Step: 162, Train Loss: 1.1989264310868972\n",
      "Step: 163, Train Loss: 1.1941952078444202\n",
      "Step: 164, Train Loss: 1.1946460931590108\n",
      "Step: 165, Train Loss: 1.1943769022283783\n",
      "Step: 166, Train Loss: 1.196022383288709\n",
      "Step: 167, Train Loss: 1.1905644207838035\n",
      "Step: 168, Train Loss: 1.191232058952546\n",
      "Step: 169, Train Loss: 1.1889521916122998\n",
      "Step: 170, Train Loss: 1.1882107831232729\n",
      "Step: 171, Train Loss: 1.191203358734763\n",
      "Step: 172, Train Loss: 1.1888551351996515\n",
      "Step: 173, Train Loss: 1.1843922098820236\n",
      "Step: 174, Train Loss: 1.1838376653194427\n",
      "Step: 175, Train Loss: 1.185077009045265\n",
      "Step: 176, Train Loss: 1.183264046907425\n",
      "Step: 177, Train Loss: 1.177274943653787\n",
      "Step: 178, Train Loss: 1.176731470493631\n",
      "Step: 179, Train Loss: 1.176081997073359\n",
      "Step: 180, Train Loss: 1.177401169805237\n",
      "Step: 181, Train Loss: 1.1799509766828882\n",
      "Step: 182, Train Loss: 1.1788929817292209\n",
      "Step: 183, Train Loss: 1.1791055046183907\n",
      "Step: 184, Train Loss: 1.1796570673987672\n",
      "Step: 185, Train Loss: 1.1806415619869386\n",
      "Step: 186, Train Loss: 1.180511120726718\n",
      "Step: 187, Train Loss: 1.181330319969578\n",
      "Step: 188, Train Loss: 1.1815831244149535\n",
      "Step: 189, Train Loss: 1.1816622814849804\n",
      "Step: 190, Train Loss: 1.1853954604002819\n",
      "Step: 191, Train Loss: 1.1851466533262283\n",
      "Step: 192, Train Loss: 1.1860691011724076\n",
      "Step: 193, Train Loss: 1.1844168243180846\n",
      "Step: 194, Train Loss: 1.1823450821332442\n",
      "Step: 195, Train Loss: 1.1826853483763275\n",
      "Step: 196, Train Loss: 1.1825609097474723\n",
      "Step: 197, Train Loss: 1.1831264452952328\n",
      "Step: 198, Train Loss: 1.1812660259068313\n",
      "Step: 199, Train Loss: 1.18049240373075\n",
      "Step: 200, Train Loss: 1.181822138268556\n",
      "Step: 201, Train Loss: 1.1834820517071403\n",
      "Step: 202, Train Loss: 1.1780800772784965\n",
      "Step: 203, Train Loss: 1.178311532071116\n",
      "Step: 204, Train Loss: 1.1771700900138877\n",
      "Step: 205, Train Loss: 1.1775873546823137\n",
      "Step: 206, Train Loss: 1.1747679650279634\n",
      "Step: 207, Train Loss: 1.173527563456446\n",
      "Step: 208, Train Loss: 1.173816432258444\n",
      "Step: 209, Train Loss: 1.1685451357492378\n",
      "Step: 210, Train Loss: 1.1679833661676584\n",
      "Step: 211, Train Loss: 1.1692169756048694\n",
      "Step: 212, Train Loss: 1.169143142029993\n",
      "Step: 213, Train Loss: 1.1676612129069377\n",
      "Step: 214, Train Loss: 1.1670002344389294\n",
      "Step: 215, Train Loss: 1.1631128039142047\n",
      "Step: 216, Train Loss: 1.1639423152337427\n",
      "Step: 217, Train Loss: 1.1634780405379763\n",
      "Step: 218, Train Loss: 1.164793584614856\n",
      "Step: 219, Train Loss: 1.1648274835199117\n",
      "Step: 220, Train Loss: 1.1651591932679193\n",
      "Step: 221, Train Loss: 1.168460746773997\n",
      "Step: 222, Train Loss: 1.1683456861986172\n",
      "Step: 223, Train Loss: 1.1663398888028627\n",
      "Step: 224, Train Loss: 1.1654851274026765\n",
      "Step: 225, Train Loss: 1.1663938166359358\n",
      "Step: 226, Train Loss: 1.1663560291284507\n",
      "Step: 227, Train Loss: 1.1677086276508737\n",
      "Step: 228, Train Loss: 1.170217505871729\n",
      "Step: 229, Train Loss: 1.171055815692829\n",
      "Step: 230, Train Loss: 1.170401282285973\n",
      "Step: 231, Train Loss: 1.1691920656132802\n",
      "Step: 232, Train Loss: 1.1686285408333648\n",
      "Step: 233, Train Loss: 1.1698641029751708\n",
      "Step: 234, Train Loss: 1.1695861523772808\n",
      "Step: 235, Train Loss: 1.1683959230086056\n",
      "Step: 236, Train Loss: 1.1706076203079163\n",
      "Step: 237, Train Loss: 1.170955080601598\n",
      "Step: 238, Train Loss: 1.168412867356793\n",
      "Step: 239, Train Loss: 1.1694873083693287\n",
      "Step: 240, Train Loss: 1.174705972222619\n",
      "Step: 241, Train Loss: 1.1755444800619743\n",
      "Step: 242, Train Loss: 1.1732731784383457\n",
      "Step: 243, Train Loss: 1.1738012556719486\n",
      "Step: 244, Train Loss: 1.1731609949347923\n",
      "Step: 245, Train Loss: 1.1739565697687913\n",
      "Step: 246, Train Loss: 1.1746386498635115\n",
      "Step: 247, Train Loss: 1.1724813156611016\n",
      "Step: 248, Train Loss: 1.174480085302309\n",
      "Step: 249, Train Loss: 1.1747824667990208\n",
      "Step: 250, Train Loss: 1.1737774517016106\n",
      "Step: 251, Train Loss: 1.172208225827605\n",
      "Step: 252, Train Loss: 1.1708210220508897\n",
      "Step: 253, Train Loss: 1.171217238251853\n",
      "Step: 254, Train Loss: 1.1727850048273218\n",
      "Step: 255, Train Loss: 1.1728370565979276\n",
      "Step: 256, Train Loss: 1.1712029916361149\n",
      "Step: 257, Train Loss: 1.1692248547319757\n",
      "Step: 258, Train Loss: 1.168837256096735\n",
      "Step: 259, Train Loss: 1.1694843247819406\n",
      "Step: 260, Train Loss: 1.1697040761167976\n",
      "Step: 261, Train Loss: 1.1699832269765493\n",
      "Step: 262, Train Loss: 1.1679856060179015\n",
      "Step: 263, Train Loss: 1.168409551098717\n",
      "Step: 264, Train Loss: 1.1651870979734187\n",
      "Step: 265, Train Loss: 1.164970587098733\n",
      "Step: 266, Train Loss: 1.1632001842284916\n",
      "Step: 267, Train Loss: 1.163405709774859\n",
      "Step: 268, Train Loss: 1.165577037211466\n",
      "Step: 269, Train Loss: 1.164792106135024\n",
      "Step: 270, Train Loss: 1.1640247896423639\n",
      "Step: 271, Train Loss: 1.1632775175933014\n",
      "Step: 272, Train Loss: 1.1641044282924125\n",
      "Step: 273, Train Loss: 1.1629886438213561\n",
      "Step: 274, Train Loss: 1.1631088454344056\n",
      "Step: 275, Train Loss: 1.1629880505033594\n",
      "Step: 276, Train Loss: 1.1609519147969756\n",
      "Step: 277, Train Loss: 1.1594421986034877\n",
      "Step: 278, Train Loss: 1.1600111340803485\n",
      "Step: 279, Train Loss: 1.1610238541715912\n",
      "Step: 280, Train Loss: 1.1610720240613743\n",
      "Step: 281, Train Loss: 1.1611491735786834\n",
      "Step: 282, Train Loss: 1.1605618943858484\n",
      "Step: 283, Train Loss: 1.1612072440362733\n",
      "Step: 284, Train Loss: 1.1601376232609415\n",
      "Step: 285, Train Loss: 1.159897040263131\n",
      "Step: 286, Train Loss: 1.15880241940765\n",
      "Step: 287, Train Loss: 1.1581767946740404\n",
      "Step: 288, Train Loss: 1.1560268905593862\n",
      "Step: 289, Train Loss: 1.1578433353582334\n",
      "Step: 290, Train Loss: 1.157584700823035\n",
      "Step: 291, Train Loss: 1.1571024191144803\n",
      "Step: 292, Train Loss: 1.1550608232820807\n",
      "Step: 293, Train Loss: 1.1534626315899041\n",
      "Step: 294, Train Loss: 1.1534921931260722\n",
      "Step: 295, Train Loss: 1.1532960765919573\n",
      "Step: 296, Train Loss: 1.1537691180402985\n",
      "Step: 297, Train Loss: 1.1529600750529927\n",
      "Step: 298, Train Loss: 1.1536152755785547\n",
      "Step: 299, Train Loss: 1.15373383226494\n",
      "Step: 300, Train Loss: 1.1521056330332724\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "optimizer.zero_grad()\n",
    "optimizer_memory = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_loss = 0\n",
    "    tr_steps = 0\n",
    "    for step, batch in enumerate((train_dataloader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        curr = memall()\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        input_memory = memall() - curr\n",
    "        \n",
    "        curr = memall()\n",
    "        output = model(**batch)\n",
    "        activation_memory = memall() - curr\n",
    "        \n",
    "        curr = memall()\n",
    "        # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        gradient_memory = memall() - input_memory - weight_memory - optimizer_memory\n",
    "\n",
    "        curr = memall()\n",
    "        optimizer.step()\n",
    "        if step == 0:\n",
    "             optimizer_memory = memall() - curr\n",
    "    \n",
    "        loss = loss.cpu()\n",
    "        train_loss += loss.item()\n",
    "        tr_steps += 1\n",
    "        train_losses.append(train_loss/tr_steps)\n",
    "        if step % 1 == 0:\n",
    "            print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "        torch.cuda.empty_cache()\n",
    "        if step == args.max_steps:\n",
    "            model.eval()\n",
    "            break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/13 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =     1.0292\n",
      "  eval_runtime            = 0:00:05.94\n",
      "  eval_samples_per_second =      16.83\n",
      "  eval_steps_per_second   =      2.188\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.eval()\n",
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = sum([max_memory_allocated(i) for i in range(gpus)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': './output',\n",
       " 'eval_loss': 1.0291874408721924,\n",
       " 'eval_runtime': 5.9418,\n",
       " 'eval_samples_per_second': 16.83,\n",
       " 'eval_steps_per_second': 2.188}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 551.56M\n",
      "(6738.423808, 551.555072)\n",
      "Method           : alpha\n",
      "Layers           : 20\n",
      "Learning Rate    : 5e-06\n",
      "Batch size       : 2\n",
      "Weight memory    : 27087.929344 MB\n",
      "Optimizer memory : 4412.440576 MB\n",
      "Activation memory: 1312.103424 MB\n",
      "Gradient memory  : 2245.021184 MB\n",
      "Input memory     : 0.003584 MB\n",
      "Total memory     : 31539.1744 MB\n",
      "Peak memory      : 38131.032064 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "memory_string = (\n",
    "        f\"{param_count(model)}\\n\"\n",
    "        f\"Method           : {args.sortby}\\n\"\n",
    "        f\"Layers           : {args.num_layers}\\n\"\n",
    "        f\"Learning Rate    : {args.learning_rate}\\n\"\n",
    "        f\"Batch size       : {args.per_device_train_batch_size}\\n\"\n",
    "        f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "        f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "        f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "        f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "        f\"Input memory     : {input_memory / 1e6} MB\\n\"\n",
    "        f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    "        f\"Peak memory      : {peek_memory / 1e6} MB\\n\"\n",
    "    )\n",
    "print(memory_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE TRAINING HISTORY\n",
    "base = {\"train_loss\": train_loss,}\n",
    "savepath = f\"./output/{args.dataset}/lr_{args.learning_rate}/batch_{args.per_device_train_batch_size}/{args.sortby}/layers_{args.num_layers}\"\n",
    "if args.memlog:\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(os.path.join(savepath, \"finetune.npy\"), base) # type: ignore\n",
    "    with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"Batch Size {args.per_device_train_batch_size} \"\n",
    "        + f\"{args.sortby} fine-tuning \"\n",
    "        + f\"{args.num_layers} Layers\"\n",
    "    )\n",
    "    logger = get_logger(savepath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
