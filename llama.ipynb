{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "# import accelerate.utils\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import ( \n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainerCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from tqdm import tqdm  \n",
    "from datasets import load_dataset\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/rscratch/tpang/kinshuk/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"]=\"/rscratch/tpang/kinshuk/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"meta-llama/Llama-2-7b-hf\"\n",
    "    )\n",
    "    trust_remote_code: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enable unpickling of arbitrary code in AutoModelForCausalLM#from_pretrained.\"}\n",
    "    )\n",
    "    use_auth_token: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"To use Huggingface auth token from Git Credentials.\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    eval_dataset_size: int = field(\n",
    "        default=1024, metadata={\"help\": \"Size of validation dataset.\"}\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging, truncate the number of train examples.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging, truncate the number of eval examples.\"\n",
    "        },\n",
    "    )\n",
    "    source_max_len: int = field(\n",
    "        default=1024,\n",
    "        metadata={\"help\": \"Maximum source sequence length.\"},\n",
    "    )\n",
    "    target_max_len: int = field(\n",
    "        default=256,\n",
    "        metadata={\"help\": \"Maximum target sequence length.\"},\n",
    "    )\n",
    "    dataset: str = field(\n",
    "        default='alpaca',\n",
    "        metadata={\"help\": \"Which dataset to finetune on. See datamodule for options.\"}\n",
    "    )\n",
    "    dataset_format: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Dataset format being used. [alpaca|chip2|self-instruct|hh-rlhf]\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.Seq2SeqTrainingArguments):\n",
    "    seed: Optional[int] = field(\n",
    "        default=7,\n",
    "        metadata={\"help\": \"Random seed for reproducibility.\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default='/rscratch/tpang/kinshuk/cache',\n",
    "    )\n",
    "    verbose: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to print verbose output.\"}\n",
    "    )\n",
    "    memlog: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to log memory usage.\"}\n",
    "    )\n",
    "    freeze: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to freeze the model.\"}\n",
    "    )\n",
    "    sortby: str = field(\n",
    "        default='random',\n",
    "        metadata={\"help\": \"Layer sorting method. [random|alpha|layer]\"}\n",
    "    )\n",
    "    num_layers: int = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Number of layers to train.\"}\n",
    "    )\n",
    "    sort_ascending: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to train in ascending order of layer sorting method.\"}\n",
    "    )\n",
    "    add_layer_norm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to add layer norm to the layers being trained.\"}\n",
    "    )\n",
    "    train_on_source: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to train on the input in addition to the target text.\"}\n",
    "    )\n",
    "    mmlu_split: Optional[str] = field(\n",
    "        default='eval',\n",
    "        metadata={\"help\": \"The MMLU split to run on\"}\n",
    "    )\n",
    "    mmlu_dataset: Optional[str] = field(\n",
    "        default='mmlu-fs',\n",
    "        metadata={\"help\": \"MMLU dataset to use: [`mmlu-zs`:zero-shot|`mmlu-fs`:few-shot].\"}\n",
    "    )\n",
    "    do_mmlu_eval: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to run the MMLU evaluation.\"}\n",
    "    )\n",
    "    max_mmlu_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If set, only evaluates on `max_mmlu_samples` of the MMMLU dataset.\"}\n",
    "    )\n",
    "    mmlu_source_max_len: int = field(\n",
    "        default=2048,\n",
    "        metadata={\"help\": \"Maximum source sequence length for MMLU.\"}\n",
    "    )\n",
    "    full_finetune: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Finetune the entire model without adapters.\"}\n",
    "    )\n",
    "    max_memory_MB: int = field(\n",
    "        default=12000,\n",
    "        metadata={\"help\": \"Free memory per gpu.\"}\n",
    "    )\n",
    "    report_to: str = field(\n",
    "        default='none',\n",
    "        metadata={\"help\": \"To use wandb or something else for reporting.\"}\n",
    "    )\n",
    "    output_dir: str = field(default='./output', metadata={\"help\": 'The output dir for logs and checkpoints'})\n",
    "    optim: str = field(default='paged_adamw_32bit', metadata={\"help\": 'The optimizer to be used'})\n",
    "    per_device_train_batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU.'})\n",
    "    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'Gradients to accumulate before performing an optimizer step'})\n",
    "    max_steps: int = field(default=10000, metadata={\"help\": 'How many optimizer update steps to take'})\n",
    "    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) # use lora dropout instead for regularization if needed\n",
    "    learning_rate: float = field(default=0.0002, metadata={\"help\": 'The learnign rate'})\n",
    "    remove_unused_columns: bool = field(default=False, metadata={\"help\": 'Removed unused columns. Needed to make this codebase work.'})\n",
    "    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n",
    "    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n",
    "    do_train: bool = field(default=True, metadata={\"help\": 'To train or not.'})\n",
    "    lr_scheduler_type: str = field(default='constant', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n",
    "    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n",
    "    logging_steps: int = field(default=10, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n",
    "    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n",
    "    save_strategy: str = field(default='steps', metadata={\"help\": 'When to save checkpoints'})\n",
    "    save_steps: int = field(default=250, metadata={\"help\": 'How often to save a model'})\n",
    "    save_total_limit: int = field(default=40, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n",
    "\n",
    "@dataclass\n",
    "class GenerationArguments:\n",
    "    # For more hyperparameters check:\n",
    "    # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    # Length arguments\n",
    "    max_new_tokens: Optional[int] = field(\n",
    "        default=256,\n",
    "        metadata={\"help\": \"Max number of new tokens to be generated in eval or prediction loops\"\n",
    "                          \"if predict_with_generate is set.\"}\n",
    "    )\n",
    "    min_new_tokens : Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Min number of new tokens to generate.\"}\n",
    "    )\n",
    "\n",
    "    # Generation strategy\n",
    "    do_sample: Optional[bool] = field(default=False)\n",
    "    num_beams: Optional[int] = field(default=1)\n",
    "    num_beam_groups: Optional[int] = field(default=1)\n",
    "    penalty_alpha: Optional[float] = field(default=None)\n",
    "    use_cache: Optional[bool] = field(default=True)\n",
    "\n",
    "    # Hyperparameters for logit manipulation\n",
    "    temperature: Optional[float] = field(default=1.0)\n",
    "    top_k: Optional[int] = field(default=50)\n",
    "    top_p: Optional[float] = field(default=1.0)\n",
    "    typical_p: Optional[float] = field(default=1.0)\n",
    "    diversity_penalty: Optional[float] = field(default=0.0)\n",
    "    repetition_penalty: Optional[float] = field(default=1.0)\n",
    "    length_penalty: Optional[float] = field(default=1.0)\n",
    "    no_repeat_ngram_size: Optional[int] = field(default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Borrowed from qlora codebase\n",
    "    Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings_data = model.get_input_embeddings().weight.data\n",
    "        output_embeddings_data = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings_data[:-num_new_tokens].mean( # type: ignore\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings_data[:-num_new_tokens].mean( # type: ignore\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings_data[-num_new_tokens:] = input_embeddings_avg # type: ignore\n",
    "        output_embeddings_data[-num_new_tokens:] = output_embeddings_avg # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    max_memory = f'{args.max_memory_MB}MB'\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "    device_map = \"auto\"\n",
    "\n",
    "    if os.environ.get('LOCAL_RANK') is not None:\n",
    "        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "        device_map = {'': local_rank}\n",
    "        max_memory = {'': max_memory[local_rank]}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path, \n",
    "        token=\"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\",\n",
    "        device_map=device_map,\n",
    "        max_memory=max_memory,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    setattr(model, 'model_parallel', True)\n",
    "    setattr(model, 'is_parallelizable', True)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        token=\"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\",\n",
    "        cache_dir=args.cache_dir,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False, # Fast tokenizer giving issues.\n",
    "        tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "        # use_auth_token=args.use_auth_token,\n",
    "    )\n",
    "    if tokenizer._pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer, # type: ignore\n",
    "            model=model,\n",
    "        )\n",
    "    if 'llama' in args.model_name_or_path or isinstance(tokenizer, LlamaTokenizer):\n",
    "        # LLaMA tokenizer may not have correct special tokens set.\n",
    "        # Check and add them if missing to prevent them from being parsed into different tokens.\n",
    "        # Note that these are present in the vocabulary.\n",
    "        # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
    "        print('Adding special tokens.')\n",
    "        tokenizer.add_special_tokens({\n",
    "                \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "                \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "                \"unk_token\": tokenizer.convert_ids_to_tokens(\n",
    "                    model.config.pad_token_id\n",
    "                    if model.config.pad_token_id != -1\n",
    "                    else tokenizer.pad_token_id # type: ignore\n",
    "                ),\n",
    "        })\n",
    "\n",
    "    # SELECTIVE FINETUNING >>>------------------------------------->\n",
    "\n",
    "    if args.freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for name, module in model.named_modules():\n",
    "            if 'norm' in name:\n",
    "                module = module.to(torch.float32)\n",
    "            if 'lm_head' in name or 'embed_tokens' in name:\n",
    "                if hasattr(module, 'weight'):\n",
    "                    if args.bf16 and module.weight.dtype == torch.float32:\n",
    "                        module = module.to(torch.bfloat16) \n",
    "        return model, tokenizer\n",
    "    \n",
    "    # if \"lora\" not in args.sortby.lower():\n",
    "    #     # Save WeightWatcher Metrics\n",
    "    #     watcher = ww.WeightWatcher(model=model)\n",
    "    #     ww_details = watcher.analyze(min_evals=10)\n",
    "\n",
    "    # if not args.debug and \"lora\" not in args.sortby.lower():\n",
    "    #     ww_details.to_csv(os.path.join(stats_path, f\"epoch_{epoch}.csv\"))  # type: ignore\n",
    "\n",
    "    ww_details = pd.read_csv(\"./llama_ww.csv\")\n",
    "    # CHOOSING LAYERS TO TRAIN BASED ON WEIGHTWATCHER METRICS/SORTBY\n",
    "    if \"lora\" not in args.sortby.lower():\n",
    "        filtered = ww_details[  # type: ignore\n",
    "            ww_details[\"longname\"].str.contains(\"embed_tokens\") == False  # type: ignore\n",
    "        ]\n",
    "        sortby = \"alpha\"\n",
    "        if args.num_layers > len(filtered):\n",
    "            args.num_layers = len(filtered)\n",
    "        if \"random\" in (args.sortby).lower():\n",
    "            train_names = random.sample(filtered[\"longname\"].to_list(), args.num_layers)\n",
    "        else:\n",
    "            if \"alpha\" in (args.sortby).lower():\n",
    "                sortby = \"alpha\"\n",
    "            elif \"layer\" in (args.sortby).lower():\n",
    "                sortby = \"layer_id\"\n",
    "            else:\n",
    "                sortby = \"random\"\n",
    "            train_names = (\n",
    "                filtered.sort_values(by=[sortby], ascending=args.sort_ascending)[ # type: ignore\n",
    "                    \"longname\"\n",
    "                ]\n",
    "                .iloc[: args.num_layers]\n",
    "                .to_list()\n",
    "            )\n",
    "        if args.verbose:\n",
    "            print(\"Sorted by \", sortby)\n",
    "            print(\"Training layers:\", train_names)\n",
    "        layer_to_train = []\n",
    "        for layer in train_names:\n",
    "            layer_to_train.append(layer + \".weight\")\n",
    "            layer_to_train.append(layer + \".bias\")\n",
    "            # Add Layer Norm\n",
    "            if args.add_layer_norm:\n",
    "                if \"output\" in layer:\n",
    "                    layer_to_train.append(\n",
    "                        layer.replace(\"dense\", \"LayerNorm\") + \".weight\"\n",
    "                    )\n",
    "                    layer_to_train.append(layer.replace(\"dense\", \"LayerNorm\") + \".bias\")\n",
    "        layer_to_train = list(set(layer_to_train))\n",
    "        # print(\"Final Training layers:\", layer_to_train)\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in layer_to_train:\n",
    "                if args.verbose:\n",
    "                    print(f\"Enabling {name} parameter\")\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if hasattr(module, 'weight'):\n",
    "                if args.bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16) \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForCausalLM(object):\n",
    "    \"\"\"Borrowed from qlora codebase.\"\"\"\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    source_max_len: int\n",
    "    target_max_len: int\n",
    "    train_on_source: bool\n",
    "    predict_with_generate: bool\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract elements\n",
    "        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n",
    "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n",
    "        # Tokenize\n",
    "        tokenized_sources_with_prompt = self.tokenizer(\n",
    "            sources,\n",
    "            max_length=self.source_max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        tokenized_targets = self.tokenizer(\n",
    "            targets,\n",
    "            max_length=self.target_max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        # Build the input and labels for causal LM\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        for tokenized_source, tokenized_target in zip(\n",
    "            tokenized_sources_with_prompt['input_ids'], # type: ignore\n",
    "            tokenized_targets['input_ids'] # type: ignore\n",
    "        ):\n",
    "            if not self.predict_with_generate:\n",
    "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
    "                if not self.train_on_source:\n",
    "                    labels.append(\n",
    "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
    "                    )\n",
    "                else:\n",
    "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
    "            else:\n",
    "                input_ids.append(torch.tensor(tokenized_source))\n",
    "        # Apply padding\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id) # type: ignore\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id), # type: ignore\n",
    "        }\n",
    "        if labels is not None:\n",
    "            data_dict['labels'] = labels\n",
    "        return data_dict\n",
    "\n",
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "\n",
    "def local_dataset(dataset_name):\n",
    "    if dataset_name.endswith('.json') or dataset_name.endswith('.jsonl'):\n",
    "        full_dataset = Dataset.from_json(path_or_paths=dataset_name)\n",
    "    elif dataset_name.endswith('.csv'):\n",
    "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name))\n",
    "    elif dataset_name.endswith('.tsv'):\n",
    "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name, delimiter='\\t'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset format: {dataset_name}\")\n",
    "\n",
    "    split_dataset = full_dataset.train_test_split(test_size=0.1) # type: ignore\n",
    "    return split_dataset\n",
    "\n",
    "def make_data_module(tokenizer: PreTrainedTokenizer, args) -> Dict:\n",
    "    \"\"\"\n",
    "    Make dataset and collator for supervised fine-tuning.\n",
    "    Datasets are expected to have the following columns: { `input`, `output` }\n",
    "\n",
    "    Available datasets to be selected with `dataset` argument:\n",
    "        - alpaca, 52002 examples\n",
    "        - alpaca cleaned, 51942 examples\n",
    "        - chip2 (OIG), 210289 examples\n",
    "        - self-instruct, 82612 examples\n",
    "        - hh-rlhf (Anthropic), 160800 examples\n",
    "        - longform, 23.7k examples\n",
    "        - oasst1 (OpenAssistant) primary message tree only, 9,846 examples\n",
    "\n",
    "    Coming soon:\n",
    "        - unnatural instructions core, 66010 examples\n",
    "        - unnatural instructions full, 240670 examples\n",
    "        - alpaca-gpt4, 52002 examples\n",
    "        - unnatural-instructions-gpt4, 9000 examples\n",
    "        - supernatural-instructions, 69624 examples (same as paper with 100 ex/task more can be used)\n",
    "        - flan (FLAN v2), up to 20M examples available\n",
    "        - vicuna\n",
    "\n",
    "    \"\"\"\n",
    "    def load_data(dataset_name):\n",
    "        if dataset_name == 'alpaca':\n",
    "            return load_dataset(\"tatsu-lab/alpaca\")\n",
    "        elif dataset_name == 'alpaca-clean':\n",
    "            return load_dataset(\"yahma/alpaca-cleaned\")\n",
    "        elif dataset_name == 'chip2':\n",
    "            return load_dataset(\"laion/OIG\", data_files='unified_chip2.jsonl')\n",
    "        elif dataset_name == 'self-instruct':\n",
    "            return load_dataset(\"yizhongw/self_instruct\", name='self_instruct')\n",
    "        elif dataset_name == 'hh-rlhf':\n",
    "            return load_dataset(\"Anthropic/hh-rlhf\")\n",
    "        elif dataset_name == 'longform':\n",
    "            return load_dataset(\"akoksal/LongForm\")\n",
    "        elif dataset_name == 'oasst1':\n",
    "            return load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "        elif dataset_name == 'vicuna':\n",
    "            raise NotImplementedError(\"Vicuna data was not released.\")\n",
    "        else:\n",
    "            if os.path.exists(dataset_name):\n",
    "                try:\n",
    "                    args.dataset_format = args.dataset_format if args.dataset_format else \"input-output\"\n",
    "                    full_dataset = local_dataset(dataset_name)\n",
    "                    return full_dataset\n",
    "                except:\n",
    "                    raise ValueError(f\"Error loading dataset from {dataset_name}\")\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Dataset {dataset_name} not implemented yet.\")\n",
    "\n",
    "    def format_dataset(dataset, dataset_format):\n",
    "        if (\n",
    "            dataset_format == 'alpaca' or dataset_format == 'alpaca-clean' or\n",
    "            (dataset_format is None and args.dataset in ['alpaca', 'alpaca-clean'])\n",
    "        ):\n",
    "            dataset = dataset.map(extract_alpaca_dataset, remove_columns=['instruction'])\n",
    "        elif dataset_format == 'chip2' or (dataset_format is None and args.dataset == 'chip2'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': x['text'].split('\\n<bot>: ')[0].replace('<human>: ', ''),\n",
    "                'output': x['text'].split('\\n<bot>: ')[1],\n",
    "            })\n",
    "        elif dataset_format == 'self-instruct' or (dataset_format is None and args.dataset == 'self-instruct'):\n",
    "            for old, new in [[\"prompt\", \"input\"], [\"completion\", \"output\"]]:\n",
    "                dataset = dataset.rename_column(old, new)\n",
    "        elif dataset_format == 'hh-rlhf' or (dataset_format is None and args.dataset == 'hh-rlhf'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': '',\n",
    "                'output': x['chosen']\n",
    "            })\n",
    "        elif dataset_format == 'oasst1' or (dataset_format is None and args.dataset == 'oasst1'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': '',\n",
    "                'output': x['text'],\n",
    "            })\n",
    "        elif dataset_format == 'input-output':\n",
    "            # leave as is\n",
    "            pass\n",
    "        # Remove unused columns.\n",
    "        dataset = dataset.remove_columns(\n",
    "            [col for col in dataset.column_names['train'] if col not in ['input', 'output']]\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "     # Load dataset.\n",
    "    dataset = load_data(args.dataset)\n",
    "    dataset = format_dataset(dataset, args.dataset_format)\n",
    "\n",
    "    # Split train/eval, reduce size\n",
    "    if args.do_eval or args.do_predict:\n",
    "        if 'eval' in dataset:\n",
    "            eval_dataset = dataset['eval']\n",
    "        else:\n",
    "            print('Splitting train dataset in train and validation according to `eval_dataset_size`')\n",
    "            dataset = dataset[\"train\"].train_test_split(\n",
    "                test_size=args.eval_dataset_size, shuffle=True, seed=42\n",
    "            )\n",
    "            eval_dataset = dataset['test']\n",
    "        if args.max_eval_samples is not None and len(eval_dataset) > args.max_eval_samples:\n",
    "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "        if args.group_by_length:\n",
    "            eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
    "    if args.do_train:\n",
    "        train_dataset = dataset['train']\n",
    "        if args.max_train_samples is not None and len(train_dataset) > args.max_train_samples:\n",
    "            train_dataset = train_dataset.select(range(args.max_train_samples))\n",
    "        if args.group_by_length:\n",
    "            train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
    "\n",
    "    data_collator = DataCollatorForCausalLM(\n",
    "        tokenizer=tokenizer,\n",
    "        source_max_len=args.source_max_len,\n",
    "        target_max_len=args.target_max_len,\n",
    "        train_on_source=args.train_on_source,\n",
    "        predict_with_generate=args.predict_with_generate,\n",
    "    )\n",
    "    return dict(\n",
    "        train_dataset=train_dataset if args.do_train else None,\n",
    "        eval_dataset=eval_dataset if args.do_eval else None,\n",
    "        predict_dataset=eval_dataset if args.do_predict else None,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_func(args, logger, trainer, all_metrics):\n",
    "    if args.do_train:\n",
    "        logger.info(\"*** Train ***\")\n",
    "        # Note: `resume_from_checkpoint` not supported for adapter checkpoints by HF.\n",
    "        # Currently adapter checkpoint is reloaded as expected but optimizer/scheduler states are not.\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        all_metrics.update(metrics)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "def eval_func(args, logger, trainer, all_metrics):\n",
    "    if args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "        all_metrics.update(metrics)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmlu_callback(args, tokenizer, trainer):\n",
    "    if args.do_mmlu_eval:\n",
    "        if args.mmlu_dataset == 'mmlu-zs':\n",
    "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
    "                'eval': 'data/mmlu/zero_shot_mmlu_val.json',\n",
    "                'test': 'data/mmlu/zero_shot_mmlu_test.json',\n",
    "            })\n",
    "            mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
    "        # MMLU Five-shot (Eval/Test only)\n",
    "        elif args.mmlu_dataset == 'mmlu' or args.mmlu_dataset == 'mmlu-fs':\n",
    "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
    "                'eval': 'data/mmlu/five_shot_mmlu_val.json',\n",
    "                'test': 'data/mmlu/five_shot_mmlu_test.json',\n",
    "            })\n",
    "            # mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
    "        mmlu_dataset = mmlu_dataset[args.mmlu_split] # type: ignore\n",
    "        if args.max_mmlu_samples is not None:\n",
    "            mmlu_dataset = mmlu_dataset.select(range(args.max_mmlu_samples))\n",
    "        abcd_idx = [\n",
    "            tokenizer(\"A\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"B\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"C\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"D\", add_special_tokens=False).input_ids[0],\n",
    "        ]\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "        class MMLUEvalCallback(TrainerCallback):\n",
    "            def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "                data_loader = trainer.get_eval_dataloader(mmlu_dataset)\n",
    "                source_max_len = trainer.data_collator.source_max_len\n",
    "                trainer.data_collator.source_max_len = args.mmlu_source_max_len\n",
    "                trainer.model.eval()\n",
    "                preds, refs = [], []\n",
    "                loss_mmlu = 0\n",
    "                for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "                    (loss, logits, labels) = trainer.prediction_step(trainer.model,batch,prediction_loss_only=False,)\n",
    "                    # There are two tokens, the output, and eos token.\n",
    "                    for i, logit in enumerate(logits):\n",
    "                        label_non_zero_id = (batch['labels'][i] != -100).nonzero()[0][0]\n",
    "                        logit_abcd = logit[label_non_zero_id-1][abcd_idx]\n",
    "                        preds.append(torch.argmax(logit_abcd).item())\n",
    "                    labels = labels[labels != IGNORE_INDEX].view(-1, 2)[:,0]\n",
    "                    refs += [abcd_idx.index(label) for label in labels.tolist()]\n",
    "                    loss_mmlu += loss.item()\n",
    "                # Extract results by subject.\n",
    "                results = {'mmlu_loss':loss_mmlu/len(data_loader)}\n",
    "                subject = mmlu_dataset['subject']\n",
    "                subjects = {s:{'refs':[], 'preds':[]} for s in set(subject)}\n",
    "                for s,p,r in zip(subject, preds, refs):\n",
    "                    subjects[s]['preds'].append(p)\n",
    "                    subjects[s]['refs'].append(r)\n",
    "                subject_scores = []\n",
    "                for subject in subjects:\n",
    "                    subject_score = accuracy.compute(\n",
    "                        references=subjects[subject]['refs'],\n",
    "                        predictions=subjects[subject]['preds']\n",
    "                    )['accuracy'] # type: ignore\n",
    "                    results[f'mmlu_{args.mmlu_split}_accuracy_{subject}'] = subject_score\n",
    "                    subject_scores.append(subject_score)\n",
    "                results[f'mmlu_{args.mmlu_split}_accuracy'] = np.mean(subject_scores)\n",
    "                trainer.log(results)\n",
    "                trainer.data_collator.source_max_len = source_max_len\n",
    "\n",
    "        trainer.add_callback(MMLUEvalCallback)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global logger\n",
    "    hfparser = transformers.HfArgumentParser((\n",
    "        ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "    )) # type: ignore\n",
    "    \n",
    "    model_args, data_args, training_args, generation_args, extra_args = \\\n",
    "        hfparser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "    \n",
    "    training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "    args = argparse.Namespace(\n",
    "        **vars(model_args), **vars(data_args), **vars(training_args)\n",
    "    )\n",
    "    print(args)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = args.cache_dir\n",
    "    cuda_device = torch.cuda.current_device()\n",
    "\n",
    "    # Memory Log Path\n",
    "    mempath = (\n",
    "        f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/alpaca/\"\n",
    "        + f\"trainseed_{args.seed}/{args.sortby}\"\n",
    "    )\n",
    "    \n",
    "    # Control randomness\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # accelerate.utils.set_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    set_seed(args.seed)  # transformers seed\n",
    "\n",
    "    # Memory Stats Initialization\n",
    "    reset_peak_memory_stats(device=cuda_device)\n",
    "    reset_max_memory_allocated(device=cuda_device)\n",
    "    start_memory = memory_allocated(device=cuda_device)\n",
    "\n",
    "    if args.verbose:\n",
    "        print(\"SEED:\", args.seed)\n",
    "        task_info = (\n",
    "            f\"\\n\\n\\nDataset to finetune on: {args.dataset}\\n\\n\\n\"\n",
    "            + f\"alpha Decreasing: {not args.sort_ascending}\\n\\n\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\\n\\n\"\n",
    "            + f\"Train randomly: {'random' in args.sortby.lower()}\\n\\n\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "    else:\n",
    "        datasets_vb_err()\n",
    "        transformers_vb_err()\n",
    "        global _tqdm_active\n",
    "        _tqdm_active = False\n",
    "\n",
    "    # WIP >>>------------------------------------------>\n",
    "\n",
    "    model, tokenizer = get_model(args)\n",
    "\n",
    "    data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "    )\n",
    "\n",
    "    if args.do_mmlu_eval:\n",
    "        trainer = mmlu_callback(args, tokenizer, trainer)\n",
    "\n",
    "    all_metrics = {\"run_name\": args.run_name}\n",
    "\n",
    "    # Train\n",
    "    if args.do_train:\n",
    "        all_metrics = train_func(args, logger, trainer, all_metrics)\n",
    "    \n",
    "    # Eval\n",
    "    if args.do_eval:\n",
    "        all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "\n",
    "    end_memory = memory_allocated(device=cuda_device)\n",
    "    peek_memory = max_memory_allocated(device=cuda_device)\n",
    "    print(\n",
    "        f\"\\n\\n\\nMemory usage before: {start_memory} bytes\\nMemory usage after: {int((end_memory/1024)/1024)}MB\"\n",
    "    )\n",
    "    print(f\"\\nPeak Memory usage: {int((peek_memory/1024)/1024)}MB\\n\\n\\n\")\n",
    "\n",
    "    # WIP <-----------------------------------------<<<\n",
    "\n",
    "    if args.memlog: # Memory Logging\n",
    "        log_info = (\n",
    "            f\"\\n\\n{args.dataset} \"\n",
    "            + f\"{args.num_layers} Layers \"\n",
    "            + f\"{args.sortby} \"\n",
    "            + f\"ascending {args.alpha_ascending}\"\n",
    "        )\n",
    "        Path(mempath).mkdir(parents=True, exist_ok=True)\n",
    "        logger = get_logger(mempath, \"memlog.log\")\n",
    "        logger.info(log_info)\n",
    "        logger.info(\n",
    "            f\"\\nMemory usage before: {(start_memory/1024)/1024}MB\\n\"\n",
    "            + f\"Memory usage after: {(end_memory/1024)/1024}MB\"\n",
    "        )\n",
    "        logger.info(f\"\\nPeak Memory usage: {(peek_memory/1024)/1024}MB\\n\\n\")\n",
    "\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
