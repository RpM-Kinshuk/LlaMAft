{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-02 13:16:07.249153: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 13:16:07.249236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 13:16:07.250971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 13:16:07.264667: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 13:16:13.706771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "\n",
    "    dataset=\"alpaca\", # DATASET [alpaca|chip2|self-instruct|hh-rlhf|oasst1|longform]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "\n",
    "    learning_rate=2e-7,     # LEARNING RATE\n",
    "    \n",
    "    max_steps=1500,         # NUMBER OF STEPS\n",
    "\n",
    "    sortby=\"alpha\",         # CAN DO \"alpha\" or \"lora\"\n",
    "\n",
    "    num_layers=4,           # NUMBER OF LAYERS FOR FULL FINE-TUNING\n",
    "\n",
    "    per_device_train_batch_size = 4, # BATCH-SIZE\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=128 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "Dataset: alpaca\n",
      "Sort by: alpha\n",
      "Layers to train: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lora' in args.sortby:\n",
    "    args.num_layers = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj']\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 50978\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'predict_dataset': None,\n",
       " 'data_collator': DataCollatorForCausalLM(tokenizer=LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[PAD]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False), source_max_len=1024, target_max_len=256, train_on_source=False, predict_with_generate=False)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {k:v for k,v in data_module.items()}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12745"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "input_memory = memall()- weight_memory\n",
    "\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/12745 [00:01<4:45:49,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.3192678689956665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/12745 [00:21<1:28:36,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50, Train Loss: 1.5059875565416672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/12745 [00:41<1:23:20,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Train Loss: 1.4654810469929536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 151/12745 [01:01<1:12:23,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 150, Train Loss: 1.449091753817552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/12745 [01:21<1:06:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Train Loss: 1.454192180538652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 251/12745 [01:42<1:40:06,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 250, Train Loss: 1.4414068139881726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 301/12745 [02:03<1:34:22,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Train Loss: 1.429423480334868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 351/12745 [02:24<1:23:33,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 350, Train Loss: 1.42841036655964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 401/12745 [02:46<1:37:50,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Train Loss: 1.421870716640777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 451/12745 [03:05<1:26:55,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 450, Train Loss: 1.4228128291286544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 501/12745 [03:27<1:19:27,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, Train Loss: 1.41930371308755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 551/12745 [03:49<1:22:38,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 550, Train Loss: 1.419229283415038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 601/12745 [04:11<1:27:09,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600, Train Loss: 1.4153620021513813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 651/12745 [04:32<1:25:22,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 650, Train Loss: 1.4097506576053191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 701/12745 [04:54<1:38:12,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 700, Train Loss: 1.4054401584086507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 751/12745 [05:17<1:41:56,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 750, Train Loss: 1.398237930871199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 801/12745 [05:37<1:19:57,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 800, Train Loss: 1.392177679714341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 851/12745 [05:57<1:11:04,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 850, Train Loss: 1.3871572397990737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 901/12745 [06:19<59:39,  3.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900, Train Loss: 1.3809269408671625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 951/12745 [06:40<1:24:25,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 950, Train Loss: 1.3748086648909952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1001/12745 [06:59<1:15:59,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Train Loss: 1.370990248290928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1051/12745 [07:19<1:10:06,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1050, Train Loss: 1.3688514456649148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 1101/12745 [07:39<1:24:59,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1100, Train Loss: 1.3656718370375256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1151/12745 [08:02<1:13:38,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1150, Train Loss: 1.3637388011976286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1201/12745 [08:23<1:28:38,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1200, Train Loss: 1.3619740419244886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1251/12745 [08:45<1:19:40,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1250, Train Loss: 1.3611805074506527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1301/12745 [09:06<1:12:36,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1300, Train Loss: 1.358691403675226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1351/12745 [09:27<1:22:48,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1350, Train Loss: 1.3564630690810418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1401/12745 [09:47<1:25:51,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1400, Train Loss: 1.3554357693588452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1451/12745 [10:07<1:13:25,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1450, Train Loss: 1.3500138837086753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1500/12745 [10:28<1:18:30,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500, Train Loss: 1.3474017736278003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "        train_loss = 0\n",
    "        tr_steps = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            activation_memory = memall() - weight_memory\n",
    "            # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "            loss = output.loss\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            gradient_memory = memall() - weight_memory\n",
    "            optimizer.step()\n",
    "            optimizer_memory = memall() - gradient_memory - weight_memory \n",
    "            tr_steps += 1\n",
    "            train_losses.append(train_loss/tr_steps)\n",
    "            if step % 50 == 0:\n",
    "                print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "            if step == args.max_steps:\n",
    "                model.eval()\n",
    "                break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/125 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =     1.2754\n",
      "  eval_runtime            = 0:01:06.73\n",
      "  eval_samples_per_second =     14.984\n",
      "  eval_steps_per_second   =      1.873\n"
     ]
    }
   ],
   "source": [
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = max([max_memory_allocated(i) for i in range(gpus)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 198.18M\n",
      "Method           : alpha\n",
      "Layers           : 4\n",
      "Batch size       : 4\n",
      "Weight memory    : 27087.929344 MB\n",
      "Activation memory: 4742.84288 MB\n",
      "Gradient memory  : 2482.826752 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 29570.756096 MB\n",
      "Peak memory      : 44877.82656 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_count(model)\n",
    "memory_string = (\n",
    "    f\"Method           : {args.sortby}\\n\"\n",
    "    f\"Layers           : {args.num_layers}\\n\"\n",
    "    f\"Batch size       : {args.per_device_train_batch_size}\\n\"\n",
    "    f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "    f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "    f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "    f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "    f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    "    f\"Peak memory      : {peek_memory / 1e6} MB\\n\"\n",
    ")\n",
    "print(memory_string)\n",
    "if args.memlog:\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"{args.sortby} \"\n",
    "        + f\"{args.num_layers} Layers \"\n",
    "        + f\"Batch Size: {args.per_device_train_batch_size}\"\n",
    "    )\n",
    "    Path(mempath).mkdir(parents=True, exist_ok=True)\n",
    "    logger = get_logger(mempath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "\n",
    "if (args.do_train or args.do_eval or args.do_predict):\n",
    "    with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE TRAINING HISTORY\n",
    "base = {\"train_loss\": train_loss,}\n",
    "savepath = f\"./output/{args.dataset}/batch_{args.per_device_train_batch_size}/{args.sortby}/layers_{args.num_layers}\"\n",
    "if True:\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(os.path.join(savepath, \"finetune.npy\"), base) # type: ignore\n",
    "    with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"Batch Size {args.per_device_train_batch_size} \"\n",
    "        + f\"{args.sortby} fine-tuning \"\n",
    "        + f\"{args.num_layers} Layers\"\n",
    "    )\n",
    "    logger = get_logger(savepath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
