{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-03 03:20:20.413405: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-03 03:20:20.413439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-03 03:20:20.414840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-03 03:20:20.423192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 03:20:30.352095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "\n",
    "    dataset=\"alpaca\", # DATASET [alpaca|chip2|self-instruct|hh-rlhf|oasst1|longform]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "\n",
    "    learning_rate=2e-6,     # LEARNING RATE\n",
    "    \n",
    "    max_steps=2000,         # NUMBER OF STEPS\n",
    "\n",
    "    sortby=\"alpha\",         # CAN DO \"alpha\" or \"lora\"\n",
    "\n",
    "    num_layers=4,           # NUMBER OF LAYERS FOR FULL FINE-TUNING\n",
    "\n",
    "    per_device_train_batch_size = 2, # BATCH-SIZE\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=128 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "Dataset: alpaca\n",
      "Sort by: alpha\n",
      "Layers to train: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lora' in args.sortby:\n",
    "    args.num_layers = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj']\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 50978\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'predict_dataset': None,\n",
       " 'data_collator': DataCollatorForCausalLM(tokenizer=LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[PAD]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False), source_max_len=1024, target_max_len=256, train_on_source=False, predict_with_generate=False)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {k:v for k,v in data_module.items()}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25489"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "input_memory = memall()- weight_memory\n",
    "\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/25489 [00:01<3:39:12,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.2829551696777344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/25489 [00:11<1:23:37,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50, Train Loss: 1.633158242001253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 102/25489 [00:22<1:30:09,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Train Loss: 1.5235318393990545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 151/25489 [00:33<1:36:27,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 150, Train Loss: 1.4431651913567094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 201/25489 [00:44<1:18:53,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Train Loss: 1.4104549600117242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 251/25489 [00:54<1:24:44,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 250, Train Loss: 1.3686336842903577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 301/25489 [01:05<1:28:38,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Train Loss: 1.3517712794269041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 352/25489 [01:17<1:38:55,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 350, Train Loss: 1.3536584560687726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 402/25489 [01:27<1:18:26,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Train Loss: 1.3303687304184026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 451/25489 [01:37<1:37:43,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 450, Train Loss: 1.3162758380108557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 501/25489 [01:49<1:36:30,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, Train Loss: 1.3065542754774797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 551/25489 [02:00<1:39:11,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 550, Train Loss: 1.2942384045483197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 601/25489 [02:11<1:32:45,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600, Train Loss: 1.2926947735311982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 651/25489 [02:22<1:27:08,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 650, Train Loss: 1.2878620744483995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 701/25489 [02:33<1:31:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 700, Train Loss: 1.2831243690682546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 752/25489 [02:44<1:18:41,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 750, Train Loss: 1.2760449403056133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 801/25489 [02:56<1:43:51,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 800, Train Loss: 1.269196626510513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 851/25489 [03:06<1:28:55,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 850, Train Loss: 1.2716259036375128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 901/25489 [03:17<1:32:59,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900, Train Loss: 1.2671615946107646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 951/25489 [03:29<1:49:41,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 950, Train Loss: 1.2636506722928598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1001/25489 [03:40<1:26:33,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Train Loss: 1.260070458128974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1051/25489 [03:51<1:29:59,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1050, Train Loss: 1.258226943940463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1102/25489 [04:03<1:23:05,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1100, Train Loss: 1.2594866162397773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1151/25489 [04:14<1:28:49,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1150, Train Loss: 1.2579274865127874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1201/25489 [04:26<1:29:43,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1200, Train Loss: 1.2549936870278964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1251/25489 [04:37<1:28:05,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1250, Train Loss: 1.2506046679070433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1301/25489 [04:48<1:33:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1300, Train Loss: 1.2501974799122653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1351/25489 [04:59<1:29:02,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1350, Train Loss: 1.2474244391031393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1401/25489 [05:10<1:35:32,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1400, Train Loss: 1.2450046826374692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1451/25489 [05:22<1:17:55,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1450, Train Loss: 1.240957829518453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1501/25489 [05:33<1:39:05,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500, Train Loss: 1.2393660918424163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1552/25489 [05:44<1:13:24,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1550, Train Loss: 1.2358210859838261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1602/25489 [05:55<1:25:44,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1600, Train Loss: 1.2333336866214677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1652/25489 [06:05<1:14:46,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1650, Train Loss: 1.2292978589387462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1702/25489 [06:17<1:18:04,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1700, Train Loss: 1.2268903006146334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1752/25489 [06:28<1:15:32,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1750, Train Loss: 1.2231265308344996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1802/25489 [06:39<1:02:35,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1800, Train Loss: 1.2217645296813513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1851/25489 [06:50<1:36:08,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1850, Train Loss: 1.218809972560966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1901/25489 [07:01<1:26:53,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1900, Train Loss: 1.215531432804216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1951/25489 [07:12<1:24:35,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1950, Train Loss: 1.2118963566133758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2000/25489 [07:23<1:26:42,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000, Train Loss: 1.2110853766885654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "        train_loss = 0\n",
    "        tr_steps = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            activation_memory = memall() - weight_memory\n",
    "            # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "            loss = output.loss\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            gradient_memory = memall() - weight_memory\n",
    "            optimizer.step()\n",
    "            optimizer_memory = memall() - gradient_memory - weight_memory \n",
    "            tr_steps += 1\n",
    "            train_losses.append(train_loss/tr_steps)\n",
    "            if step % 50 == 0:\n",
    "                print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "            if step == args.max_steps:\n",
    "                model.eval()\n",
    "                break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 36/125 00:17 < 00:44, 1.99 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =     1.1706\n",
      "  eval_runtime            = 0:01:07.61\n",
      "  eval_samples_per_second =     14.791\n",
      "  eval_steps_per_second   =      1.849\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.eval()\n",
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = max([max_memory_allocated(i) for i in range(gpus)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': './output',\n",
       " 'eval_loss': 1.1705690622329712,\n",
       " 'eval_runtime': 67.611,\n",
       " 'eval_samples_per_second': 14.791,\n",
       " 'eval_steps_per_second': 1.849}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 198.18M\n",
      "Method           : alpha\n",
      "Layers           : 4\n",
      "Learning Rate    : 2e-06\n",
      "Batch size       : 2\n",
      "Weight memory    : 27087.929344 MB\n",
      "Activation memory: 3427.019264 MB\n",
      "Gradient memory  : 2445.723648 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 28740.913152 MB\n",
      "Peak memory      : 37007.917056 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_count(model)\n",
    "memory_string = (\n",
    "    f\"Method           : {args.sortby}\\n\"\n",
    "    f\"Layers           : {args.num_layers}\\n\"\n",
    "    f\"Learning Rate    : {args.learning_rate}\\n\"\n",
    "    f\"Batch size       : {args.per_device_train_batch_size}\\n\"\n",
    "    f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "    f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "    f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "    f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "    f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    "    f\"Peak memory      : {peek_memory / 1e6} MB\\n\"\n",
    ")\n",
    "print(memory_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "alpaca Batch Size 2 alpha fine-tuning 4 Layers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-03 03:29:40;INFO;\n",
      "\n",
      "alpaca Batch Size 2 alpha fine-tuning 4 Layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Method           : alpha\n",
      "Layers           : 4\n",
      "Learning Rate    : 2e-06\n",
      "Batch size       : 2\n",
      "Weight memory    : 27087.929344 MB\n",
      "Activation memory: 3427.019264 MB\n",
      "Gradient memory  : 2445.723648 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 28740.913152 MB\n",
      "Peak memory      : 37007.917056 MB\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-03 03:29:40;INFO;\n",
      "Method           : alpha\n",
      "Layers           : 4\n",
      "Learning Rate    : 2e-06\n",
      "Batch size       : 2\n",
      "Weight memory    : 27087.929344 MB\n",
      "Activation memory: 3427.019264 MB\n",
      "Gradient memory  : 2445.723648 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 28740.913152 MB\n",
      "Peak memory      : 37007.917056 MB\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAVE TRAINING HISTORY\n",
    "base = {\"train_loss\": train_loss,}\n",
    "savepath = f\"./output/{args.dataset}/lr_{args.learning_rate}/batch_{args.per_device_train_batch_size}/{args.sortby}/layers_{args.num_layers}\"\n",
    "if True:\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(os.path.join(savepath, \"finetune.npy\"), base) # type: ignore\n",
    "    with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"Batch Size {args.per_device_train_batch_size} \"\n",
    "        + f\"{args.sortby} fine-tuning \"\n",
    "        + f\"{args.num_layers} Layers\"\n",
    "    )\n",
    "    logger = get_logger(savepath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
