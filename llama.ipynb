{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-29 12:25:05.698668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 12:25:05.700555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 12:25:05.702294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 12:25:05.736896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 12:25:10.774496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=50,\n",
    "    dataset=\"alpaca\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    logging_steps=10,\n",
    "    data_seed=42,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=False,\n",
    "    max_steps=5,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "    seed=7,\n",
    "    sortby=\"alpha\",\n",
    "    num_layers=100,\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    # Define generation-specific arguments here, if any are required\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# accelerate.utils.set_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=1024, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='./output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/Mar29_12-25-22_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=0, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=False, freeze=True, sortby='alpha', num_layers=100, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=3, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "\n",
      "Seed: 7\n",
      "\n",
      "Dataset: alpaca\n",
      "\n",
      "Sort by: alpha\n",
      "\n",
      "Layers to train: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "print(args)\n",
    "gpus = torch.cuda.device_count()\n",
    "# Memory Log Path\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 16.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.27.self_attn.v_proj', 'model.layers.11.self_attn.v_proj', 'model.layers.23.self_attn.o_proj', 'model.layers.3.mlp.up_proj', 'model.layers.21.self_attn.o_proj', 'model.layers.25.mlp.up_proj', 'model.layers.24.self_attn.o_proj', 'model.layers.20.self_attn.v_proj', 'model.layers.26.mlp.up_proj', 'model.layers.27.self_attn.o_proj', 'model.layers.25.self_attn.o_proj', 'model.layers.22.self_attn.o_proj', 'model.layers.24.mlp.up_proj', 'model.layers.19.self_attn.o_proj', 'model.layers.23.self_attn.v_proj', 'model.layers.27.mlp.up_proj', 'model.layers.22.mlp.down_proj', 'model.layers.20.self_attn.o_proj', 'model.layers.18.self_attn.o_proj', 'model.layers.25.self_attn.v_proj', 'model.layers.30.mlp.down_proj', 'model.layers.28.self_attn.o_proj', 'model.layers.29.self_attn.v_proj', 'model.layers.30.self_attn.o_proj', 'model.layers.20.mlp.down_proj', 'model.layers.23.mlp.up_proj', 'model.layers.26.self_attn.o_proj', 'model.layers.21.mlp.down_proj', 'model.layers.23.mlp.down_proj', 'model.layers.16.self_attn.o_proj', 'model.layers.29.mlp.down_proj', 'model.layers.17.self_attn.o_proj', 'model.layers.13.self_attn.v_proj', 'model.layers.31.self_attn.v_proj', 'model.layers.28.mlp.up_proj', 'model.layers.24.mlp.down_proj', 'model.layers.21.mlp.up_proj', 'model.layers.29.self_attn.o_proj', 'model.layers.4.mlp.up_proj', 'model.layers.19.mlp.up_proj', 'model.layers.17.mlp.down_proj', 'model.layers.18.mlp.up_proj', 'model.layers.20.mlp.up_proj', 'model.layers.19.mlp.down_proj', 'model.layers.22.mlp.up_proj', 'model.layers.18.mlp.down_proj', 'model.layers.5.mlp.down_proj', 'model.layers.5.mlp.up_proj', 'model.layers.14.self_attn.v_proj', 'model.layers.29.mlp.up_proj', 'model.layers.28.mlp.down_proj', 'model.layers.6.mlp.up_proj', 'model.layers.7.mlp.up_proj', 'lm_head', 'model.layers.2.mlp.down_proj', 'model.layers.16.mlp.down_proj', 'model.layers.11.self_attn.o_proj', 'model.layers.6.mlp.down_proj', 'model.layers.17.mlp.up_proj', 'model.layers.25.mlp.down_proj', 'model.layers.10.self_attn.o_proj', 'model.layers.26.self_attn.v_proj', 'model.layers.3.self_attn.v_proj', 'model.layers.27.mlp.gate_proj', 'model.layers.31.mlp.down_proj', 'model.layers.16.mlp.up_proj', 'model.layers.8.self_attn.q_proj', 'model.layers.5.self_attn.o_proj', 'model.layers.8.mlp.up_proj', 'model.layers.15.mlp.up_proj', 'model.layers.26.mlp.down_proj', 'model.layers.4.self_attn.v_proj', 'model.layers.1.mlp.down_proj', 'model.layers.13.mlp.up_proj', 'model.layers.30.mlp.up_proj', 'model.layers.28.mlp.gate_proj', 'model.layers.14.mlp.up_proj', 'model.layers.26.mlp.gate_proj', 'model.layers.24.self_attn.v_proj', 'model.layers.11.mlp.up_proj', 'model.layers.29.mlp.gate_proj', 'model.layers.12.mlp.up_proj', 'model.layers.9.mlp.up_proj', 'model.layers.25.mlp.gate_proj', 'model.layers.27.mlp.down_proj', 'model.layers.10.mlp.up_proj', 'model.layers.12.self_attn.o_proj', 'model.layers.5.self_attn.v_proj', 'model.layers.14.self_attn.o_proj', 'model.layers.2.self_attn.o_proj', 'model.layers.15.mlp.down_proj', 'model.layers.8.self_attn.o_proj']\n",
      "Enabling model.layers.1.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.2.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.2.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.3.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.3.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.4.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.4.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.5.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.5.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.5.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.5.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.6.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.6.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.7.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.8.self_attn.q_proj.weight parameter\n",
      "Enabling model.layers.8.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.8.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.9.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.10.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.10.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.11.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.11.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.11.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.12.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.12.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.13.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.13.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.14.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.14.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.14.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.15.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.15.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.16.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.16.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.16.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.17.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.17.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.17.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.17.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.18.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.18.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.19.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.19.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.20.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.20.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.20.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.20.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.21.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.21.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.22.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.22.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.23.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.23.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.23.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.23.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.24.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.24.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.24.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.24.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.25.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.25.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.25.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.25.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.25.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.26.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.26.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.26.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.26.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.26.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.27.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.27.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.27.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.27.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.27.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.28.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.28.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.28.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.28.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.28.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.29.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.29.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.29.mlp.gate_proj.weight parameter\n",
      "Enabling model.layers.29.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.29.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.30.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.30.mlp.down_proj.weight parameter\n",
      "Enabling model.layers.31.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.31.mlp.down_proj.weight parameter\n",
      "Enabling lm_head.weight parameter\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 50978\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 50\n",
       " }),\n",
       " 'predict_dataset': None,\n",
       " 'data_collator': DataCollatorForCausalLM(tokenizer=LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[PAD]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False), source_max_len=1024, target_max_len=256, train_on_source=False, predict_with_generate=False)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {k:v for k,v in data_module.items()}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nComplete the following sentence with a proper verb:\\nThe teacher ____ the results of the exam.\\n\\n### Response: '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train_dataset']['input'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=1,\n",
    "    collate_fn=dataset['data_collator']\n",
    ")\n",
    "\n",
    "input_memory = memall()- weight_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/50978 [00:05<15:43:31,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    output = model(**batch)\n",
    "    activation_memory = memall() - weight_memory\n",
    "    # loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps\n",
    "    loss = output.loss\n",
    "    loss.backward()\n",
    "    gradient_memory = memall() - weight_memory\n",
    "    optimizer.step()\n",
    "    optimizer_memory = memall() - gradient_memory - weight_memory \n",
    "    optimizer.zero_grad()\n",
    "    if step == 5:\n",
    "        break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight memory    : 28136.505344 MB\n",
      "Input memory     : 0.0 MB\n",
      "Activation memory: 27918.253056 MB\n",
      "Gradient memory  : 39917.12512 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 54770.253312 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "memory_string = (\n",
    "    f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "    f\"Input memory     : {input_memory / 1e6} MB\\n\"\n",
    "    f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "    f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "    f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "    f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    ")\n",
    "\n",
    "print(memory_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.memlog: # Memory Logging\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"{args.sortby} \"\n",
    "        + f\"{args.num_layers} Layers \"\n",
    "    )\n",
    "    Path(mempath).mkdir(parents=True, exist_ok=True)\n",
    "    logger = get_logger(mempath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(\n",
    "        f\"\\n{memory_string}\\n\"\n",
    "    )\n",
    "    logger.info(f\"\\nPeak Memory usage: {int(peek_memory/1e6)} MB\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and (args.do_train or args.do_eval or args.do_predict):\n",
    "    metrics_file_path = os.path.join(args.output_dir,\n",
    "                                f'trainseed_{args.seed}',\n",
    "                                args.dataset,\n",
    "                                f\"{args.sortby}_asc_{args.sort_ascending}\",\n",
    "                                f\"layers_{args.num_layers}\",\n",
    "                                \"metrics.json\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(metrics_file_path), exist_ok=True)\n",
    "    with open(metrics_file_path, \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
