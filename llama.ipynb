{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-07 03:17:32.970894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 03:17:32.970930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 03:17:32.972129: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-07 03:17:32.979418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 03:17:36.794875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/rscratch/tpang/kinshuk/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "    dataset=\"alpaca\", # DATASET [alpaca|chip2|self-instruct|hh-rlhf|oasst1|longform]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "\n",
    "    learning_rate=2e-5,     # LEARNING RATE\n",
    "    \n",
    "    max_steps=2000,         # NUMBER OF STEPS\n",
    "\n",
    "    sortby=\"alpha\",         # CAN DO \"alpha\" or \"lora\" or \"dora\"\n",
    "\n",
    "    num_layers=4,           # NUMBER OF LAYERS FOR FULL FINE-TUNING\n",
    "\n",
    "    per_device_train_batch_size = 2, # BATCH-SIZE\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=256 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "Dataset: alpaca\n",
      "Sort by: alpha\n",
      "Layers to train: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lora' in args.sortby:\n",
    "    args.num_layers = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj']\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 50978\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['input', 'output', 'length'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'predict_dataset': None,\n",
       " 'data_collator': DataCollatorForCausalLM(tokenizer=LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[PAD]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False), source_max_len=1024, target_max_len=256, train_on_source=False, predict_with_generate=False)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {k:v for k,v in data_module.items()}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25489"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.8763920068740845\n",
      "Step: 1, Train Loss: 1.6893244981765747\n",
      "Step: 2, Train Loss: 1.7251990636189778\n",
      "Step: 3, Train Loss: 1.7055594325065613\n",
      "Step: 4, Train Loss: 1.6468265056610107\n",
      "Step: 5, Train Loss: 1.643478790918986\n",
      "Step: 6, Train Loss: 1.6358988455363683\n",
      "Step: 7, Train Loss: 1.5874865353107452\n",
      "Step: 8, Train Loss: 1.5863214333852131\n",
      "Step: 9, Train Loss: 1.5107819139957428\n",
      "Step: 10, Train Loss: 1.43630220673301\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "optimizer.zero_grad() \n",
    "\n",
    "for epoch in range(1):\n",
    "        train_loss = 0\n",
    "        tr_steps = 0\n",
    "        for step, batch in enumerate((train_dataloader)):\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            curr = memall()\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            input_memory = memall() - curr\n",
    "\n",
    "            curr = memall()\n",
    "            output = model(**batch)\n",
    "            activation_memory = memall() - curr\n",
    "            \n",
    "            # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "            loss = output.loss\n",
    "\n",
    "            curr = memall()\n",
    "            loss.backward()\n",
    "            gradient_memory = memall() - curr\n",
    "    \n",
    "            curr = memall()\n",
    "            optimizer.step()\n",
    "            if step == 0:\n",
    "                 optimizer_memory = memall() - curr\n",
    "        \n",
    "            loss = loss.cpu()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "            tr_steps += 1\n",
    "            train_losses.append(train_loss/tr_steps)\n",
    "            if step % 1 == 0:\n",
    "                print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            if step == args.max_steps:\n",
    "                model.eval()\n",
    "                break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/125 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =     1.3176\n",
      "  eval_runtime            = 0:01:05.57\n",
      "  eval_samples_per_second =     15.249\n",
      "  eval_steps_per_second   =      1.906\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.eval()\n",
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = sum([max_memory_allocated(i) for i in range(gpus)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': './output',\n",
       " 'eval_loss': 1.317569375038147,\n",
       " 'eval_runtime': 65.576,\n",
       " 'eval_samples_per_second': 15.249,\n",
       " 'eval_steps_per_second': 1.906}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 198.18M\n",
      "Method           : alpha\n",
      "Layers           : 4\n",
      "Learning Rate    : 2e-05\n",
      "Batch size       : 2\n",
      "Weight memory    : 27087.929344 MB\n",
      "Activation memory: 702.235648 MB\n",
      "Gradient memory  : 68.23168 MB\n",
      "Optimizer memory : 0.0 MB\n",
      "Total memory     : 28711.701504 MB\n",
      "Peak memory      : 31703.087104 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_count(model)\n",
    "memory_string = (\n",
    "        f\"{param_count(model)}\\n\"\n",
    "        f\"Method           : {args.sortby}\\n\"\n",
    "        f\"Layers           : {args.num_layers}\\n\"\n",
    "        f\"Learning Rate    : {args.learning_rate}\\n\"\n",
    "        f\"Batch size       : {args.per_device_train_batch_size}\\n\"\n",
    "        f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "        f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "        f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "        f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "        f\"Input memory     : {input_memory / 1e6} MB\\n\"\n",
    "        f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    "        f\"Peak memory      : {peek_memory / 1e6} MB\\n\"\n",
    "    )\n",
    "print(memory_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE TRAINING HISTORY\n",
    "base = {\"train_loss\": train_loss,}\n",
    "savepath = f\"./output/{args.dataset}/lr_{args.learning_rate}/batch_{args.per_device_train_batch_size}/{args.sortby}/layers_{args.num_layers}\"\n",
    "if args.memlog:\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(os.path.join(savepath, \"finetune.npy\"), base) # type: ignore\n",
    "    with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"Batch Size {args.per_device_train_batch_size} \"\n",
    "        + f\"{args.sortby} fine-tuning \"\n",
    "        + f\"{args.num_layers} Layers\"\n",
    "    )\n",
    "    logger = get_logger(savepath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
