2024-04-11 13:31:28;INFO;

alpaca Batch Size 4 alpha fine-tuning 250 Layers
2024-04-11 13:31:28;INFO;
(6738.423808, 6738.423808)
Dataset          : alpaca
Method           : alpha
Layers           : 250
Batch size       : 4
Learning Rate    : 2e-07
Forward time     : 1.6876361966133118 min
Backward time    : 1.8700018207232156 min
Weight memory    : 28136.505344 MB
Optimizer memory : 53907.390464 MB
Activation memory: 10058.704384 MB
Gradient memory  : 26105.52064 MB
Input memory     : 0.016384 MB
Total memory     : 81195.7376 MB
Peak memory      : 136322.643456 MB


