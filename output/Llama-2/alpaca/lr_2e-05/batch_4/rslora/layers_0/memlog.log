2024-04-11 15:19:16;INFO;

alpaca Batch Size 4 rslora fine-tuning 0 Layers
2024-04-11 15:19:16;INFO;
(6739.048448, 0.62464)
Dataset          : alpaca
Method           : rslora
Layers           : 0
Batch size       : 4
Learning Rate    : 2e-05
Forward time     : 3.010078227519989 min
Backward time    : 7.255437850952148 min
Weight memory    : 27090.427904 MB
Optimizer memory : 4.99712 MB
Activation memory: 3859.825152 MB
Gradient memory  : 85.588992 MB
Input memory     : 0.01024 MB
Total memory     : 27178.525696 MB
Peak memory      : 45100.524544 MB


