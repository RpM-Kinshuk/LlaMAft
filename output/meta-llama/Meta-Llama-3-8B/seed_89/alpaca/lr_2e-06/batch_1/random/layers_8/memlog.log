2024-05-11 03:06:00;INFO;

alpaca Batch Size 1 random fine-tuning 8 Layers
2024-05-11 03:06:00;INFO;
(8030.26944, 234.881024)
Dataset          : alpaca
Method           : random
Layers           : 8
Batch size       : 1
Learning Rate    : 2e-06
Forward time     : 30.49415428241094 min
Backward time    : 52.37016123135884 min
Weight memory    : 32389.5296 MB
Optimizer memory : 1879.556096 MB
Activation memory: 1753.096704 MB
Gradient memory  : 1057.63072 MB
Input memory     : 0.004608 MB
Total memory     : 34387.196928 MB
Peak memory      : 40185.61024 MB


