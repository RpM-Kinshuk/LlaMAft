2024-05-16 21:55:19;INFO;

alpaca Batch Size 1 alpha_mid fine-tuning 12 Layers
2024-05-16 21:55:19;INFO;
(8030.26944, 898.633728)
Dataset          : alpaca
Method           : alpha_mid
Layers           : 12
Batch size       : 1
Learning Rate    : 2e-06
Forward time     : 47.235042707125345 min
Backward time    : 59.582913112640384 min
Weight memory    : 36592.222208 MB
Optimizer memory : 7189.069824 MB
Activation memory: 934.469632 MB
Gradient memory  : -523.621376 MB
Input memory     : 0.00256 MB
Total memory     : 39663.138304 MB
Peak memory      : 51203.649536 MB


