2024-04-07 12:58:13;INFO;

alpaca Batch Size 2 lora fine-tuning 0 Layers
2024-04-07 12:58:13;INFO;
(6898.331648, 159.90784)
Method           : lora
Layers           : 0
Learning Rate    : 0.0002
Batch size       : 2
Forward time     : 0.35843817790349325 min
Backward time    : 0.5871870756149292 min
Weight memory    : 27727.560704 MB
Optimizer memory : 1293.346816 MB
Activation memory: 2537.526784 MB
Gradient memory  : 691.675648 MB
Input memory     : 0.003584 MB
Total memory     : 29061.019648 MB
Peak memory      : 39728.390144 MB


2024-04-15 13:53:53;INFO;

alpaca Batch Size 2 lora fine-tuning 0 Layers
2024-04-15 13:53:53;INFO;
(6758.412288, 19.98848)
Dataset          : alpaca
Method           : lora
Layers           : 0
Batch size       : 2
Learning Rate    : 0.0002
Forward time     : 63.265262416998546 min
Backward time    : 48.61177171866099 min
Weight memory    : 27167.883264 MB
Optimizer memory : 159.90784 MB
Activation memory: 1964.267008 MB
Gradient memory  : 122.338816 MB
Input memory     : 0.004608 MB
Total memory     : 27370.180608 MB
Peak memory      : 41854.526464 MB


