2024-04-11 16:08:12;INFO;

alpaca Batch Size 2 dora fine-tuning 0 Layers
2024-04-11 16:08:12;INFO;
(6739.090944, 0.667136)
Dataset          : alpaca
Method           : dora
Layers           : 0
Batch size       : 2
Learning Rate    : 2e-05
Forward time     : 2.410907594362895 min
Backward time    : 3.055533548196157 min
Weight memory    : 27099.117568 MB
Optimizer memory : 5.337088 MB
Activation memory: 1882.46016 MB
Gradient memory  : 42.646016 MB
Input memory     : 0.004608 MB
Total memory     : 27144.436736 MB
Peak memory      : 34508.85376 MB


