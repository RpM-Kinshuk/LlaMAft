2024-05-13 09:13:57;INFO;

alpaca Batch Size 2 alpha_peak fine-tuning 12 Layers
2024-05-13 09:13:57;INFO;
(6738.423808, 342.884352)
Dataset          : alpaca
Method           : alpha_peak
Layers           : 12
Batch size       : 2
Learning Rate    : 2e-06
Forward time     : 18.211578694979348 min
Backward time    : 36.77779244979222 min
Weight memory    : 27087.929344 MB
Optimizer memory : 2743.074816 MB
Activation memory: 1539.894784 MB
Gradient memory  : 1418.911232 MB
Input memory     : 0.004608 MB
Total memory     : 29877.700608 MB
Peak memory      : 39689.501184 MB


