2024-05-02 13:09:38;INFO;

alpaca Batch Size 2 alora fine-tuning 32 Layers
2024-05-02 13:09:38;INFO;
(8033.284096, 3.014656)
Dataset          : alpaca
Method           : alora
Layers           : 32
Batch size       : 2
Learning Rate    : 2e-05
Forward time     : 24.73748109738032 min
Backward time    : 67.27342060407003 min
Weight memory    : 32401.588224 MB
Optimizer memory : 24.117248 MB
Activation memory: 2909.702656 MB
Gradient memory  : 169.668608 MB
Input memory     : 0.005632 MB
Total memory     : 32583.321088 MB
Peak memory      : 48498.553856 MB


