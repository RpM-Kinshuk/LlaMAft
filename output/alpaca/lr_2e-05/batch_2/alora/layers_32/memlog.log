2024-04-14 10:20:40;INFO;

alpaca Batch Size 2 alora fine-tuning 32 Layers
2024-04-14 10:20:40;INFO;
(6740.908032, 2.484224)
Dataset          : alpaca
Method           : alora
Layers           : 32
Batch size       : 2
Learning Rate    : 2e-05
Forward time     : 27.904895893732707 min
Backward time    : 30.805520594120026 min
Weight memory    : 27097.86624 MB
Optimizer memory : 19.873792 MB
Activation memory: 1381.326336 MB
Gradient memory  : 52.404736 MB
Input memory     : 0.004608 MB
Total memory     : 27160.21248 MB
Peak memory      : 37557.500416 MB


