2024-05-02 15:39:26;INFO;

alpaca Batch Size 1 alora fine-tuning 32 Layers
2024-05-02 15:39:26;INFO;
(8033.284096, 3.014656)
Dataset          : alpaca
Method           : alora
Layers           : 32
Batch size       : 1
Learning Rate    : 2e-05
Forward time     : 51.99799064795176 min
Backward time    : 69.47963432470958 min
Weight memory    : 32401.588224 MB
Optimizer memory : 24.117248 MB
Activation memory: 1103.881216 MB
Gradient memory  : 83.479552 MB
Input memory     : 0.00256 MB
Total memory     : 32497.12896 MB
Peak memory      : 40469.008896 MB


