2024-04-11 14:22:39;INFO;

alpaca Batch Size 2 rslora fine-tuning 0 Layers
2024-04-11 14:22:39;INFO;
(6739.048448, 0.62464)
Dataset          : alpaca
Method           : rslora
Layers           : 0
Batch size       : 2
Learning Rate    : 2e-06
Forward time     : 2.039109992980957 min
Backward time    : 2.821745495001475 min
Weight memory    : 27090.427904 MB
Optimizer memory : 4.99712 MB
Activation memory: 1800.538112 MB
Gradient memory  : 50.995712 MB
Input memory     : 0.004608 MB
Total memory     : 27143.926784 MB
Peak memory      : 34208.324608 MB


