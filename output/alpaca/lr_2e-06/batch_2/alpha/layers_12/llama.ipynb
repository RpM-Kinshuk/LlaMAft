{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jumbo/yaoqingyang/vipul/miniconda3/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-3dqbmtll because the default path (/thayerfs/home/f00752b/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "INFO:datasets:PyTorch version 2.1.2 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cachedir = '/jumbo/yaoqingyang/vipul/cache'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]= cachedir\n",
    "from model import get_model\n",
    "from loader.layers import param_count\n",
    "from loader.data_module import make_data_module\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch.backends.mps\n",
    "import torch.backends.cudnn\n",
    "from torch.cuda import (\n",
    "    max_memory_allocated,\n",
    "    reset_peak_memory_stats,\n",
    "    reset_max_memory_allocated,\n",
    "    memory_allocated,\n",
    ")\n",
    "from loader.logger import get_logger\n",
    "from transformers import set_seed\n",
    "# from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "from transformers.utils.logging import (\n",
    "    set_verbosity_error as transformers_vb_err,\n",
    ")\n",
    "from datasets.utils.logging import (\n",
    "    set_verbosity_error as datasets_vb_err,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from traineval.eval import eval_func\n",
    "logger = logging.getLogger(__name__)\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "from llamaft import ModelArguments, DataArguments, TrainingArguments, GenerationArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the arguments\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    eval_dataset_size=1024,\n",
    "    max_eval_samples=1000,\n",
    "    source_max_len = 1024,\n",
    "    target_max_len = 256,\n",
    "    dataset=\"alpaca\", # DATASET [alpaca|chip2|self-instruct|hh-rlhf|oasst1|longform]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    data_seed=7,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=187,\n",
    "    adam_beta2=0.999,\n",
    "\n",
    "    learning_rate=2e-6,     # LEARNING RATE\n",
    "    \n",
    "    max_steps=2000,         # NUMBER OF STEPS\n",
    "\n",
    "    sortby=\"alpha\",         # CAN DO \"alpha\" or \"lora\" or \"dora\"\n",
    "\n",
    "    num_layers=12,           # NUMBER OF LAYERS FOR FULL FINE-TUNING\n",
    "\n",
    "    per_device_train_batch_size = 2, # BATCH-SIZE\n",
    "    memlog=False,\n",
    ")\n",
    "\n",
    "generation_args = GenerationArguments(\n",
    "    max_new_tokens=256 # default is 256\n",
    ")\n",
    "\n",
    "# If you need to use GenerationConfig or similar for generation_args\n",
    "training_args.generation_config = transformers.GenerationConfig(\n",
    "    **vars(generation_args)\n",
    ")\n",
    "\n",
    "# Combine arguments into a single Namespace object (if needed)\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args),\n",
    ")\n",
    "\n",
    "args.cache_dir=cachedir\n",
    "\n",
    "# Control randomness\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_seed(args.seed)  # transformers seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed: 7\n",
      "Dataset: alpaca\n",
      "Sort by: alpha\n",
      "Layers to train: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'lora' in args.sortby:\n",
    "    args.num_layers = 0\n",
    "logger = logging.getLogger(__name__)\n",
    "gpus = torch.cuda.device_count()\n",
    "mempath = (\n",
    "    f\"/rscratch/tpang/kinshuk/RpMKin/llama_ft/{args.dataset}/\"\n",
    "    + f\"{args.sortby}\"\n",
    ")\n",
    "start_memory = [0] * gpus\n",
    "end_memory = [0] * gpus\n",
    "peek_memory = 0\n",
    "\n",
    "if args.verbose:\n",
    "        task_info = (\n",
    "            f\"\\nSeed: {args.seed}\\n\"\n",
    "            + f\"Dataset: {args.dataset}\\n\"\n",
    "            + f\"Sort by: {args.sortby}\\n\"\n",
    "            + f\"Layers to train: {args.num_layers}\\n\"\n",
    "        )\n",
    "        print(task_info)\n",
    "else:\n",
    "    datasets_vb_err()\n",
    "    transformers_vb_err()\n",
    "    global _tqdm_active\n",
    "    _tqdm_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5d7386793b4eed94ec1b90f05fb319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Sorted by  alpha\n",
      "Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.27.self_attn.v_proj', 'model.layers.11.self_attn.v_proj', 'model.layers.23.self_attn.o_proj', 'model.layers.3.mlp.up_proj']\n",
      "Enabling model.layers.3.mlp.up_proj.weight parameter\n",
      "Enabling model.layers.11.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.16.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.17.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.18.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.19.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.21.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.22.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.23.self_attn.o_proj.weight parameter\n",
      "Enabling model.layers.27.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.28.self_attn.v_proj.weight parameter\n",
      "Enabling model.layers.30.self_attn.v_proj.weight parameter\n"
     ]
    }
   ],
   "source": [
    "def memall(gpus=gpus):\n",
    "    for i in range(gpus):\n",
    "        start_memory[i] = torch.cuda.memory_allocated(i)\n",
    "    return sum(start_memory)\n",
    "\n",
    "model, tokenizer = get_model(args)\n",
    "\n",
    "for device in range(gpus):\n",
    "    reset_peak_memory_stats(device=device)\n",
    "    reset_max_memory_allocated(device=device)\n",
    "\n",
    "weight_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25489"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args) # type: ignore\n",
    "dataset = {k:v for k,v in data_module.items()}\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['train_dataset'], # type: ignore\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    collate_fn=dataset['data_collator'],\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Train Loss: 1.5841203927993774\n",
      "Step: 50, Train Loss: 1.3310706405078663\n",
      "Step: 100, Train Loss: 1.2850244369837318\n",
      "Step: 150, Train Loss: 1.2148389697864355\n",
      "Step: 200, Train Loss: 1.2144424544638068\n",
      "Step: 250, Train Loss: 1.1975869113705548\n",
      "Step: 300, Train Loss: 1.1830967158772225\n",
      "Step: 350, Train Loss: 1.1710468609618325\n",
      "Step: 400, Train Loss: 1.1466584839948692\n",
      "Step: 450, Train Loss: 1.1490224383424497\n",
      "Step: 500, Train Loss: 1.142504332278541\n",
      "Step: 550, Train Loss: 1.1358274322293846\n",
      "Step: 600, Train Loss: 1.1364754121099752\n",
      "Step: 650, Train Loss: 1.1316224191560997\n",
      "Step: 700, Train Loss: 1.1258130778057684\n",
      "Step: 750, Train Loss: 1.120587999514551\n",
      "Step: 800, Train Loss: 1.118191792765695\n",
      "Step: 850, Train Loss: 1.1145173816971228\n",
      "Step: 900, Train Loss: 1.1096800343153148\n",
      "Step: 950, Train Loss: 1.1063320579753502\n",
      "Step: 1000, Train Loss: 1.1058452502153553\n",
      "Step: 1050, Train Loss: 1.1034407803414834\n",
      "Step: 1100, Train Loss: 1.10134063752116\n",
      "Step: 1150, Train Loss: 1.0993011231873213\n",
      "Step: 1200, Train Loss: 1.0987606129510565\n",
      "Step: 1250, Train Loss: 1.0947316981030168\n",
      "Step: 1300, Train Loss: 1.0906780878716162\n",
      "Step: 1350, Train Loss: 1.0868103422367907\n",
      "Step: 1400, Train Loss: 1.0828788046094242\n",
      "Step: 1450, Train Loss: 1.079926671756805\n",
      "Step: 1500, Train Loss: 1.0790691177803107\n",
      "Step: 1550, Train Loss: 1.0765324641573937\n",
      "Step: 1600, Train Loss: 1.0735796919345\n",
      "Step: 1650, Train Loss: 1.07146328515957\n",
      "Step: 1700, Train Loss: 1.070308985846716\n",
      "Step: 1750, Train Loss: 1.0688185884310375\n",
      "Step: 1800, Train Loss: 1.0689887511153509\n",
      "Step: 1850, Train Loss: 1.066757136391328\n",
      "Step: 1900, Train Loss: 1.0676809493971617\n",
      "Step: 1950, Train Loss: 1.0657396163144122\n",
      "Step: 2000, Train Loss: 1.0645452429117381\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "times = []\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "optimizer.zero_grad()\n",
    "optimizer_memory = 0\n",
    "forward_time = 0\n",
    "backward_time = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_loss = 0\n",
    "    tr_steps = 0\n",
    "    tick = 0\n",
    "    total_time = 0\n",
    "    for step, batch in enumerate((train_dataloader)):\n",
    "\n",
    "        tick = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        curr = memall()\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        input_memory = memall() - curr\n",
    "        \n",
    "        curr = memall()\n",
    "        start = time.time()\n",
    "        output = model(**batch)\n",
    "        forward_time += time.time() - start\n",
    "        activation_memory = memall() - curr\n",
    "        \n",
    "        curr = memall()\n",
    "        start = time.time()\n",
    "        # loss = loss_fn(out.logits, batch[\"labels\"]) / args.gradient_accumulation_steps\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        backward_time += time.time() - start\n",
    "        gradient_memory = memall() - input_memory - weight_memory - optimizer_memory\n",
    "\n",
    "        curr = memall()\n",
    "        optimizer.step()\n",
    "        if step == 0:\n",
    "             optimizer_memory = memall() - curr\n",
    "    \n",
    "        loss = loss.cpu()\n",
    "        train_loss += loss.item()\n",
    "        tr_steps += 1\n",
    "        train_losses.append(train_loss/tr_steps)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step: {step}, Train Loss: {train_loss/tr_steps}')\n",
    "        torch.cuda.empty_cache()\n",
    "        total_time += time.time() - tick\n",
    "        times.append(total_time)\n",
    "        if step == args.max_steps:\n",
    "            model.eval()\n",
    "            break\n",
    "\n",
    "total_memory = memall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/125 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_loss               =     1.0835\n",
      "  eval_runtime            = 0:00:54.07\n",
      "  eval_samples_per_second =     18.494\n",
      "  eval_steps_per_second   =      2.312\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.eval()\n",
    "trainer=Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    "            )\n",
    "all_metrics = {\"run_name\": args.run_name}\n",
    "if args.do_eval:\n",
    "    all_metrics = eval_func(args, logger, trainer, all_metrics)\n",
    "total_memory = memall()\n",
    "peek_memory = sum([max_memory_allocated(i) for i in range(gpus)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': './output',\n",
       " 'eval_loss': 1.0834892988204956,\n",
       " 'eval_runtime': 54.0704,\n",
       " 'eval_samples_per_second': 18.494,\n",
       " 'eval_steps_per_second': 2.312}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 360.71M\n",
      "(6738.423808, 360.71424)\n",
      "Dataset          : alpaca\n",
      "Method           : alpha\n",
      "Layers           : 12\n",
      "Batch size       : 2\n",
      "Learning Rate    : 2e-06\n",
      "Forward time     : 1.2104967792828878 min\n",
      "Backward time    : 3.509366476535797 min\n",
      "Weight memory    : 27087.929344 MB\n",
      "Optimizer memory : 2885.71392 MB\n",
      "Activation memory: 4503.892992 MB\n",
      "Gradient memory  : 1525.43488 MB\n",
      "Input memory     : 0.008704 MB\n",
      "Total memory     : 30056.229888 MB\n",
      "Peak memory      : 41205.979136 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "memory_string = (\n",
    "        f\"{param_count(model)}\\n\"\n",
    "        f\"Dataset          : {args.dataset}\\n\"\n",
    "        f\"Method           : {args.sortby}\\n\"\n",
    "        f\"Layers           : {args.num_layers}\\n\"\n",
    "        f\"Batch size       : {args.per_device_train_batch_size}\\n\"\n",
    "        f\"Learning Rate    : {args.learning_rate}\\n\"\n",
    "        f\"Forward time     : {forward_time/60} min\\n\"\n",
    "        f\"Backward time    : {backward_time/60} min\\n\"\n",
    "        f\"Weight memory    : {weight_memory / 1e6} MB\\n\"\n",
    "        f\"Optimizer memory : {optimizer_memory / 1e6} MB\\n\"\n",
    "        f\"Activation memory: {activation_memory / 1e6} MB\\n\"\n",
    "        f\"Gradient memory  : {gradient_memory / 1e6} MB\\n\"\n",
    "        f\"Input memory     : {input_memory / 1e6} MB\\n\"\n",
    "        f\"Total memory     : {total_memory / 1e6} MB\\n\"\n",
    "        f\"Peak memory      : {peek_memory / 1e6} MB\\n\"\n",
    "    )\n",
    "print(memory_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "alpaca Batch Size 2 alpha fine-tuning 12 Layers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-08 01:26:42;INFO;\n",
      "\n",
      "alpaca Batch Size 2 alpha fine-tuning 12 Layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "(6738.423808, 360.71424)\n",
      "Dataset          : alpaca\n",
      "Method           : alpha\n",
      "Layers           : 12\n",
      "Batch size       : 2\n",
      "Learning Rate    : 2e-06\n",
      "Forward time     : 1.2104967792828878 min\n",
      "Backward time    : 3.509366476535797 min\n",
      "Weight memory    : 27087.929344 MB\n",
      "Optimizer memory : 2885.71392 MB\n",
      "Activation memory: 4503.892992 MB\n",
      "Gradient memory  : 1525.43488 MB\n",
      "Input memory     : 0.008704 MB\n",
      "Total memory     : 30056.229888 MB\n",
      "Peak memory      : 41205.979136 MB\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-08 01:26:42;INFO;\n",
      "(6738.423808, 360.71424)\n",
      "Dataset          : alpaca\n",
      "Method           : alpha\n",
      "Layers           : 12\n",
      "Batch size       : 2\n",
      "Learning Rate    : 2e-06\n",
      "Forward time     : 1.2104967792828878 min\n",
      "Backward time    : 3.509366476535797 min\n",
      "Weight memory    : 27087.929344 MB\n",
      "Optimizer memory : 2885.71392 MB\n",
      "Activation memory: 4503.892992 MB\n",
      "Gradient memory  : 1525.43488 MB\n",
      "Input memory     : 0.008704 MB\n",
      "Total memory     : 30056.229888 MB\n",
      "Peak memory      : 41205.979136 MB\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAVE TRAINING HISTORY\n",
    "base = {\"train_loss\": train_losses,\"time\": times}\n",
    "savepath = f\"./output/{args.dataset}/lr_{args.learning_rate}/batch_{args.per_device_train_batch_size}/{args.sortby}/layers_{args.num_layers}\"\n",
    "if True:\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(os.path.join(savepath, \"finetune.npy\"), base) # type: ignore\n",
    "    with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    log_info = (\n",
    "        f\"\\n\\n{args.dataset} \"\n",
    "        + f\"Batch Size {args.per_device_train_batch_size} \"\n",
    "        + f\"{args.sortby} fine-tuning \"\n",
    "        + f\"{args.num_layers} Layers\"\n",
    "    )\n",
    "    logger = get_logger(savepath, \"memlog.log\")\n",
    "    logger.info(log_info)\n",
    "    logger.info(f\"\\n{memory_string}\\n\")\n",
    "    if (args.do_train or args.do_eval or args.do_predict):\n",
    "        with open(os.path.join(savepath, \"metrics.json\"), \"w\") as fout:\n",
    "            fout.write(json.dumps(all_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
