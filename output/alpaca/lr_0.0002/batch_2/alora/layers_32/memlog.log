2024-04-14 16:59:50;INFO;

alpaca Batch Size 2 alora fine-tuning 32 Layers
2024-04-14 16:59:50;INFO;
(6740.908032, 2.484224)
Dataset          : alpaca
Method           : alora
Layers           : 32
Batch size       : 2
Learning Rate    : 0.0002
Forward time     : 24.06538178126017 min
Backward time    : 34.201606929302216 min
Weight memory    : 27097.86624 MB
Optimizer memory : 19.873792 MB
Activation memory: 1381.326336 MB
Gradient memory  : 52.404736 MB
Input memory     : 0.004608 MB
Total memory     : 27160.21248 MB
Peak memory      : 37557.500416 MB


