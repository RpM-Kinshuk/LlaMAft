2024-03-29 13:39:08;INFO;Starting Alpaca dataset training
2024-03-29 13:39:10;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 0 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 5
2024-03-29 13:39:21;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 1 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 6
2024-03-29 13:39:22.197480: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:39:22.197524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:39:22.199245: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:39:22.207935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:39:26.369231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:39:30.414454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:39:30.414501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:39:30.416148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:39:30.439233: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:39:32;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 2 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 7
2024-03-29 13:39:33.840422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-39-35_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=0, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:39:42.353118: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:39:42.353168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:39:42.354806: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:39:42.363290: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-39-41_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=1, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 1

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:39:43;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
2024-03-29 13:39:46.271392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:39:54;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-39-55_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=2, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 2

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:40:05;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.72s/it]2024-03-29 13:40:16;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 21.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.47s/it]
2024-03-29 13:40:27;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("./llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.87s/it]
2024-03-29 13:40:32;INFO;Finishing 0th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 0 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.17s/it]Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("./llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
2024-03-29 13:40:38;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 3 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 5
2024-03-29 13:40:39;INFO;Finishing 1th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 1 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.90s/it]
2024-03-29 13:40:50;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 4 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 6
2024-03-29 13:40:51.285280: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:40:51.285331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:40:51.286892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:40:51.297396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("./llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
2024-03-29 13:40:55;INFO;Finishing 2th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 2 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


2024-03-29 13:40:56.246474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:41:00.472065: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:41:00.472113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:41:00.473561: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:41:00.481911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:41:01;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 5 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 7
2024-03-29 13:41:04.237313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-41-06_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=3, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 3

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:41:12.397429: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:41:12.397592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:41:12.399315: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:41:12.408631: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:41:12;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-41-13_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=4, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 4

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:41:15.941746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:41:23;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-41-24_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=5, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 5

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:41:34;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.38s/it]2024-03-29 13:41:45;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.92s/it]
2024-03-29 13:41:56;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 21.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.29s/it]
Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("./llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
2024-03-29 13:42:06;INFO;Finishing 3th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 3 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


2024-03-29 13:42:07;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 6 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 5
Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("./llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.86s/it]
2024-03-29 13:42:11;INFO;Finishing 4th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 4 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("./llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
2024-03-29 13:42:18;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 8 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 6
2024-03-29 13:42:18.892042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:42:18.892094: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:42:18.893797: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:42:18.924040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:42:19;INFO;Finishing 5th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 5 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


2024-03-29 13:42:22.632409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:42:28.275242: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:42:28.275294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:42:28.277118: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:42:28.286904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:42:29;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 10 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 7
2024-03-29 13:42:31.430718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-42-31_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=6, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 6

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:42:39.818811: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:42:39.818859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:42:39.820370: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:42:39.828977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:42:41;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-42-39_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=8, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 8

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:42:43.530720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:42:52;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-42-51_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=10, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 10

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:43:03;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.93s/it]2024-03-29 13:43:14;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 22.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 24.47s/it]
2024-03-29 13:43:24;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Adding special tokens.
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 304, in main
    model, tokenizer = get_model(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/model.py", line 177, in get_model
    layer_to_train = get_layers(args)
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/loader/layers.py", line 5, in get_layers
    ww_details = pd.read_csv("/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llama_ww.csv")
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './llama_ww.csv'
Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 20.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.00s/it]
2024-03-29 13:43:32;INFO;Finishing 6th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 6 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


2024-03-29 13:43:35;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 12 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 5
Adding special tokens.
Sorted by  alpha
Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.17.self_attn.v_proj']
Enabling model.layers.16.self_attn.v_proj.weight parameter
Enabling model.layers.17.self_attn.v_proj.weight parameter
Enabling model.layers.18.self_attn.v_proj.weight parameter
Enabling model.layers.19.self_attn.v_proj.weight parameter
Enabling model.layers.21.self_attn.v_proj.weight parameter
Enabling model.layers.22.self_attn.v_proj.weight parameter
Enabling model.layers.28.self_attn.v_proj.weight parameter
Enabling model.layers.30.self_attn.v_proj.weight parameter
  0%|          | 0/1626 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 21.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.32s/it]
  0%|          | 0/1626 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 326, in main
    output = model(**batch)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 806, in forward
    outputs = self.model(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 693, in forward
    layer_outputs = decoder_layer(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 421, in forward
    hidden_states = self.mlp(hidden_states)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 216, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 438.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 84.94 MiB is free. Process 167017 has 806.00 MiB memory in use. Including non-PyTorch memory, this process has 46.64 GiB memory in use. Of the allocated memory 44.85 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-03-29 13:43:44;INFO;Finishing 7th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 8 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


Adding special tokens.
Sorted by  alpha
Training layers: ['model.layers.21.self_attn.v_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.27.self_attn.v_proj', 'model.layers.11.self_attn.v_proj']
Enabling model.layers.11.self_attn.v_proj.weight parameter
Enabling model.layers.16.self_attn.v_proj.weight parameter
Enabling model.layers.17.self_attn.v_proj.weight parameter
Enabling model.layers.18.self_attn.v_proj.weight parameter
Enabling model.layers.19.self_attn.v_proj.weight parameter
Enabling model.layers.21.self_attn.v_proj.weight parameter
Enabling model.layers.22.self_attn.v_proj.weight parameter
Enabling model.layers.27.self_attn.v_proj.weight parameter
Enabling model.layers.28.self_attn.v_proj.weight parameter
Enabling model.layers.30.self_attn.v_proj.weight parameter
  0%|          | 0/1626 [00:00<?, ?it/s]2024-03-29 13:43:47;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 18 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 6
2024-03-29 13:43:48.200839: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:43:48.200889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:43:48.202544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:43:48.210957: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
  0%|          | 0/1626 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 364, in <module>
    main()
  File "/rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py", line 326, in main
    output = model(**batch)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 806, in forward
    outputs = self.model(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 693, in forward
    layer_outputs = decoder_layer(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 408, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/rscratch/tpang/kinshuk/anaconda3/envs/kinbert/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 330, in forward
    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 414.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 357.06 MiB is free. Process 4061344 has 698.00 MiB memory in use. Process 4062530 has 902.00 MiB memory in use. Process 4063830 has 798.00 MiB memory in use. Including non-PyTorch memory, this process has 44.79 GiB memory in use. Of the allocated memory 42.96 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-03-29 13:43:53.025230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:43:54;INFO;Finishing 8th + OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 10 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32 


2024-03-29 13:43:58.245567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:43:58.245619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:43:58.247160: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:43:58.256869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:43:58;INFO;executing OMP_NUM_THREADS=1 python /rscratch/tpang/kinshuk/RpMKin/llama_ft/LlaMAft/llamaft.py --seed 7 --data_seed 7 --output_dir /rscratch/tpang/kinshuk/RpMKin/llama_ft/data --dataset alpaca --max_eval_samples 50 --dataloader_num_workers 1 --do_eval false --max_steps 2 --sortby alpha --num_layers 24 --source_max_len 512 --verbose --memlog --per_device_train_batch_size 32  on GPU: 7
2024-03-29 13:44:02.264646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-44-01_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=12, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 12

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:44:09.123697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-29 13:44:09.123754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-29 13:44:09.125425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-29 13:44:09.136064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-29 13:44:09;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-44-11_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=18, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 18

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:44:13.074279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-29 13:44:20;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=True, use_auth_token=False, lora_modules=[], eval_dataset_size=1024, max_train_samples=None, max_eval_samples=50, source_max_len=512, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data/runs/Mar29_13-44-21_a23', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=7, data_seed=7, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/rscratch/tpang/kinshuk/RpMKin/llama_ft/data', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, cache_dir='/rscratch/tpang/kinshuk/cache', verbose=True, memlog=True, freeze=True, sortby='alpha', num_layers=24, sort_ascending=False, add_layer_norm=False, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, adam8bit=False, double_quant=False, quant_type='nf4', bits=16, lora_r=8, lora_alpha=16, lora_dropout=0.0, full_finetune=False, max_memory_MB=45000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)

Seed: 7
Dataset: alpaca
Sort by: alpha
Layers to train: 24

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2024-03-29 13:44:31;INFO;Waiting on GPUs, Checking 0/0 at gpu -1
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.88s/it]