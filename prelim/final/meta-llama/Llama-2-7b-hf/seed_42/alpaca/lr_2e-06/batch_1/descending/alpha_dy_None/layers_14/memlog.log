2024-05-31 21:22:52;INFO;

alpaca Batch Size 1 alpha_dy_None fine-tuning 14 Layers
2024-05-31 21:22:52;INFO;
Total param      : 6738.423808
Train param      : 319.81568
Dataset          : alpaca
Method           : alpha_dy_None
Layers           : 14
Batch size       : 1
Learning Rate    : 2e-06
Eval Loss        : 1.0582424402236938
Forward time     : 12.864950243631998 min
Backward time    : 26.775179505348206 min
Weight memory    : 26953.711616 MB
Optimizer memory : 2332.033024 MB
Activation memory: 871.4808100542746 MB
Gradient memory  : 1438.4670638481568 MB
Input memory     : 0.0030693898039215687 MB
Total memory     : 31230.52544 MB
Peak memory      : 34003.705856 MB


