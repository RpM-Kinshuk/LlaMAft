2024-05-31 08:59:03;INFO;

alpaca Batch Size 1 alora fine-tuning 32 Layers
2024-05-31 08:59:03;INFO;
Total param      : 6758.2976
Train param      : 19.873792
Dataset          : alpaca
Method           : alora
Layers           : 32
Batch size       : 1
Learning Rate    : 2e-05
Eval Loss        : 1.0680131912231445
Forward time     : 17.00812643369039 min
Backward time    : 23.920892802874246 min
Weight memory    : 27033.206784 MB
Optimizer memory : 161.824768 MB
Activation memory: 904.9351000345098 MB
Gradient memory  : 114.41345985756863 MB
Input memory     : 0.0030701527843137255 MB
Total memory     : 27321.422848 MB
Peak memory      : 32326.382592 MB


